{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03c2c19-95d7-47d5-9d8c-00b179b83a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment --quiet\n",
    "!pip install textstat --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install tpot --quiet\n",
    "!pip install seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b71739-66be-47d2-934e-5047eab61a9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "from config import LSHS_DATAFILE, gpt_filtered_rephrase_lshs_file\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44b6745-28d9-439b-8423-560034f1e87f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/atif/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/atif/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1436210b-642c-4d22-9e0f-720489b70a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "#     level=logging.INFO,\n",
    "#     datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# # The default levels are DEBUG, INFO, WARNING, ERROR, and CRITICAL.\n",
    "# print(logging.WARNING)\n",
    "# logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d6c6e2-3c9a-48d0-b5bb-a389b5dce5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "gpt_filtered_rephrase_tweets_file = gpt_filtered_rephrase_lshs_file\n",
    "out_file = open(gpt_filtered_rephrase_tweets_file, \"r\")\n",
    "filtered_rephrase_tweet_gpt = json.load(out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff75588-e4db-4f2a-87cf-0f49f383acf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender (9454, 4)\n",
      "Religion (10869, 4)\n",
      "Race (12013, 4)\n",
      "Politics (11018, 4)\n",
      "Sports (12306, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(LSHS_DATAFILE)\n",
    "domains = df['Domain'].unique().tolist()\n",
    "for d in domains:\n",
    "    print(d, df[df['Domain'] == d].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933cc930-8e57-4542-b945-b93722434f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>1344706773245038592</td>\n",
       "      <td>WATCH: Video previews #SurreyBC-shot film to f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gender</td>\n",
       "      <td>1344706877217792005</td>\n",
       "      <td>Men and women donâ€™t have to solve their proble...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gender</td>\n",
       "      <td>1344707261155962880</td>\n",
       "      <td>At last I awake, very queer about the head, as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gender</td>\n",
       "      <td>1344707529213792256</td>\n",
       "      <td>WATCH: Video previews Surrey-shot film to focu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gender</td>\n",
       "      <td>1344709019865403394</td>\n",
       "      <td>heteronormativity is killing my people. how so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Domain              TweetID   \n",
       "0  Gender  1344706773245038592  \\\n",
       "1  Gender  1344706877217792005   \n",
       "2  Gender  1344707261155962880   \n",
       "3  Gender  1344707529213792256   \n",
       "4  Gender  1344709019865403394   \n",
       "\n",
       "                                               Tweet  Label  \n",
       "0  WATCH: Video previews #SurreyBC-shot film to f...      0  \n",
       "1  Men and women donâ€™t have to solve their proble...      0  \n",
       "2  At last I awake, very queer about the head, as...      0  \n",
       "3  WATCH: Video previews Surrey-shot film to focu...      0  \n",
       "4  heteronormativity is killing my people. how so...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9f0675-8550-4fc6-8a6a-4304c4f01943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "\n",
    "\n",
    "    #############LINE FIXED: * REPLACED WITH +##################### PREVIOUS::: tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]+\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "\n",
    "    #############LINE FIXED: * REPLACED WITH +##################### PREVIOUS::: tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]+\", tweet.lower())).strip()\n",
    "    return tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61cb2a3-97ed-4bbc-aa6f-ed1d4e78a4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6953610f-d140-4baf-af43-2d642e2d82ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Features:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize,\n",
    "        preprocessor=preprocess,\n",
    "        ngram_range=(1, 3),\n",
    "        stop_words=stopwords,\n",
    "        use_idf=True,\n",
    "        smooth_idf=False,\n",
    "        norm=None,\n",
    "        decode_error='replace',\n",
    "        max_features=10000,\n",
    "        min_df=5,\n",
    "        max_df=0.75\n",
    "        )\n",
    "        \n",
    "        #We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "        self.pos_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=None,\n",
    "        lowercase=False,\n",
    "        preprocessor=None,\n",
    "        ngram_range=(1, 3),\n",
    "        stop_words=None,\n",
    "        use_idf=False,\n",
    "        smooth_idf=False,\n",
    "        norm=None,\n",
    "        decode_error='replace',\n",
    "        max_features=5000,\n",
    "        min_df=5,\n",
    "        max_df=0.75,\n",
    "        )\n",
    "        \n",
    "        self.other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]\n",
    "        \n",
    "    def __tfidf__(self, tweets, isTrain=True):\n",
    "        #Construct tfidf matrix and get relevant scores\n",
    "        if isTrain:\n",
    "            return self.vectorizer.fit_transform(tweets).toarray()\n",
    "        else:\n",
    "            return self.vectorizer.transform(tweets).toarray()\n",
    "    \n",
    "    def __get_pos_tags__(self, tweets):\n",
    "        #Get POS tags for tweets and save as a string\n",
    "        tweet_tags = []\n",
    "        for t in tweets:\n",
    "            tokens = basic_tokenize(preprocess(t))\n",
    "            tags = nltk.pos_tag(tokens)\n",
    "            tag_list = [x[1] for x in tags]\n",
    "            tag_str = \" \".join(tag_list)\n",
    "            tweet_tags.append(tag_str)\n",
    "        return tweet_tags\n",
    "    \n",
    "    def __pos_tags__(self, tweets, isTrain=True):\n",
    "        tweet_tags = self.__get_pos_tags__(tweets)\n",
    "        \n",
    "        #Construct POS TF matrix and get vocab dict\n",
    "        if isTrain:\n",
    "            return self.pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "        else:\n",
    "            return self.pos_vectorizer.transform(pd.Series(tweet_tags)).toarray()\n",
    "    \n",
    "    def get_features(self, tweets, isTrain=True):\n",
    "        tfidf = self.__tfidf__(tweets, isTrain=isTrain)\n",
    "        pos = self.__pos_tags__(tweets, isTrain=isTrain)\n",
    "        self.feats = get_feature_array(tweets)\n",
    "        \n",
    "        #Now join them all up\n",
    "        # recover ids for mapping\n",
    "        # ids = np.array(tweets.index.to_list())\n",
    "        # ids = ids.reshape(ids.shape[0], 1)\n",
    "        # M = np.concatenate([ids, tfidf,pos,feats],axis=1)\n",
    "        M = np.concatenate([tfidf, pos, self.feats],axis=1)\n",
    "        \n",
    "        X = pd.DataFrame(M)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8eae50-4ace-44fb-ae31-7d9088925c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X, y = df[['Domain', 'Tweet']], df['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8538a42c-2aad-4033-86e8-f1ca50239c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HateLabel\tFinal hate label decision 0-Normal, 1-Offensive, 2-Hate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45aaa4-b29a-41a5-bd6e-7e176999bcf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Our Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ed865f-c2b9-4e32-85fd-749b1660daf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CF_LABEL = 0\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a2a446-ece6-44a5-857e-cc7159f9c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    44874\n",
      "1     9669\n",
      "2     1117\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10786, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['Label'].value_counts())\n",
    "problematic_df = df[df['Label']>0]\n",
    "problematic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8651df44-7400-4918-8dfc-1c42bbf8d9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55660, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65245a9b-7012-4de0-8608-91a8dce64f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55655</th>\n",
       "      <td>Sports</td>\n",
       "      <td>1277315350254751747</td>\n",
       "      <td>Fuck off Gayle, professional footballer and yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55656</th>\n",
       "      <td>Sports</td>\n",
       "      <td>1277319456071581698</td>\n",
       "      <td>Omo I hate mancity abeg. What is this fluid fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55657</th>\n",
       "      <td>Sports</td>\n",
       "      <td>1277316487271854082</td>\n",
       "      <td>I hate playing Manchester United again</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55658</th>\n",
       "      <td>Sports</td>\n",
       "      <td>1277319975305445381</td>\n",
       "      <td>I'll get trolled to fuck but I'd give anything...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55659</th>\n",
       "      <td>Sports</td>\n",
       "      <td>1277311388759863296</td>\n",
       "      <td>Chelsea though! I think I hate football. ðŸ™„ #mufc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Domain              TweetID   \n",
       "55655  Sports  1277315350254751747  \\\n",
       "55656  Sports  1277319456071581698   \n",
       "55657  Sports  1277316487271854082   \n",
       "55658  Sports  1277319975305445381   \n",
       "55659  Sports  1277311388759863296   \n",
       "\n",
       "                                                   Tweet  Label  \n",
       "55655  Fuck off Gayle, professional footballer and yo...      1  \n",
       "55656  Omo I hate mancity abeg. What is this fluid fo...      2  \n",
       "55657             I hate playing Manchester United again      2  \n",
       "55658  I'll get trolled to fuck but I'd give anything...      1  \n",
       "55659   Chelsea though! I think I hate football. ðŸ™„ #mufc      2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problematic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f78bdb-cf13-452f-9400-b134e55a7b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    gpt_counterfactual_tweets = {}\n",
    "    tot = problematic_df.shape[0]\n",
    "    # print(tot)\n",
    "    for i in range(0, tot):\n",
    "        idx = problematic_df.iloc[i].name\n",
    "        if str(i) in filtered_rephrase_tweet_gpt:\n",
    "            gpt_counterfactual_tweets[idx] = filtered_rephrase_tweet_gpt[str(i)]\n",
    "    return gpt_counterfactual_tweets\n",
    "    \n",
    "gpt_counterfactual_tweets = get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe652ef-b511-48f8-bd32-13f821f905d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_counterfactual_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b13567a-c2bd-481a-878f-1efd8380f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import mosestokenizer\n",
    "import numpy as np\n",
    "\n",
    "def get_offensive_words():\n",
    "    _df = pd.read_csv(config.en_swear_words_datafile, index_col=0)\n",
    "    \n",
    "    s = np.logical_or(_df['Level of offensiveness']=='Strongest words', _df['Level of offensiveness']=='Strong words')\n",
    "    # display(_df[s]['Word'].to_list())\n",
    "    wd_list = _df['Word'].to_list()\n",
    "    \n",
    "    _df = pd.read_csv(config.en_profanity_datafile, index_col=None)\n",
    "    s = _df['severity_description'] == 'Severe'\n",
    "    # wd_list.extend(_df[s]['text'].to_list())\n",
    "    wd_list.extend(_df['text'].to_list())\n",
    "    wd_list = set(map(str.lower, wd_list))\n",
    "    return wd_list\n",
    "\n",
    "offensive_wd_list = get_offensive_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4964bae-e256-4389-88f1-2937754490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrases(tokens, phrases):\n",
    "    tokens = list(map(str.lower, tokens))\n",
    "    \"\"\"\n",
    "    Find phrases in a list of sequential tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of sequential tokens.\n",
    "        phrases (list): List of phrases to search for.\n",
    "        \n",
    "    Returns:\n",
    "        A list of tuples containing the start and end index of each found phrase.\n",
    "    \"\"\"\n",
    "    found_phrases = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        for phrase in phrases:\n",
    "            if tokens[i:i+len(phrase)] == phrase:\n",
    "                found_phrases.append((i, i+len(phrase)-1))\n",
    "    \n",
    "    return found_phrases\n",
    "\n",
    "def offensive_lexicon_used(t):\n",
    "    tk = TweetTokenizer()\n",
    "    detk = mosestokenizer.MosesDetokenizer('en')\n",
    "    tk = tk.tokenize(t)\n",
    "    # print(tk)\n",
    "    phrase_index = find_phrases(tk, list(map(str.split, offensive_wd_list)))\n",
    "    return len(phrase_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9655ac19-9e57-493a-8118-129c493e66a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_counterfactual_tweets(data, labels, cf_label, single_cf_per_tweet=False, cf_size_prop_to_data=1.0, only_tweets_with_offensive_lexicon=True):\n",
    "    tweets = []\n",
    "    cnt =0 \n",
    "    for idx in data.index:\n",
    "        if idx in gpt_counterfactual_tweets:\n",
    "            if (not only_tweets_with_offensive_lexicon) or offensive_lexicon_used(X[idx]):\n",
    "                cnt += 1\n",
    "                if not single_cf_per_tweet:\n",
    "                    tweets.extend(gpt_counterfactual_tweets[idx])\n",
    "                else:\n",
    "                    tweets.append(gpt_counterfactual_tweets[idx][0])\n",
    "    print('> Total Tweets used to generate counterfactuals ' + str(cnt))\n",
    "    print('> Total counterfactuals added ' + str(len(tweets)))\n",
    "    k = round(cf_size_prop_to_data * len(tweets))\n",
    "    \n",
    "    tweets = random.sample(tweets, k=k)\n",
    "    print('> Counterfactual size ' + str(k) + ' at rate ' + str(cf_size_prop_to_data))\n",
    "    cf_target = k*[cf_label]\n",
    "    return pd.concat([data, pd.Series(tweets)], axis=0), pd.concat([labels, pd.Series(cf_target)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f96b6b9-79e2-4a75-a85e-0eb9d264133c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# winner tpot-pipeline 40\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76dfa7e4-3df3-4809-9640-8f5a7b8f4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_tweets_with_offensive_lexicon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ef56a29-ba93-4871-b643-e2f59cec786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender (9454, 4)\n",
      "0\n",
      "> Total Tweets used to generate counterfactuals 845\n",
      "> Total counterfactuals added 845\n",
      "> Counterfactual size 84 at rate 0.1\n",
      "1\n",
      "> Total Tweets used to generate counterfactuals 767\n",
      "> Total counterfactuals added 767\n",
      "> Counterfactual size 77 at rate 0.1\n",
      "Religion (10869, 4)\n",
      "0\n",
      "> Total Tweets used to generate counterfactuals 762\n",
      "> Total counterfactuals added 762\n",
      "> Counterfactual size 76 at rate 0.1\n",
      "1\n",
      "> Total Tweets used to generate counterfactuals 771\n",
      "> Total counterfactuals added 771\n",
      "> Counterfactual size 77 at rate 0.1\n",
      "Race (12013, 4)\n",
      "0\n",
      "> Total Tweets used to generate counterfactuals 570\n",
      "> Total counterfactuals added 570\n",
      "> Counterfactual size 57 at rate 0.1\n",
      "1\n",
      "> Total Tweets used to generate counterfactuals 520\n",
      "> Total counterfactuals added 520\n",
      "> Counterfactual size 52 at rate 0.1\n",
      "Politics (11018, 4)\n",
      "0\n",
      "> Total Tweets used to generate counterfactuals 1001\n",
      "> Total counterfactuals added 1001\n",
      "> Counterfactual size 100 at rate 0.1\n",
      "1\n",
      "> Total Tweets used to generate counterfactuals 1040\n",
      "> Total counterfactuals added 1040\n",
      "> Counterfactual size 104 at rate 0.1\n",
      "Sports (12306, 4)\n",
      "0\n",
      "> Total Tweets used to generate counterfactuals 871\n",
      "> Total counterfactuals added 871\n",
      "> Counterfactual size 87 at rate 0.1\n",
      "1\n",
      "> Total Tweets used to generate counterfactuals 988\n",
      "> Total counterfactuals added 988\n",
      "> Counterfactual size 99 at rate 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "for d in domains:\n",
    "    sel_df = df[df['Domain'] == d]\n",
    "    print(d, sel_df.shape)\n",
    "    X, y = sel_df['Tweet'], sel_df['Label'].astype(int)\n",
    "    skf = StratifiedKFold(n_splits=2, random_state=None)\n",
    "    for splt_idx, (train_index , test_index) in enumerate(skf.split(X, y)):\n",
    "        print(splt_idx)\n",
    "        X_train , X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train , y_test = y.iloc[train_index] , y.iloc[train_index]\n",
    "        _,_ = get_counterfactual_tweets(\n",
    "                X_train, y_train, cf_label=CF_LABEL, single_cf_per_tweet=True, cf_size_prop_to_data=.1, only_tweets_with_offensive_lexicon=only_tweets_with_offensive_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03ca45-9b27-47be-bcae-79758db00dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88d17bd2-4227-44dc-819d-e353a92ba7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def __exp__(pipeline, _x_train, _x_test, _y_train, _y_test, CF=False):\n",
    "    f = Features()\n",
    "    training_features = f.get_features(_x_train, isTrain=True)\n",
    "    testing_features = f.get_features(_x_test, isTrain=False)\n",
    "    \n",
    "    if not CF:\n",
    "        print('> Train samples', _x_train.shape[0])\n",
    "    else:\n",
    "        print('> Train with CF samples', _x_train.shape[0])\n",
    "    \n",
    "    try:\n",
    "        # Fix random state for all the steps in exported pipeline\n",
    "        set_param_recursive(pipeline.steps, 'random_state', 42)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pipeline.fit(training_features, _y_train)\n",
    "    results = pipeline.predict(testing_features)\n",
    "\n",
    "    # report = classification_report(_y_test, results)\n",
    "    # print(report)\n",
    "    acc = accuracy_score(_y_test, results)\n",
    "    f1_marco = f1_score(_y_test, results, average='macro')\n",
    "    f1_weighted = f1_score(_y_test, results, average='weighted')\n",
    "    f1_non_avg = f1_score(_y_test, results, average=None)\n",
    "    r = {'Accuracy': acc,\n",
    "         'F1-Macro': f1_marco,\n",
    "         'F1-Weighted': f1_weighted,\n",
    "         'F1_Class 0': f1_non_avg[0],\n",
    "         'F1_Class 1': f1_non_avg[1],\n",
    "         'F1_Class 2': f1_non_avg[2],\n",
    "        }\n",
    "    print(r)\n",
    "    return [r]\n",
    "\n",
    "def run_experiment_org(pipeline, n_splits=2):\n",
    "    out_lst = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=None)\n",
    "    for splt_idx, (train_index , test_index) in enumerate(skf.split(X, y)):\n",
    "        # print(splt_idx)\n",
    "        x_train , x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        training_target , testing_target = y.iloc[train_index] , y.iloc[test_index]\n",
    "\n",
    "        org = __exp__(pipeline, x_train, x_test, training_target, testing_target, CF=False)\n",
    "\n",
    "        l = [('splt_idx', splt_idx, len(x_test)),  ('train', len(x_train)), {'Org': org}]\n",
    "        out_lst.append(l)\n",
    "    return out_lst\n",
    "\n",
    "def run_experiment_counter_factuals(pipeline, n_splits=2, cf_size_prop_to_data=0.1):\n",
    "    out_lst = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=None)\n",
    "    for splt_idx, (train_index , test_index) in enumerate(skf.split(X, y)):\n",
    "        # print(splt_idx)\n",
    "        x_train , x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        training_target , testing_target = y.iloc[train_index] , y.iloc[test_index]\n",
    "\n",
    "        # org = __exp__(pipeline, x_train, x_test, training_target, testing_target, CF=False)\n",
    "\n",
    "        x_train_with_cf, training_with_cf_target = get_counterfactual_tweets(\n",
    "            x_train, training_target, cf_label=CF_LABEL, single_cf_per_tweet=True, cf_size_prop_to_data=cf_size_prop_to_data, only_tweets_with_offensive_lexicon=only_tweets_with_offensive_lexicon)\n",
    "\n",
    "        cf = __exp__(pipeline, x_train_with_cf, x_test, training_with_cf_target, testing_target , CF=True)\n",
    "        l = [('splt_idx', splt_idx, len(x_test)),  ('train', len(x_train_with_cf)), {'CF': cf}]\n",
    "        out_lst.append(l)\n",
    "    return out_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7d6915d-0eaf-44e3-9e27-6d206d6dcac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender (9454, 4)\n",
      "0\n",
      "(6302,) (6302,) (3152,) (3152,)\n",
      "Religion (10869, 4)\n",
      "0\n",
      "(7246,) (7246,) (3623,) (3623,)\n",
      "Race (12013, 4)\n",
      "0\n",
      "(8008,) (8008,) (4005,) (4005,)\n",
      "Politics (11018, 4)\n",
      "0\n",
      "(7345,) (7345,) (3673,) (3673,)\n",
      "Sports (12306, 4)\n",
      "0\n",
      "(8204,) (8204,) (4102,) (4102,)\n"
     ]
    }
   ],
   "source": [
    "for d in domains:\n",
    "    sel_df = df[df['Domain'] == d]\n",
    "    print(d, sel_df.shape)\n",
    "    X, y = sel_df['Tweet'], sel_df['Label'].astype(int)\n",
    "    skf = StratifiedKFold(n_splits=3, random_state=None)\n",
    "    for splt_idx, (train_index , test_index) in enumerate(skf.split(X, y)):\n",
    "        print(splt_idx)\n",
    "        X_train , X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        training_target , testing_target = y.iloc[train_index] , y.iloc[test_index]\n",
    "        print(X_train.shape, training_target.shape, X_test.shape, testing_target.shape)\n",
    "        break\n",
    "        _,_ = get_counterfactual_tweets(\n",
    "            x_train, training_target, cf_label=CF_LABEL, single_cf_per_tweet=True, cf_size_prop_to_data=cf_size_prop_to_data, only_tweets_with_offensive_lexicon=only_tweets_with_offensive_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76a24f-77b9-4240-a1c8-1f2e5d710f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "960d5457-796b-4789-bbb2-236404c99470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "cf_size_prop_to_data_lst = list(np.arange(0.1, 1.1, 0.1))\n",
    "# n_splits=2\n",
    "# cf_size_prop_to_data_lst = list(np.arange(0.1, 0.3, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49dd3e6c-c187-4c0f-b9ca-bf5b6e1d9dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender (9454, 4)\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.8677948175568482, 'F1-Macro': 0.5809316618755357, 'F1-Weighted': 0.8507515378499279, 'F1_Class 0': 0.9249190938511327, 'F1_Class 1': 0.6439628482972135, 'F1_Class 2': 0.1739130434782609}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.8958223162347964, 'F1-Macro': 0.6021672042777194, 'F1-Weighted': 0.8818566852903429, 'F1_Class 0': 0.937724160417346, 'F1_Class 1': 0.7437774524158125, 'F1_Class 2': 0.125}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.9439450026441036, 'F1-Macro': 0.6939388704506445, 'F1-Weighted': 0.938223027421659, 'F1_Class 0': 0.9661933739012847, 'F1_Class 1': 0.8934010152284265, 'F1_Class 2': 0.2222222222222222}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.9481755684822845, 'F1-Macro': 0.6656443948269043, 'F1-Weighted': 0.9408933346703915, 'F1_Class 0': 0.9682432432432433, 'F1_Class 1': 0.9074778200253485, 'F1_Class 2': 0.12121212121212122}\n",
      "> Train samples 7564\n",
      "{'Accuracy': 0.8174603174603174, 'F1-Macro': 0.5270614631141506, 'F1-Weighted': 0.8240735718103097, 'F1_Class 0': 0.875717017208413, 'F1_Class 1': 0.7054673721340388, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 132 at rate 0.1\n",
      "> Train with CF samples 7695\n",
      "{'Accuracy': 0.866737176097303, 'F1-Macro': 0.5553626333875927, 'F1-Weighted': 0.8470146341022515, 'F1_Class 0': 0.9233247422680413, 'F1_Class 1': 0.6375, 'F1_Class 2': 0.10526315789473685}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 128 at rate 0.1\n",
      "> Train with CF samples 7691\n",
      "{'Accuracy': 0.8952934955050238, 'F1-Macro': 0.6489720122148798, 'F1-Weighted': 0.8830414083659166, 'F1_Class 0': 0.9376428338230492, 'F1_Class 1': 0.7390029325513198, 'F1_Class 2': 0.2702702702702703}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 131 at rate 0.1\n",
      "> Train with CF samples 7694\n",
      "{'Accuracy': 0.9423585404547858, 'F1-Macro': 0.6752143914386418, 'F1-Weighted': 0.9359074855045975, 'F1_Class 0': 0.9656102494942683, 'F1_Class 1': 0.8886043533930857, 'F1_Class 2': 0.17142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 129 at rate 0.1\n",
      "> Train with CF samples 7692\n",
      "{'Accuracy': 0.9413008989952406, 'F1-Macro': 0.6555204059020854, 'F1-Weighted': 0.9344103283975156, 'F1_Class 0': 0.9648648648648649, 'F1_Class 1': 0.89058524173028, 'F1_Class 2': 0.1111111111111111}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 126 at rate 0.1\n",
      "> Train with CF samples 7690\n",
      "{'Accuracy': 0.8380952380952381, 'F1-Macro': 0.5399407780538442, 'F1-Weighted': 0.841741405425419, 'F1_Class 0': 0.8925496068888057, 'F1_Class 1': 0.7272727272727272, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 263 at rate 0.2\n",
      "> Train with CF samples 7826\n",
      "{'Accuracy': 0.8672659968270756, 'F1-Macro': 0.568700804692899, 'F1-Weighted': 0.8490997166224651, 'F1_Class 0': 0.9238218205293737, 'F1_Class 1': 0.6427457098283931, 'F1_Class 2': 0.13953488372093023}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 255 at rate 0.2\n",
      "> Train with CF samples 7818\n",
      "{'Accuracy': 0.8926493918561608, 'F1-Macro': 0.6495368389949411, 'F1-Weighted': 0.8792933677711451, 'F1_Class 0': 0.9358932639114872, 'F1_Class 1': 0.7270029673590505, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 262 at rate 0.2\n",
      "> Train with CF samples 7825\n",
      "{'Accuracy': 0.9465891062929667, 'F1-Macro': 0.661058510947454, 'F1-Weighted': 0.9398930061602369, 'F1_Class 0': 0.9686340640809444, 'F1_Class 1': 0.9002557544757034, 'F1_Class 2': 0.1142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 258 at rate 0.2\n",
      "> Train with CF samples 7821\n",
      "{'Accuracy': 0.9397144368059228, 'F1-Macro': 0.6553816225836548, 'F1-Weighted': 0.9322865599491922, 'F1_Class 0': 0.9635873229939312, 'F1_Class 1': 0.8849104859335039, 'F1_Class 2': 0.1176470588235294}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 251 at rate 0.2\n",
      "> Train with CF samples 7815\n",
      "{'Accuracy': 0.8354497354497354, 'F1-Macro': 0.5385539146304698, 'F1-Weighted': 0.8395623264583544, 'F1_Class 0': 0.8902255639097745, 'F1_Class 1': 0.7254361799816346, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 395 at rate 0.30000000000000004\n",
      "> Train with CF samples 7958\n",
      "{'Accuracy': 0.8709677419354839, 'F1-Macro': 0.5643411456379136, 'F1-Weighted': 0.852229786868, 'F1_Class 0': 0.9253634894991922, 'F1_Class 1': 0.6533742331288344, 'F1_Class 2': 0.1142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 383 at rate 0.30000000000000004\n",
      "> Train with CF samples 7946\n",
      "{'Accuracy': 0.8979375991538868, 'F1-Macro': 0.6703130939948702, 'F1-Weighted': 0.8867278750498893, 'F1_Class 0': 0.9387888707037643, 'F1_Class 1': 0.7478260869565218, 'F1_Class 2': 0.32432432432432434}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 394 at rate 0.30000000000000004\n",
      "> Train with CF samples 7957\n",
      "{'Accuracy': 0.9402432575356954, 'F1-Macro': 0.6720334304780495, 'F1-Weighted': 0.9342903894621357, 'F1_Class 0': 0.9647218453188603, 'F1_Class 1': 0.8847117794486216, 'F1_Class 2': 0.16666666666666669}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 387 at rate 0.30000000000000004\n",
      "> Train with CF samples 7950\n",
      "{'Accuracy': 0.940772078265468, 'F1-Macro': 0.6569774256115712, 'F1-Weighted': 0.933633304806042, 'F1_Class 0': 0.964116452268111, 'F1_Class 1': 0.8891687657430731, 'F1_Class 2': 0.1176470588235294}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 377 at rate 0.30000000000000004\n",
      "> Train with CF samples 7941\n",
      "{'Accuracy': 0.8306878306878307, 'F1-Macro': 0.5355689828801611, 'F1-Weighted': 0.8356744513179557, 'F1_Class 0': 0.8867069486404834, 'F1_Class 1': 0.7199999999999999, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 526 at rate 0.4\n",
      "> Train with CF samples 8089\n",
      "{'Accuracy': 0.8699101004759386, 'F1-Macro': 0.5602859240671084, 'F1-Weighted': 0.8513902104119387, 'F1_Class 0': 0.9253634894991922, 'F1_Class 1': 0.650231124807396, 'F1_Class 2': 0.10526315789473685}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 510 at rate 0.4\n",
      "> Train with CF samples 8073\n",
      "{'Accuracy': 0.8947646747752512, 'F1-Macro': 0.6534241082441284, 'F1-Weighted': 0.8824483219525143, 'F1_Class 0': 0.9369487095720354, 'F1_Class 1': 0.7376093294460642, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 525 at rate 0.4\n",
      "> Train with CF samples 8088\n",
      "{'Accuracy': 0.9439450026441036, 'F1-Macro': 0.6753110753785063, 'F1-Weighted': 0.93768215671521, 'F1_Class 0': 0.96695886716116, 'F1_Class 1': 0.8923076923076922, 'F1_Class 2': 0.16666666666666669}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 516 at rate 0.4\n",
      "> Train with CF samples 8079\n",
      "{'Accuracy': 0.9413008989952406, 'F1-Macro': 0.6556050547102945, 'F1-Weighted': 0.9344537577535306, 'F1_Class 0': 0.964841108857336, 'F1_Class 1': 0.8908629441624365, 'F1_Class 2': 0.1111111111111111}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 502 at rate 0.4\n",
      "> Train with CF samples 8066\n",
      "{'Accuracy': 0.8417989417989418, 'F1-Macro': 0.5418364122882315, 'F1-Weighted': 0.8446938522108453, 'F1_Class 0': 0.8956780923994038, 'F1_Class 1': 0.7298311444652908, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 658 at rate 0.5\n",
      "> Train with CF samples 8221\n",
      "{'Accuracy': 0.8582760444209413, 'F1-Macro': 0.5419158052232216, 'F1-Weighted': 0.8356739262021619, 'F1_Class 0': 0.9188842577749279, 'F1_Class 1': 0.6016, 'F1_Class 2': 0.10526315789473685}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 638 at rate 0.5\n",
      "> Train with CF samples 8201\n",
      "{'Accuracy': 0.8947646747752512, 'F1-Macro': 0.6652918431825967, 'F1-Weighted': 0.882604572224254, 'F1_Class 0': 0.9373368146214099, 'F1_Class 1': 0.7342143906020558, 'F1_Class 2': 0.32432432432432434}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 656 at rate 0.5\n",
      "> Train with CF samples 8219\n",
      "{'Accuracy': 0.9476467477525119, 'F1-Macro': 0.68091152898378, 'F1-Weighted': 0.9414786587178079, 'F1_Class 0': 0.968897903989182, 'F1_Class 1': 0.9024081115335868, 'F1_Class 2': 0.17142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 644 at rate 0.5\n",
      "> Train with CF samples 8207\n",
      "{'Accuracy': 0.9476467477525119, 'F1-Macro': 0.6647994434116108, 'F1-Weighted': 0.9403316720955507, 'F1_Class 0': 0.9682432432432433, 'F1_Class 1': 0.9049429657794678, 'F1_Class 2': 0.12121212121212122}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 628 at rate 0.5\n",
      "> Train with CF samples 8192\n",
      "{'Accuracy': 0.8328042328042328, 'F1-Macro': 0.5370929736452758, 'F1-Weighted': 0.8372777352533425, 'F1_Class 0': 0.887797506611258, 'F1_Class 1': 0.7234814143245694, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 789 at rate 0.6\n",
      "> Train with CF samples 8352\n",
      "{'Accuracy': 0.8582760444209413, 'F1-Macro': 0.5430373272301524, 'F1-Weighted': 0.8350853294233312, 'F1_Class 0': 0.9186418962203715, 'F1_Class 1': 0.5993589743589745, 'F1_Class 2': 0.1111111111111111}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 766 at rate 0.6\n",
      "> Train with CF samples 8329\n",
      "{'Accuracy': 0.8958223162347964, 'F1-Macro': 0.6541335806399218, 'F1-Weighted': 0.8833174502484029, 'F1_Class 0': 0.9376835236541597, 'F1_Class 1': 0.7390029325513198, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 787 at rate 0.6\n",
      "> Train with CF samples 8350\n",
      "{'Accuracy': 0.9450026441036489, 'F1-Macro': 0.6608476048989096, 'F1-Weighted': 0.9381427125581037, 'F1_Class 0': 0.9669365721997301, 'F1_Class 1': 0.8979591836734694, 'F1_Class 2': 0.11764705882352941}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 773 at rate 0.6\n",
      "> Train with CF samples 8336\n",
      "{'Accuracy': 0.9370703331570598, 'F1-Macro': 0.651723571626778, 'F1-Weighted': 0.9298020035543562, 'F1_Class 0': 0.9622132253711201, 'F1_Class 1': 0.8786717752234994, 'F1_Class 2': 0.1142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 754 at rate 0.6\n",
      "> Train with CF samples 8318\n",
      "{'Accuracy': 0.8391534391534392, 'F1-Macro': 0.5396602786988385, 'F1-Weighted': 0.8423033164308202, 'F1_Class 0': 0.893933755117231, 'F1_Class 1': 0.7250470809792844, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 921 at rate 0.7000000000000001\n",
      "> Train with CF samples 8484\n",
      "{'Accuracy': 0.8693812797461661, 'F1-Macro': 0.5572404289416668, 'F1-Weighted': 0.8506621998018259, 'F1_Class 0': 0.9257585539057456, 'F1_Class 1': 0.6459627329192547, 'F1_Class 2': 0.1}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 893 at rate 0.7000000000000001\n",
      "> Train with CF samples 8456\n",
      "{'Accuracy': 0.896351136964569, 'F1-Macro': 0.6548510121930641, 'F1-Weighted': 0.8839599053996391, 'F1_Class 0': 0.9379895561357703, 'F1_Class 1': 0.7408491947291361, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 918 at rate 0.7000000000000001\n",
      "> Train with CF samples 8481\n",
      "{'Accuracy': 0.9354838709677419, 'F1-Macro': 0.6670400895159226, 'F1-Weighted': 0.9290693683352377, 'F1_Class 0': 0.9612141652613829, 'F1_Class 1': 0.8732394366197184, 'F1_Class 2': 0.16666666666666669}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 902 at rate 0.7000000000000001\n",
      "> Train with CF samples 8465\n",
      "{'Accuracy': 0.9423585404547858, 'F1-Macro': 0.659300444576104, 'F1-Weighted': 0.9347388704395131, 'F1_Class 0': 0.9646583641871423, 'F1_Class 1': 0.8920308483290488, 'F1_Class 2': 0.12121212121212122}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 879 at rate 0.7000000000000001\n",
      "> Train with CF samples 8443\n",
      "{'Accuracy': 0.8465608465608465, 'F1-Macro': 0.5439838528773586, 'F1-Weighted': 0.8484391412532531, 'F1_Class 0': 0.8999630860095976, 'F1_Class 1': 0.7319884726224782, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1052 at rate 0.8\n",
      "> Train with CF samples 8615\n",
      "{'Accuracy': 0.86409307244844, 'F1-Macro': 0.563012775137358, 'F1-Weighted': 0.8452996318361737, 'F1_Class 0': 0.9225806451612903, 'F1_Class 1': 0.6300940438871474, 'F1_Class 2': 0.13636363636363638}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1021 at rate 0.8\n",
      "> Train with CF samples 8584\n",
      "{'Accuracy': 0.8900052882072977, 'F1-Macro': 0.5935519321711716, 'F1-Weighted': 0.8752144234700796, 'F1_Class 0': 0.9345916042954767, 'F1_Class 1': 0.7248520710059172, 'F1_Class 2': 0.1212121212121212}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1050 at rate 0.8\n",
      "> Train with CF samples 8613\n",
      "{'Accuracy': 0.9439450026441036, 'F1-Macro': 0.675728214468558, 'F1-Weighted': 0.9378997513470997, 'F1_Class 0': 0.9668470906630581, 'F1_Class 1': 0.8936708860759495, 'F1_Class 2': 0.16666666666666669}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1031 at rate 0.8\n",
      "> Train with CF samples 8594\n",
      "{'Accuracy': 0.9397144368059228, 'F1-Macro': 0.655017753805515, 'F1-Weighted': 0.9320976235126336, 'F1_Class 0': 0.9636852723604572, 'F1_Class 1': 0.8837209302325582, 'F1_Class 2': 0.1176470588235294}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1005 at rate 0.8\n",
      "> Train with CF samples 8569\n",
      "{'Accuracy': 0.8407407407407408, 'F1-Macro': 0.5408670748861665, 'F1-Weighted': 0.8439529646377678, 'F1_Class 0': 0.8955001859427297, 'F1_Class 1': 0.7271010387157697, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1184 at rate 0.9\n",
      "> Train with CF samples 8747\n",
      "{'Accuracy': 0.86409307244844, 'F1-Macro': 0.5627006233176503, 'F1-Weighted': 0.8456884648770404, 'F1_Class 0': 0.9225306649451258, 'F1_Class 1': 0.6322378716744914, 'F1_Class 2': 0.13333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1148 at rate 0.9\n",
      "> Train with CF samples 8711\n",
      "{'Accuracy': 0.8915917503966155, 'F1-Macro': 0.6411166182789928, 'F1-Weighted': 0.878478644107244, 'F1_Class 0': 0.936156351791531, 'F1_Class 1': 0.7240356083086054, 'F1_Class 2': 0.2631578947368421}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1181 at rate 0.9\n",
      "> Train with CF samples 8744\n",
      "{'Accuracy': 0.9354838709677419, 'F1-Macro': 0.6503063838730836, 'F1-Weighted': 0.9280996635897157, 'F1_Class 0': 0.9613445378151261, 'F1_Class 1': 0.8719275549805952, 'F1_Class 2': 0.11764705882352941}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1160 at rate 0.9\n",
      "> Train with CF samples 8723\n",
      "{'Accuracy': 0.9428873611845584, 'F1-Macro': 0.6584448242697146, 'F1-Weighted': 0.9354410869299377, 'F1_Class 0': 0.9656565656565657, 'F1_Class 1': 0.8920308483290488, 'F1_Class 2': 0.1176470588235294}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1130 at rate 0.9\n",
      "> Train with CF samples 8694\n",
      "{'Accuracy': 0.846031746031746, 'F1-Macro': 0.5431695512756677, 'F1-Weighted': 0.8477979822121311, 'F1_Class 0': 0.8997789240972733, 'F1_Class 1': 0.7297297297297297, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1315 at rate 1.0\n",
      "> Train with CF samples 8878\n",
      "{'Accuracy': 0.8646218931782126, 'F1-Macro': 0.5522514639778099, 'F1-Weighted': 0.844437223390748, 'F1_Class 0': 0.9223832528180355, 'F1_Class 1': 0.6291079812206573, 'F1_Class 2': 0.10526315789473685}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1276 at rate 1.0\n",
      "> Train with CF samples 8839\n",
      "{'Accuracy': 0.88841882601798, 'F1-Macro': 0.6372294668964077, 'F1-Weighted': 0.8748606243845335, 'F1_Class 0': 0.9342447916666667, 'F1_Class 1': 0.7142857142857143, 'F1_Class 2': 0.2631578947368421}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1312 at rate 1.0\n",
      "> Train with CF samples 8875\n",
      "{'Accuracy': 0.9413008989952406, 'F1-Macro': 0.6727464038883423, 'F1-Weighted': 0.9350441036753949, 'F1_Class 0': 0.9652379345258185, 'F1_Class 1': 0.8863346104725415, 'F1_Class 2': 0.16666666666666669}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1289 at rate 1.0\n",
      "> Train with CF samples 8852\n",
      "{'Accuracy': 0.9428873611845584, 'F1-Macro': 0.6573476679411367, 'F1-Weighted': 0.9355896231338227, 'F1_Class 0': 0.9660047122181084, 'F1_Class 1': 0.8917525773195877, 'F1_Class 2': 0.1142857142857143}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1256 at rate 1.0\n",
      "> Train with CF samples 8820\n",
      "{'Accuracy': 0.8333333333333334, 'F1-Macro': 0.5362876705690134, 'F1-Weighted': 0.8375577492038658, 'F1_Class 0': 0.8893058161350845, 'F1_Class 1': 0.7195571955719556, 'F1_Class 2': 0.0}\n",
      "Religion (10869, 4)\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.8666053357865685, 'F1-Macro': 0.5718483675189061, 'F1-Weighted': 0.8427980545127222, 'F1_Class 0': 0.9276663146779304, 'F1_Class 1': 0.47619047619047616, 'F1_Class 2': 0.3116883116883117}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.9199632014719411, 'F1-Macro': 0.6808985269891208, 'F1-Weighted': 0.9107248196814348, 'F1_Class 0': 0.9554896142433235, 'F1_Class 1': 0.7538726333907056, 'F1_Class 2': 0.3333333333333333}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.9287028518859246, 'F1-Macro': 0.6742933422981459, 'F1-Weighted': 0.9203208026502178, 'F1_Class 0': 0.9590230664857532, 'F1_Class 1': 0.8045977011494254, 'F1_Class 2': 0.2592592592592593}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.9415823367065317, 'F1-Macro': 0.7094742909136241, 'F1-Weighted': 0.9343534573177972, 'F1_Class 0': 0.9662125340599454, 'F1_Class 1': 0.8484848484848484, 'F1_Class 2': 0.3137254901960785}\n",
      "> Train samples 8696\n",
      "{'Accuracy': 0.7616198803497469, 'F1-Macro': 0.48300735379874654, 'F1-Weighted': 0.7837447033132992, 'F1_Class 0': 0.8456162642947903, 'F1_Class 1': 0.5617391304347826, 'F1_Class 2': 0.04166666666666667}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 123 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.8633854645814167, 'F1-Macro': 0.5549327569007613, 'F1-Weighted': 0.8367400860218602, 'F1_Class 0': 0.9253024723829564, 'F1_Class 1': 0.45378151260504196, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8817\n",
      "{'Accuracy': 0.9195032198712052, 'F1-Macro': 0.6897524614061634, 'F1-Weighted': 0.9114805576251782, 'F1_Class 0': 0.9547548089948523, 'F1_Class 1': 0.7596638655462185, 'F1_Class 2': 0.3548387096774194}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 123 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.9259429622815087, 'F1-Macro': 0.6484410122576795, 'F1-Weighted': 0.9156662142560588, 'F1_Class 0': 0.9573664328116568, 'F1_Class 1': 0.7918781725888325, 'F1_Class 2': 0.19607843137254904}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8817\n",
      "{'Accuracy': 0.9328426862925483, 'F1-Macro': 0.6572234229106196, 'F1-Weighted': 0.9238704657290637, 'F1_Class 0': 0.9617989704687077, 'F1_Class 1': 0.8211920529801324, 'F1_Class 2': 0.18867924528301888}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.7606994937873907, 'F1-Macro': 0.4833732274756093, 'F1-Weighted': 0.783635547643365, 'F1_Class 0': 0.8448275862068967, 'F1_Class 1': 0.5652920962199313, 'F1_Class 2': 0.04}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 246 at rate 0.2\n",
      "> Train with CF samples 8941\n",
      "{'Accuracy': 0.8666053357865685, 'F1-Macro': 0.5720528133651909, 'F1-Weighted': 0.8431135547623884, 'F1_Class 0': 0.9271383315733895, 'F1_Class 1': 0.4813278008298754, 'F1_Class 2': 0.3076923076923077}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 245 at rate 0.2\n",
      "> Train with CF samples 8940\n",
      "{'Accuracy': 0.9167433302667893, 'F1-Macro': 0.6618833368095904, 'F1-Weighted': 0.9051786875219069, 'F1_Class 0': 0.9530705282917671, 'F1_Class 1': 0.736283185840708, 'F1_Class 2': 0.2962962962962963}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 247 at rate 0.2\n",
      "> Train with CF samples 8942\n",
      "{'Accuracy': 0.9287028518859246, 'F1-Macro': 0.6302485116884182, 'F1-Weighted': 0.918080331612891, 'F1_Class 0': 0.9591339648173207, 'F1_Class 1': 0.8066115702479338, 'F1_Class 2': 0.125}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 244 at rate 0.2\n",
      "> Train with CF samples 8939\n",
      "{'Accuracy': 0.937442502299908, 'F1-Macro': 0.6746765619181, 'F1-Weighted': 0.9294556289590618, 'F1_Class 0': 0.9646739130434783, 'F1_Class 1': 0.8371335504885994, 'F1_Class 2': 0.22222222222222218}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 245 at rate 0.2\n",
      "> Train with CF samples 8941\n",
      "{'Accuracy': 0.7685227795674183, 'F1-Macro': 0.487737910296566, 'F1-Weighted': 0.789975691477404, 'F1_Class 0': 0.8516169942929613, 'F1_Class 1': 0.5699300699300699, 'F1_Class 2': 0.04166666666666667}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 369 at rate 0.30000000000000004\n",
      "> Train with CF samples 9064\n",
      "{'Accuracy': 0.8647654093836247, 'F1-Macro': 0.5462814942504801, 'F1-Weighted': 0.8363431524501204, 'F1_Class 0': 0.9260230849947535, 'F1_Class 1': 0.4505263157894737, 'F1_Class 2': 0.2622950819672131}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9062\n",
      "{'Accuracy': 0.9149034038638455, 'F1-Macro': 0.658853815173762, 'F1-Weighted': 0.9030263545439988, 'F1_Class 0': 0.9520235861699275, 'F1_Class 1': 0.7282415630550622, 'F1_Class 2': 0.2962962962962963}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 370 at rate 0.30000000000000004\n",
      "> Train with CF samples 9065\n",
      "{'Accuracy': 0.9264029438822448, 'F1-Macro': 0.6596332426127166, 'F1-Weighted': 0.916373174314264, 'F1_Class 0': 0.9576705311404692, 'F1_Class 1': 0.7904599659284497, 'F1_Class 2': 0.23076923076923078}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9062\n",
      "{'Accuracy': 0.9346826126954921, 'F1-Macro': 0.6833080051476162, 'F1-Weighted': 0.9264846867204474, 'F1_Class 0': 0.9628222523744911, 'F1_Class 1': 0.8229508196721311, 'F1_Class 2': 0.2641509433962264}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9063\n",
      "{'Accuracy': 0.747814081914404, 'F1-Macro': 0.46130849057149637, 'F1-Weighted': 0.7721066164241271, 'F1_Class 0': 0.8349389852280026, 'F1_Class 1': 0.5489864864864865, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 492 at rate 0.4\n",
      "> Train with CF samples 9187\n",
      "{'Accuracy': 0.8647654093836247, 'F1-Macro': 0.5683114937335455, 'F1-Weighted': 0.8367660993459033, 'F1_Class 0': 0.9257412752558383, 'F1_Class 1': 0.44585987261146487, 'F1_Class 2': 0.33333333333333337}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 489 at rate 0.4\n",
      "> Train with CF samples 9184\n",
      "{'Accuracy': 0.9195032198712052, 'F1-Macro': 0.6841756855596614, 'F1-Weighted': 0.9098535386925871, 'F1_Class 0': 0.9542272482498653, 'F1_Class 1': 0.7534722222222223, 'F1_Class 2': 0.3448275862068966}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 494 at rate 0.4\n",
      "> Train with CF samples 9189\n",
      "{'Accuracy': 0.9277828886844526, 'F1-Macro': 0.6497428677331433, 'F1-Weighted': 0.9178639752338198, 'F1_Class 0': 0.9589411129119395, 'F1_Class 1': 0.797979797979798, 'F1_Class 2': 0.19230769230769232}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 489 at rate 0.4\n",
      "> Train with CF samples 9184\n",
      "{'Accuracy': 0.9282428702851886, 'F1-Macro': 0.6298222275441506, 'F1-Weighted': 0.9170458773271626, 'F1_Class 0': 0.9584457636265517, 'F1_Class 1': 0.803361344537815, 'F1_Class 2': 0.1276595744680851}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 490 at rate 0.4\n",
      "> Train with CF samples 9186\n",
      "{'Accuracy': 0.786470317533364, 'F1-Macro': 0.4994249865169253, 'F1-Weighted': 0.8046097888444133, 'F1_Class 0': 0.8652882205513784, 'F1_Class 1': 0.5913200723327305, 'F1_Class 2': 0.04166666666666667}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 614 at rate 0.5\n",
      "> Train with CF samples 9309\n",
      "{'Accuracy': 0.8610855565777369, 'F1-Macro': 0.5245318186272157, 'F1-Weighted': 0.8290453481516, 'F1_Class 0': 0.9231974921630093, 'F1_Class 1': 0.423982869379015, 'F1_Class 2': 0.22641509433962265}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 612 at rate 0.5\n",
      "> Train with CF samples 9307\n",
      "{'Accuracy': 0.9130634774609016, 'F1-Macro': 0.6370364048093965, 'F1-Weighted': 0.9009672624918946, 'F1_Class 0': 0.9508460918614021, 'F1_Class 1': 0.7294938917975567, 'F1_Class 2': 0.23076923076923078}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 617 at rate 0.5\n",
      "> Train with CF samples 9312\n",
      "{'Accuracy': 0.9231830726770929, 'F1-Macro': 0.6403078390717484, 'F1-Weighted': 0.912921105487078, 'F1_Class 0': 0.9568267674042094, 'F1_Class 1': 0.7789115646258504, 'F1_Class 2': 0.18518518518518515}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 611 at rate 0.5\n",
      "> Train with CF samples 9306\n",
      "{'Accuracy': 0.9305427782888684, 'F1-Macro': 0.6782317818090361, 'F1-Weighted': 0.9222324584639768, 'F1_Class 0': 0.9600868856910127, 'F1_Class 1': 0.8104575163398694, 'F1_Class 2': 0.2641509433962264}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 612 at rate 0.5\n",
      "> Train with CF samples 9308\n",
      "{'Accuracy': 0.805798435342844, 'F1-Macro': 0.5097394790992767, 'F1-Weighted': 0.8190787929567604, 'F1_Class 0': 0.8799019607843137, 'F1_Class 1': 0.6067632850241547, 'F1_Class 2': 0.04255319148936171}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 737 at rate 0.6\n",
      "> Train with CF samples 9432\n",
      "{'Accuracy': 0.8684452621895125, 'F1-Macro': 0.5688550924939592, 'F1-Weighted': 0.8446641779007819, 'F1_Class 0': 0.9287598944591029, 'F1_Class 1': 0.484472049689441, 'F1_Class 2': 0.29333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 734 at rate 0.6\n",
      "> Train with CF samples 9429\n",
      "{'Accuracy': 0.9172033118675254, 'F1-Macro': 0.6523630680651229, 'F1-Weighted': 0.9060816655698823, 'F1_Class 0': 0.9534821188491529, 'F1_Class 1': 0.7443478260869564, 'F1_Class 2': 0.2592592592592593}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 740 at rate 0.6\n",
      "> Train with CF samples 9435\n",
      "{'Accuracy': 0.9245630174793008, 'F1-Macro': 0.6347246872716158, 'F1-Weighted': 0.9132116157940932, 'F1_Class 0': 0.9566621803499328, 'F1_Class 1': 0.7842465753424657, 'F1_Class 2': 0.16326530612244897}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 733 at rate 0.6\n",
      "> Train with CF samples 9428\n",
      "{'Accuracy': 0.9356025758969642, 'F1-Macro': 0.6287359972406421, 'F1-Weighted': 0.9247450900982477, 'F1_Class 0': 0.9626218851570965, 'F1_Class 1': 0.8346972176759411, 'F1_Class 2': 0.08888888888888888}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 734 at rate 0.6\n",
      "> Train with CF samples 9430\n",
      "{'Accuracy': 0.7942936033133916, 'F1-Macro': 0.48786598612382576, 'F1-Weighted': 0.8097061299172736, 'F1_Class 0': 0.8726035868893012, 'F1_Class 1': 0.5909943714821763, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 860 at rate 0.7000000000000001\n",
      "> Train with CF samples 9555\n",
      "{'Accuracy': 0.8647654093836247, 'F1-Macro': 0.5380342818416669, 'F1-Weighted': 0.8346579383055627, 'F1_Class 0': 0.9259356189479195, 'F1_Class 1': 0.44255319148936173, 'F1_Class 2': 0.24561403508771928}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 856 at rate 0.7000000000000001\n",
      "> Train with CF samples 9551\n",
      "{'Accuracy': 0.906163753449862, 'F1-Macro': 0.6002102189781022, 'F1-Weighted': 0.8907745767833523, 'F1_Class 0': 0.9472000000000002, 'F1_Class 1': 0.6934306569343065, 'F1_Class 2': 0.16}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 864 at rate 0.7000000000000001\n",
      "> Train with CF samples 9559\n",
      "{'Accuracy': 0.9167433302667893, 'F1-Macro': 0.6214770680038297, 'F1-Weighted': 0.9037769119499836, 'F1_Class 0': 0.9520492901151887, 'F1_Class 1': 0.7491166077738516, 'F1_Class 2': 0.16326530612244897}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 855 at rate 0.7000000000000001\n",
      "> Train with CF samples 9550\n",
      "{'Accuracy': 0.9346826126954921, 'F1-Macro': 0.6824965751311658, 'F1-Weighted': 0.9267569653132359, 'F1_Class 0': 0.9627818527574028, 'F1_Class 1': 0.8254486133768353, 'F1_Class 2': 0.2592592592592593}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 857 at rate 0.7000000000000001\n",
      "> Train with CF samples 9553\n",
      "{'Accuracy': 0.7722043258168431, 'F1-Macro': 0.4890944626454115, 'F1-Weighted': 0.7924899049173193, 'F1_Class 0': 0.8544423440453686, 'F1_Class 1': 0.5711743772241993, 'F1_Class 2': 0.04166666666666667}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 983 at rate 0.8\n",
      "> Train with CF samples 9678\n",
      "{'Accuracy': 0.8620055197792088, 'F1-Macro': 0.5288879021452546, 'F1-Weighted': 0.831640464519002, 'F1_Class 0': 0.9246073298429319, 'F1_Class 1': 0.43254817987152033, 'F1_Class 2': 0.2295081967213115}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 978 at rate 0.8\n",
      "> Train with CF samples 9673\n",
      "{'Accuracy': 0.9070837166513339, 'F1-Macro': 0.6250274939251996, 'F1-Weighted': 0.8923129787853935, 'F1_Class 0': 0.9474806718208477, 'F1_Class 1': 0.6923076923076923, 'F1_Class 2': 0.2352941176470588}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 987 at rate 0.8\n",
      "> Train with CF samples 9682\n",
      "{'Accuracy': 0.9176632934682613, 'F1-Macro': 0.6226697046020717, 'F1-Weighted': 0.9052847226005634, 'F1_Class 0': 0.9527643585614601, 'F1_Class 1': 0.7552447552447551, 'F1_Class 2': 0.16}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 978 at rate 0.8\n",
      "> Train with CF samples 9673\n",
      "{'Accuracy': 0.9356025758969642, 'F1-Macro': 0.6540183926429429, 'F1-Weighted': 0.9259593249483402, 'F1_Class 0': 0.9626016260162601, 'F1_Class 1': 0.8327868852459017, 'F1_Class 2': 0.16666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 979 at rate 0.8\n",
      "> Train with CF samples 9675\n",
      "{'Accuracy': 0.7606994937873907, 'F1-Macro': 0.4814301558093903, 'F1-Weighted': 0.7833455381140338, 'F1_Class 0': 0.8459828516989522, 'F1_Class 1': 0.5574912891986064, 'F1_Class 2': 0.04081632653061225}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 1106 at rate 0.9\n",
      "> Train with CF samples 9801\n",
      "{'Accuracy': 0.8633854645814167, 'F1-Macro': 0.541360176704217, 'F1-Weighted': 0.8322006748492068, 'F1_Class 0': 0.9252483010977521, 'F1_Class 1': 0.427645788336933, 'F1_Class 2': 0.2711864406779661}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 1101 at rate 0.9\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.9047838086476541, 'F1-Macro': 0.6205784371429788, 'F1-Weighted': 0.8893339470950861, 'F1_Class 0': 0.9462193823216187, 'F1_Class 1': 0.6802218114602588, 'F1_Class 2': 0.2352941176470588}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 1111 at rate 0.9\n",
      "> Train with CF samples 9806\n",
      "{'Accuracy': 0.9139834406623735, 'F1-Macro': 0.5915936518612749, 'F1-Weighted': 0.8995427173395782, 'F1_Class 0': 0.9507758159443553, 'F1_Class 1': 0.738898756660746, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 1100 at rate 0.9\n",
      "> Train with CF samples 9795\n",
      "{'Accuracy': 0.9314627414903404, 'F1-Macro': 0.6350649505263286, 'F1-Weighted': 0.9207718930783149, 'F1_Class 0': 0.9602595296025952, 'F1_Class 1': 0.8172757475083057, 'F1_Class 2': 0.1276595744680851}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 1102 at rate 0.9\n",
      "> Train with CF samples 9798\n",
      "{'Accuracy': 0.7657616198803497, 'F1-Macro': 0.4706938974180353, 'F1-Weighted': 0.7863806474736857, 'F1_Class 0': 0.8497474747474747, 'F1_Class 1': 0.5623342175066313, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 1229 at rate 1.0\n",
      "> Train with CF samples 9924\n",
      "{'Accuracy': 0.8698252069917203, 'F1-Macro': 0.5805164001427336, 'F1-Weighted': 0.8457346966595167, 'F1_Class 0': 0.9283078545071165, 'F1_Class 1': 0.48971193415637865, 'F1_Class 2': 0.3235294117647059}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 1223 at rate 1.0\n",
      "> Train with CF samples 9918\n",
      "{'Accuracy': 0.9158233670653174, 'F1-Macro': 0.6405213174015456, 'F1-Weighted': 0.9035934780968601, 'F1_Class 0': 0.9522788203753352, 'F1_Class 1': 0.7385159010600707, 'F1_Class 2': 0.23076923076923078}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 1234 at rate 1.0\n",
      "> Train with CF samples 9929\n",
      "{'Accuracy': 0.9259429622815087, 'F1-Macro': 0.6370535389594949, 'F1-Weighted': 0.9148453290598683, 'F1_Class 0': 0.9574353448275862, 'F1_Class 1': 0.7904599659284497, 'F1_Class 2': 0.16326530612244897}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 1222 at rate 1.0\n",
      "> Train with CF samples 9917\n",
      "{'Accuracy': 0.9328426862925483, 'F1-Macro': 0.6501976986982985, 'F1-Weighted': 0.923043704934463, 'F1_Class 0': 0.9609756097560975, 'F1_Class 1': 0.8229508196721311, 'F1_Class 2': 0.16666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 1224 at rate 1.0\n",
      "> Train with CF samples 9920\n",
      "{'Accuracy': 0.7726645190980211, 'F1-Macro': 0.48878482135466456, 'F1-Weighted': 0.7918713075587844, 'F1_Class 0': 0.8539184952978057, 'F1_Class 1': 0.569882777276826, 'F1_Class 2': 0.04255319148936171}\n",
      "Race (12013, 4)\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9130253849354973, 'F1-Macro': 0.5776766239556602, 'F1-Weighted': 0.8927309017956129, 'F1_Class 0': 0.9543700340522133, 'F1_Class 1': 0.49696969696969695, 'F1_Class 2': 0.2816901408450704}\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9209321681231794, 'F1-Macro': 0.5552954564313832, 'F1-Weighted': 0.9043906468577471, 'F1_Class 0': 0.9588665447897624, 'F1_Class 1': 0.6187845303867403, 'F1_Class 2': 0.08823529411764705}\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9146899708697461, 'F1-Macro': 0.5725240894582565, 'F1-Weighted': 0.8939669266887007, 'F1_Class 0': 0.9564230594643669, 'F1_Class 1': 0.4924924924924925, 'F1_Class 2': 0.26865671641791045}\n",
      "> Train samples 9611\n",
      "{'Accuracy': 0.9367194004995837, 'F1-Macro': 0.6734162262181106, 'F1-Weighted': 0.9291555144907782, 'F1_Class 0': 0.9736780805963197, 'F1_Class 1': 0.6681922196796338, 'F1_Class 2': 0.37837837837837834}\n",
      "> Train samples 9611\n",
      "{'Accuracy': 0.7048293089092423, 'F1-Macro': 0.5266411771268307, 'F1-Weighted': 0.7605786473480043, 'F1_Class 0': 0.8140116763969976, 'F1_Class 1': 0.37881508078994613, 'F1_Class 2': 0.3870967741935484}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 88 at rate 0.1\n",
      "> Train with CF samples 9698\n",
      "{'Accuracy': 0.9088639200998752, 'F1-Macro': 0.5272911260688599, 'F1-Weighted': 0.8851736002605631, 'F1_Class 0': 0.9517772243604257, 'F1_Class 1': 0.4738461538461538, 'F1_Class 2': 0.15625}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 89 at rate 0.1\n",
      "> Train with CF samples 9699\n",
      "{'Accuracy': 0.9192675821889305, 'F1-Macro': 0.5452004005065231, 'F1-Weighted': 0.8996091587079894, 'F1_Class 0': 0.9572727272727274, 'F1_Class 1': 0.5830903790087464, 'F1_Class 2': 0.09523809523809525}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 87 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.9250936329588015, 'F1-Macro': 0.6239997376361014, 'F1-Weighted': 0.9118532226295251, 'F1_Class 0': 0.9632690541781452, 'F1_Class 1': 0.6031746031746033, 'F1_Class 2': 0.3055555555555555}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 86 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.9333888426311407, 'F1-Macro': 0.6162813837851032, 'F1-Weighted': 0.9213102791485489, 'F1_Class 0': 0.9722735674676526, 'F1_Class 1': 0.6341463414634146, 'F1_Class 2': 0.24242424242424243}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 86 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.7194004995836802, 'F1-Macro': 0.5212402463454998, 'F1-Weighted': 0.7702276463960974, 'F1_Class 0': 0.8248835297341738, 'F1_Class 1': 0.38883720930232557, 'F1_Class 2': 0.35}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 176 at rate 0.2\n",
      "> Train with CF samples 9786\n",
      "{'Accuracy': 0.910528506034124, 'F1-Macro': 0.5503539514787988, 'F1-Weighted': 0.8884821881644906, 'F1_Class 0': 0.9526184538653365, 'F1_Class 1': 0.486322188449848, 'F1_Class 2': 0.2121212121212121}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 179 at rate 0.2\n",
      "> Train with CF samples 9789\n",
      "{'Accuracy': 0.9205160216396171, 'F1-Macro': 0.5420883119496374, 'F1-Weighted': 0.9022874458754472, 'F1_Class 0': 0.9587229190421893, 'F1_Class 1': 0.6050420168067226, 'F1_Class 2': 0.0625}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 173 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.9292550977944236, 'F1-Macro': 0.6412586607239222, 'F1-Weighted': 0.9168437461485509, 'F1_Class 0': 0.9657707328279348, 'F1_Class 1': 0.6246719160104987, 'F1_Class 2': 0.3333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 172 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.9254787676935887, 'F1-Macro': 0.630240403910102, 'F1-Weighted': 0.9119593737567336, 'F1_Class 0': 0.9664522058823529, 'F1_Class 1': 0.5631578947368421, 'F1_Class 2': 0.3611111111111111}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 172 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.7081598667776853, 'F1-Macro': 0.5538093567152272, 'F1-Weighted': 0.7637566351769911, 'F1_Class 0': 0.8152264517921645, 'F1_Class 1': 0.38155515370705245, 'F1_Class 2': 0.4646464646464647}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 264 at rate 0.30000000000000004\n",
      "> Train with CF samples 9874\n",
      "{'Accuracy': 0.9113607990012484, 'F1-Macro': 0.5514556875893146, 'F1-Weighted': 0.8896395031378869, 'F1_Class 0': 0.9530718657900702, 'F1_Class 1': 0.4954128440366973, 'F1_Class 2': 0.20588235294117646}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 268 at rate 0.30000000000000004\n",
      "> Train with CF samples 9878\n",
      "{'Accuracy': 0.9151061173533084, 'F1-Macro': 0.5374678420441855, 'F1-Weighted': 0.8948303915681369, 'F1_Class 0': 0.9559491371480472, 'F1_Class 1': 0.5421686746987951, 'F1_Class 2': 0.11428571428571428}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 260 at rate 0.30000000000000004\n",
      "> Train with CF samples 9870\n",
      "{'Accuracy': 0.9300873907615481, 'F1-Macro': 0.6417135296708479, 'F1-Weighted': 0.9178550582288294, 'F1_Class 0': 0.9664522058823529, 'F1_Class 1': 0.6299212598425197, 'F1_Class 2': 0.3287671232876712}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 259 at rate 0.30000000000000004\n",
      "> Train with CF samples 9870\n",
      "{'Accuracy': 0.9267277268942548, 'F1-Macro': 0.5903938255231359, 'F1-Weighted': 0.9119119552782056, 'F1_Class 0': 0.9678160919540231, 'F1_Class 1': 0.5846153846153845, 'F1_Class 2': 0.21875}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 258 at rate 0.30000000000000004\n",
      "> Train with CF samples 9869\n",
      "{'Accuracy': 0.7414654454621149, 'F1-Macro': 0.5558117726819478, 'F1-Weighted': 0.7877982460721367, 'F1_Class 0': 0.8412526997840173, 'F1_Class 1': 0.403960396039604, 'F1_Class 2': 0.4222222222222222}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 352 at rate 0.4\n",
      "> Train with CF samples 9962\n",
      "{'Accuracy': 0.9009571369121931, 'F1-Macro': 0.5001609480320671, 'F1-Weighted': 0.8713701453880967, 'F1_Class 0': 0.9475343391128125, 'F1_Class 1': 0.3654485049833887, 'F1_Class 2': 0.1875}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 357 at rate 0.4\n",
      "> Train with CF samples 9967\n",
      "{'Accuracy': 0.9171868497711194, 'F1-Macro': 0.5374278157565204, 'F1-Weighted': 0.8967268213896515, 'F1_Class 0': 0.9564032697547684, 'F1_Class 1': 0.5621301775147929, 'F1_Class 2': 0.09375}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 346 at rate 0.4\n",
      "> Train with CF samples 9956\n",
      "{'Accuracy': 0.9288389513108615, 'F1-Macro': 0.6571133405762325, 'F1-Weighted': 0.9165913938017969, 'F1_Class 0': 0.9653431260041312, 'F1_Class 1': 0.611260053619303, 'F1_Class 2': 0.39473684210526316}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 345 at rate 0.4\n",
      "> Train with CF samples 9956\n",
      "{'Accuracy': 0.9333888426311407, 'F1-Macro': 0.654116270048942, 'F1-Weighted': 0.9223536910908418, 'F1_Class 0': 0.9711516270482345, 'F1_Class 1': 0.625, 'F1_Class 2': 0.3661971830985915}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 344 at rate 0.4\n",
      "> Train with CF samples 9955\n",
      "{'Accuracy': 0.694421315570358, 'F1-Macro': 0.5303922128611547, 'F1-Weighted': 0.7529223818236734, 'F1_Class 0': 0.8059284116331096, 'F1_Class 1': 0.3652482269503546, 'F1_Class 2': 0.41999999999999993}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 440 at rate 0.5\n",
      "> Train with CF samples 10050\n",
      "{'Accuracy': 0.9092800665834374, 'F1-Macro': 0.537953754896252, 'F1-Weighted': 0.8873075717472074, 'F1_Class 0': 0.9521433431617147, 'F1_Class 1': 0.4878048780487804, 'F1_Class 2': 0.17391304347826086}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 446 at rate 0.5\n",
      "> Train with CF samples 10056\n",
      "{'Accuracy': 0.9192675821889305, 'F1-Macro': 0.5409597857262913, 'F1-Weighted': 0.902390767267803, 'F1_Class 0': 0.9588477366255143, 'F1_Class 1': 0.606060606060606, 'F1_Class 2': 0.057971014492753624}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 433 at rate 0.5\n",
      "> Train with CF samples 10043\n",
      "{'Accuracy': 0.9296712442779859, 'F1-Macro': 0.6597299870418488, 'F1-Weighted': 0.9177167658936345, 'F1_Class 0': 0.9657864523536166, 'F1_Class 1': 0.6186666666666667, 'F1_Class 2': 0.39473684210526316}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 431 at rate 0.5\n",
      "> Train with CF samples 10042\n",
      "{'Accuracy': 0.9333888426311407, 'F1-Macro': 0.6413655222222289, 'F1-Weighted': 0.9222894821275059, 'F1_Class 0': 0.9713493530499075, 'F1_Class 1': 0.6339066339066339, 'F1_Class 2': 0.3188405797101449}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 430 at rate 0.5\n",
      "> Train with CF samples 10041\n",
      "{'Accuracy': 0.7227310574521232, 'F1-Macro': 0.5436313036313037, 'F1-Weighted': 0.7731596736259518, 'F1_Class 0': 0.8266448266448267, 'F1_Class 1': 0.3866666666666667, 'F1_Class 2': 0.4175824175824176}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 528 at rate 0.6\n",
      "> Train with CF samples 10138\n",
      "{'Accuracy': 0.9096962130669995, 'F1-Macro': 0.5445383666788525, 'F1-Weighted': 0.8870638924571758, 'F1_Class 0': 0.9524241051200724, 'F1_Class 1': 0.47530864197530864, 'F1_Class 2': 0.20588235294117646}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 536 at rate 0.6\n",
      "> Train with CF samples 10146\n",
      "{'Accuracy': 0.9205160216396171, 'F1-Macro': 0.5495132254404539, 'F1-Weighted': 0.9014037257844488, 'F1_Class 0': 0.9579258585399135, 'F1_Class 1': 0.5953757225433526, 'F1_Class 2': 0.09523809523809525}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 520 at rate 0.6\n",
      "> Train with CF samples 10130\n",
      "{'Accuracy': 0.928006658343737, 'F1-Macro': 0.6476160317283964, 'F1-Weighted': 0.9167373550953483, 'F1_Class 0': 0.9656919180290121, 'F1_Class 1': 0.6181818181818183, 'F1_Class 2': 0.358974358974359}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 517 at rate 0.6\n",
      "> Train with CF samples 10128\n",
      "{'Accuracy': 0.9321398834304746, 'F1-Macro': 0.6570599565991271, 'F1-Weighted': 0.920653725216551, 'F1_Class 0': 0.9700460829493087, 'F1_Class 1': 0.6122448979591837, 'F1_Class 2': 0.38888888888888884}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 515 at rate 0.6\n",
      "> Train with CF samples 10126\n",
      "{'Accuracy': 0.6935886761032473, 'F1-Macro': 0.5256050922242129, 'F1-Weighted': 0.7519071242991933, 'F1_Class 0': 0.8049258326336411, 'F1_Class 1': 0.3676341248900616, 'F1_Class 2': 0.4042553191489361}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 616 at rate 0.7000000000000001\n",
      "> Train with CF samples 10226\n",
      "{'Accuracy': 0.9055347482313775, 'F1-Macro': 0.5187856946414074, 'F1-Weighted': 0.8793768473570359, 'F1_Class 0': 0.9501016030706705, 'F1_Class 1': 0.42443729903536975, 'F1_Class 2': 0.18181818181818182}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 625 at rate 0.7000000000000001\n",
      "> Train with CF samples 10235\n",
      "{'Accuracy': 0.9196837286724927, 'F1-Macro': 0.5506196204388868, 'F1-Weighted': 0.9025552267633475, 'F1_Class 0': 0.9584285061671997, 'F1_Class 1': 0.6038781163434903, 'F1_Class 2': 0.08955223880597013}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 606 at rate 0.7000000000000001\n",
      "> Train with CF samples 10216\n",
      "{'Accuracy': 0.9209321681231794, 'F1-Macro': 0.613921926685432, 'F1-Weighted': 0.904774055506887, 'F1_Class 0': 0.9602921040620721, 'F1_Class 1': 0.5527065527065528, 'F1_Class 2': 0.3287671232876712}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 603 at rate 0.7000000000000001\n",
      "> Train with CF samples 10214\n",
      "{'Accuracy': 0.9304746044962531, 'F1-Macro': 0.6390523843330462, 'F1-Weighted': 0.9187521719146844, 'F1_Class 0': 0.9695571955719559, 'F1_Class 1': 0.6095717884130982, 'F1_Class 2': 0.33802816901408456}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 601 at rate 0.7000000000000001\n",
      "> Train with CF samples 10212\n",
      "{'Accuracy': 0.7331390507910075, 'F1-Macro': 0.5219741907161388, 'F1-Weighted': 0.7804182828352295, 'F1_Class 0': 0.8360877810891357, 'F1_Class 1': 0.3965014577259475, 'F1_Class 2': 0.3333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 704 at rate 0.8\n",
      "> Train with CF samples 10314\n",
      "{'Accuracy': 0.9084477736163129, 'F1-Macro': 0.5321195382769475, 'F1-Weighted': 0.8839928262160675, 'F1_Class 0': 0.9511754068716094, 'F1_Class 1': 0.4605678233438485, 'F1_Class 2': 0.1846153846153846}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 714 at rate 0.8\n",
      "> Train with CF samples 10324\n",
      "{'Accuracy': 0.9113607990012484, 'F1-Macro': 0.516722977979313, 'F1-Weighted': 0.8881766052673925, 'F1_Class 0': 0.9533514492753623, 'F1_Class 1': 0.5030674846625767, 'F1_Class 2': 0.09375}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 693 at rate 0.8\n",
      "> Train with CF samples 10303\n",
      "{'Accuracy': 0.9275905118601748, 'F1-Macro': 0.6344833375997446, 'F1-Weighted': 0.9139985627704348, 'F1_Class 0': 0.964711274060495, 'F1_Class 1': 0.6054054054054054, 'F1_Class 2': 0.3333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 690 at rate 0.8\n",
      "> Train with CF samples 10301\n",
      "{'Accuracy': 0.93255620316403, 'F1-Macro': 0.6533636755046867, 'F1-Weighted': 0.9215855896701921, 'F1_Class 0': 0.9704524469067405, 'F1_Class 1': 0.6234413965087281, 'F1_Class 2': 0.3661971830985915}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 687 at rate 0.8\n",
      "> Train with CF samples 10298\n",
      "{'Accuracy': 0.7160699417152373, 'F1-Macro': 0.5453975785831262, 'F1-Weighted': 0.7688820560719004, 'F1_Class 0': 0.8216840946615299, 'F1_Class 1': 0.38440111420612816, 'F1_Class 2': 0.4301075268817204}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 792 at rate 0.9\n",
      "> Train with CF samples 10402\n",
      "{'Accuracy': 0.9092800665834374, 'F1-Macro': 0.5354801428913626, 'F1-Weighted': 0.885333812994618, 'F1_Class 0': 0.9516056083220262, 'F1_Class 1': 0.47021943573667707, 'F1_Class 2': 0.1846153846153846}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 804 at rate 0.9\n",
      "> Train with CF samples 10414\n",
      "{'Accuracy': 0.9188514357053683, 'F1-Macro': 0.5440671790395973, 'F1-Weighted': 0.8992559270688979, 'F1_Class 0': 0.9572532969531604, 'F1_Class 1': 0.5797101449275363, 'F1_Class 2': 0.09523809523809525}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 779 at rate 0.9\n",
      "> Train with CF samples 10389\n",
      "{'Accuracy': 0.9188514357053683, 'F1-Macro': 0.5724703395031367, 'F1-Weighted': 0.9004968839317454, 'F1_Class 0': 0.9596535217688625, 'F1_Class 1': 0.5423728813559321, 'F1_Class 2': 0.2153846153846154}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 776 at rate 0.9\n",
      "> Train with CF samples 10387\n",
      "{'Accuracy': 0.9250624479600333, 'F1-Macro': 0.5825327904215799, 'F1-Weighted': 0.910974149371887, 'F1_Class 0': 0.9665821617884306, 'F1_Class 1': 0.5935162094763092, 'F1_Class 2': 0.1875}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 773 at rate 0.9\n",
      "> Train with CF samples 10384\n",
      "{'Accuracy': 0.7339716902581183, 'F1-Macro': 0.5627699454251903, 'F1-Weighted': 0.7823031699434958, 'F1_Class 0': 0.8343758498776176, 'F1_Class 1': 0.402321083172147, 'F1_Class 2': 0.4516129032258065}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 880 at rate 1.0\n",
      "> Train with CF samples 10490\n",
      "{'Accuracy': 0.9101123595505618, 'F1-Macro': 0.5315789342158314, 'F1-Weighted': 0.8873212401423973, 'F1_Class 0': 0.9524241051200724, 'F1_Class 1': 0.49079754601226994, 'F1_Class 2': 0.1515151515151515}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 893 at rate 1.0\n",
      "> Train with CF samples 10503\n",
      "{'Accuracy': 0.920099875156055, 'F1-Macro': 0.5479429128365569, 'F1-Weighted': 0.901202470702184, 'F1_Class 0': 0.9581247155211652, 'F1_Class 1': 0.5919540229885057, 'F1_Class 2': 0.09375}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 866 at rate 1.0\n",
      "> Train with CF samples 10476\n",
      "{'Accuracy': 0.9300873907615481, 'F1-Macro': 0.6567382376594761, 'F1-Weighted': 0.9189146737282828, 'F1_Class 0': 0.9663904235727439, 'F1_Class 1': 0.6304909560723514, 'F1_Class 2': 0.3733333333333333}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 862 at rate 1.0\n",
      "> Train with CF samples 10473\n",
      "{'Accuracy': 0.9321398834304746, 'F1-Macro': 0.6389803235931484, 'F1-Weighted': 0.9210334538897869, 'F1_Class 0': 0.9706494106771436, 'F1_Class 1': 0.6274509803921567, 'F1_Class 2': 0.3188405797101449}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 859 at rate 1.0\n",
      "> Train with CF samples 10470\n",
      "{'Accuracy': 0.7223147377185679, 'F1-Macro': 0.5376059323216712, 'F1-Weighted': 0.7729265719185731, 'F1_Class 0': 0.8264915161466885, 'F1_Class 1': 0.3909774436090226, 'F1_Class 2': 0.3953488372093023}\n",
      "Politics (11018, 4)\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.8185117967332124, 'F1-Macro': 0.5233816014444055, 'F1-Weighted': 0.7762775105439844, 'F1_Class 0': 0.8963480128893663, 'F1_Class 1': 0.40909090909090906, 'F1_Class 2': 0.2647058823529412}\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.926950998185118, 'F1-Macro': 0.6656259494730388, 'F1-Weighted': 0.9190891550957514, 'F1_Class 0': 0.958682461716267, 'F1_Class 1': 0.8590909090909091, 'F1_Class 2': 0.17910447761194026}\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.925589836660617, 'F1-Macro': 0.684968517769493, 'F1-Weighted': 0.9190268689865758, 'F1_Class 0': 0.9567757009345794, 'F1_Class 1': 0.8593238822246456, 'F1_Class 2': 0.23880597014925373}\n",
      "> Train samples 8815\n",
      "{'Accuracy': 0.9414434861552429, 'F1-Macro': 0.6889210195504522, 'F1-Weighted': 0.9346188639507832, 'F1_Class 0': 0.9735449735449735, 'F1_Class 1': 0.8744680851063831, 'F1_Class 2': 0.21875}\n",
      "> Train samples 8815\n",
      "{'Accuracy': 0.8352246935996369, 'F1-Macro': 0.5726607708094458, 'F1-Weighted': 0.8401338330000716, 'F1_Class 0': 0.897044494965898, 'F1_Class 1': 0.719242902208202, 'F1_Class 2': 0.10169491525423728}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 162 at rate 0.1\n",
      "> Train with CF samples 8976\n",
      "{'Accuracy': 0.8176043557168784, 'F1-Macro': 0.5138444932109842, 'F1-Weighted': 0.7735004784853386, 'F1_Class 0': 0.8960342979635584, 'F1_Class 1': 0.3993453355155483, 'F1_Class 2': 0.24615384615384614}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 164 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.9228675136116152, 'F1-Macro': 0.6543352208339674, 'F1-Weighted': 0.9134553672660787, 'F1_Class 0': 0.9554725653547831, 'F1_Class 1': 0.846242774566474, 'F1_Class 2': 0.16129032258064516}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 164 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.9337568058076225, 'F1-Macro': 0.6582948689146226, 'F1-Weighted': 0.9250332953551023, 'F1_Class 0': 0.9621653084982539, 'F1_Class 1': 0.8793859649122807, 'F1_Class 2': 0.13333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 163 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.9500680889695869, 'F1-Macro': 0.69117057573219, 'F1-Weighted': 0.9426994256919157, 'F1_Class 0': 0.9790498672174683, 'F1_Class 1': 0.8944618599791013, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 165 at rate 0.1\n",
      "> Train with CF samples 8980\n",
      "{'Accuracy': 0.8520199727644122, 'F1-Macro': 0.6411373143081233, 'F1-Weighted': 0.8563972441220353, 'F1_Class 0': 0.9074664964901085, 'F1_Class 1': 0.7390223695111847, 'F1_Class 2': 0.27692307692307694}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 323 at rate 0.2\n",
      "> Train with CF samples 9137\n",
      "{'Accuracy': 0.8171506352087115, 'F1-Macro': 0.5280581307260198, 'F1-Weighted': 0.7740603941915801, 'F1_Class 0': 0.895626509256775, 'F1_Class 1': 0.39869281045751637, 'F1_Class 2': 0.2898550724637681}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 327 at rate 0.2\n",
      "> Train with CF samples 9141\n",
      "{'Accuracy': 0.9228675136116152, 'F1-Macro': 0.6536831001235036, 'F1-Weighted': 0.9136623239245378, 'F1_Class 0': 0.9557216791259344, 'F1_Class 1': 0.8465974625144176, 'F1_Class 2': 0.15873015873015872}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 327 at rate 0.2\n",
      "> Train with CF samples 9141\n",
      "{'Accuracy': 0.9364791288566243, 'F1-Macro': 0.6709399188035872, 'F1-Weighted': 0.9280335590807604, 'F1_Class 0': 0.9634570765661252, 'F1_Class 1': 0.885428253615128, 'F1_Class 2': 0.1639344262295082}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 325 at rate 0.2\n",
      "> Train with CF samples 9140\n",
      "{'Accuracy': 0.9518837948252383, 'F1-Macro': 0.6939413064874703, 'F1-Weighted': 0.944197202019963, 'F1_Class 0': 0.9796999117387467, 'F1_Class 1': 0.8987341772151899, 'F1_Class 2': 0.20338983050847456}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 329 at rate 0.2\n",
      "> Train with CF samples 9144\n",
      "{'Accuracy': 0.8506581933726737, 'F1-Macro': 0.6476566963496184, 'F1-Weighted': 0.856167519887196, 'F1_Class 0': 0.9069319640564828, 'F1_Class 1': 0.7375306623058054, 'F1_Class 2': 0.29850746268656714}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 485 at rate 0.30000000000000004\n",
      "> Train with CF samples 9299\n",
      "{'Accuracy': 0.8180580762250453, 'F1-Macro': 0.5212405477158027, 'F1-Weighted': 0.7756288822382283, 'F1_Class 0': 0.8963480128893663, 'F1_Class 1': 0.40650406504065045, 'F1_Class 2': 0.2608695652173913}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 491 at rate 0.30000000000000004\n",
      "> Train with CF samples 9305\n",
      "{'Accuracy': 0.9260435571687841, 'F1-Macro': 0.64586716171934, 'F1-Weighted': 0.9172660827354625, 'F1_Class 0': 0.9590069284064665, 'F1_Class 1': 0.8555176336746302, 'F1_Class 2': 0.12307692307692307}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 491 at rate 0.30000000000000004\n",
      "> Train with CF samples 9305\n",
      "{'Accuracy': 0.9274047186932849, 'F1-Macro': 0.6719094667177393, 'F1-Weighted': 0.9192974136218703, 'F1_Class 0': 0.9567740063823614, 'F1_Class 1': 0.8654060066740823, 'F1_Class 2': 0.1935483870967742}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 488 at rate 0.30000000000000004\n",
      "> Train with CF samples 9303\n",
      "{'Accuracy': 0.9487063095778484, 'F1-Macro': 0.6897287518553098, 'F1-Weighted': 0.9408869764663915, 'F1_Class 0': 0.9774392030471726, 'F1_Class 1': 0.8917470525187567, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 494 at rate 0.30000000000000004\n",
      "> Train with CF samples 9309\n",
      "{'Accuracy': 0.8388561053109397, 'F1-Macro': 0.6324265357442699, 'F1-Weighted': 0.8454467662384086, 'F1_Class 0': 0.8977346278317152, 'F1_Class 1': 0.7226219024780176, 'F1_Class 2': 0.27692307692307694}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 647 at rate 0.4\n",
      "> Train with CF samples 9461\n",
      "{'Accuracy': 0.8198729582577132, 'F1-Macro': 0.5127217755669086, 'F1-Weighted': 0.7786019680355791, 'F1_Class 0': 0.8970153267007259, 'F1_Class 1': 0.4224, 'F1_Class 2': 0.21875}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 655 at rate 0.4\n",
      "> Train with CF samples 9469\n",
      "{'Accuracy': 0.9260435571687841, 'F1-Macro': 0.6582810923768129, 'F1-Weighted': 0.9167168284245303, 'F1_Class 0': 0.9576978417266186, 'F1_Class 1': 0.853211009174312, 'F1_Class 2': 0.1639344262295082}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 655 at rate 0.4\n",
      "> Train with CF samples 9469\n",
      "{'Accuracy': 0.9369328493647913, 'F1-Macro': 0.652374990450946, 'F1-Weighted': 0.9271929094389358, 'F1_Class 0': 0.9632203880683464, 'F1_Class 1': 0.888641425389755, 'F1_Class 2': 0.10526315789473684}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 650 at rate 0.4\n",
      "> Train with CF samples 9465\n",
      "{'Accuracy': 0.9491602360417613, 'F1-Macro': 0.6606685134451601, 'F1-Weighted': 0.9395036797826971, 'F1_Class 0': 0.9779994133176885, 'F1_Class 1': 0.8928950159066809, 'F1_Class 2': 0.1111111111111111}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 659 at rate 0.4\n",
      "> Train with CF samples 9474\n",
      "{'Accuracy': 0.8588288697231049, 'F1-Macro': 0.6416065295538453, 'F1-Weighted': 0.8629011867862326, 'F1_Class 0': 0.9145217667619955, 'F1_Class 1': 0.7455919395465994, 'F1_Class 2': 0.2647058823529412}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 808 at rate 0.5\n",
      "> Train with CF samples 9622\n",
      "{'Accuracy': 0.8198729582577132, 'F1-Macro': 0.5190158655407954, 'F1-Weighted': 0.7767986438128447, 'F1_Class 0': 0.8972914990614106, 'F1_Class 1': 0.4097560975609756, 'F1_Class 2': 0.25}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 818 at rate 0.5\n",
      "> Train with CF samples 9632\n",
      "{'Accuracy': 0.9233212341197822, 'F1-Macro': 0.6545058131219901, 'F1-Weighted': 0.9145497228761973, 'F1_Class 0': 0.9563709910430512, 'F1_Class 1': 0.8484162895927603, 'F1_Class 2': 0.15873015873015872}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 818 at rate 0.5\n",
      "> Train with CF samples 9632\n",
      "{'Accuracy': 0.9323956442831216, 'F1-Macro': 0.6360346672220084, 'F1-Weighted': 0.9223475912244774, 'F1_Class 0': 0.9609261939218524, 'F1_Class 1': 0.8782122905027934, 'F1_Class 2': 0.06896551724137931}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 813 at rate 0.5\n",
      "> Train with CF samples 9628\n",
      "{'Accuracy': 0.9491602360417613, 'F1-Macro': 0.6910313714085282, 'F1-Weighted': 0.941233400930795, 'F1_Class 0': 0.977725674091442, 'F1_Class 1': 0.8919786096256684, 'F1_Class 2': 0.20338983050847456}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 824 at rate 0.5\n",
      "> Train with CF samples 9639\n",
      "{'Accuracy': 0.8488424875170223, 'F1-Macro': 0.5819116230340319, 'F1-Weighted': 0.8517203186015594, 'F1_Class 0': 0.9079242861726019, 'F1_Class 1': 0.7343623070674248, 'F1_Class 2': 0.10344827586206895}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 970 at rate 0.6\n",
      "> Train with CF samples 9784\n",
      "{'Accuracy': 0.8194192377495463, 'F1-Macro': 0.5200796989458468, 'F1-Weighted': 0.7762997970171033, 'F1_Class 0': 0.896514745308311, 'F1_Class 1': 0.4097560975609756, 'F1_Class 2': 0.25396825396825395}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 982 at rate 0.6\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.9246823956442831, 'F1-Macro': 0.6472777565468278, 'F1-Weighted': 0.9152427732912415, 'F1_Class 0': 0.9564716056500433, 'F1_Class 1': 0.8542141230068337, 'F1_Class 2': 0.13114754098360654}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 982 at rate 0.6\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.9314882032667876, 'F1-Macro': 0.656603988240725, 'F1-Weighted': 0.9225806970507939, 'F1_Class 0': 0.9596750797795185, 'F1_Class 1': 0.876803551609323, 'F1_Class 2': 0.13333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 976 at rate 0.6\n",
      "> Train with CF samples 9791\n",
      "{'Accuracy': 0.9477984566500227, 'F1-Macro': 0.6803187747879716, 'F1-Weighted': 0.9392127590640403, 'F1_Class 0': 0.9768667642752562, 'F1_Class 1': 0.8886509635974305, 'F1_Class 2': 0.17543859649122806}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 988 at rate 0.6\n",
      "> Train with CF samples 9803\n",
      "{'Accuracy': 0.8597367226509306, 'F1-Macro': 0.6337137000549974, 'F1-Weighted': 0.863087451677341, 'F1_Class 0': 0.9150823827629911, 'F1_Class 1': 0.7472527472527473, 'F1_Class 2': 0.23880597014925373}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1132 at rate 0.7000000000000001\n",
      "> Train with CF samples 9946\n",
      "{'Accuracy': 0.8207803992740472, 'F1-Macro': 0.5063788480601941, 'F1-Weighted': 0.7782280194968111, 'F1_Class 0': 0.8972363831499865, 'F1_Class 1': 0.4219001610305958, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1146 at rate 0.7000000000000001\n",
      "> Train with CF samples 9960\n",
      "{'Accuracy': 0.9251361161524501, 'F1-Macro': 0.6816213677390147, 'F1-Weighted': 0.9176978915418117, 'F1_Class 0': 0.9569985569985571, 'F1_Class 1': 0.8525714285714285, 'F1_Class 2': 0.23529411764705882}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1146 at rate 0.7000000000000001\n",
      "> Train with CF samples 9960\n",
      "{'Accuracy': 0.9355716878402904, 'F1-Macro': 0.6700815038712914, 'F1-Weighted': 0.9270319287223995, 'F1_Class 0': 0.9626411815812336, 'F1_Class 1': 0.883668903803132, 'F1_Class 2': 0.1639344262295082}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1138 at rate 0.7000000000000001\n",
      "> Train with CF samples 9953\n",
      "{'Accuracy': 0.9505220154334998, 'F1-Macro': 0.6723445337810187, 'F1-Weighted': 0.9415806745556119, 'F1_Class 0': 0.9791605518051071, 'F1_Class 1': 0.895015906680806, 'F1_Class 2': 0.14285714285714285}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1153 at rate 0.7000000000000001\n",
      "> Train with CF samples 9968\n",
      "{'Accuracy': 0.8492964139809351, 'F1-Macro': 0.601796713100452, 'F1-Weighted': 0.8532291108334636, 'F1_Class 0': 0.9075144508670521, 'F1_Class 1': 0.7365853658536586, 'F1_Class 2': 0.16129032258064516}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1294 at rate 0.8\n",
      "> Train with CF samples 10108\n",
      "{'Accuracy': 0.8194192377495463, 'F1-Macro': 0.5113394405365437, 'F1-Weighted': 0.7774724036432206, 'F1_Class 0': 0.8965887724952994, 'F1_Class 1': 0.4186795491143317, 'F1_Class 2': 0.21875}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1310 at rate 0.8\n",
      "> Train with CF samples 10124\n",
      "{'Accuracy': 0.926497277676951, 'F1-Macro': 0.6592811825085815, 'F1-Weighted': 0.9172162766264144, 'F1_Class 0': 0.9574223245109321, 'F1_Class 1': 0.8564867967853043, 'F1_Class 2': 0.1639344262295082}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1310 at rate 0.8\n",
      "> Train with CF samples 10124\n",
      "{'Accuracy': 0.9314882032667876, 'F1-Macro': 0.6861402952911696, 'F1-Weighted': 0.9232538590380348, 'F1_Class 0': 0.9598381970528749, 'F1_Class 1': 0.8690744920993226, 'F1_Class 2': 0.2295081967213115}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1301 at rate 0.8\n",
      "> Train with CF samples 10116\n",
      "{'Accuracy': 0.9509759418974126, 'F1-Macro': 0.7003290300535981, 'F1-Weighted': 0.9438681665639588, 'F1_Class 0': 0.9794359576968272, 'F1_Class 1': 0.8957446808510638, 'F1_Class 2': 0.22580645161290325}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1318 at rate 0.8\n",
      "> Train with CF samples 10133\n",
      "{'Accuracy': 0.8433953699500681, 'F1-Macro': 0.5883253518150163, 'F1-Weighted': 0.8464193637914252, 'F1_Class 0': 0.9036375239310785, 'F1_Class 1': 0.7209876543209878, 'F1_Class 2': 0.14035087719298245}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1455 at rate 0.9\n",
      "> Train with CF samples 10269\n",
      "{'Accuracy': 0.8189655172413793, 'F1-Macro': 0.5124575904537688, 'F1-Weighted': 0.7765726021915148, 'F1_Class 0': 0.8961073825503355, 'F1_Class 1': 0.4154589371980676, 'F1_Class 2': 0.22580645161290325}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1473 at rate 0.9\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.9219600725952813, 'F1-Macro': 0.6466480423694444, 'F1-Weighted': 0.9111615315654048, 'F1_Class 0': 0.9538814093382985, 'F1_Class 1': 0.843205574912892, 'F1_Class 2': 0.14285714285714285}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1473 at rate 0.9\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.9355716878402904, 'F1-Macro': 0.6816610440739552, 'F1-Weighted': 0.9272521793683359, 'F1_Class 0': 0.9620399884091567, 'F1_Class 1': 0.8829431438127089, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1463 at rate 0.9\n",
      "> Train with CF samples 10278\n",
      "{'Accuracy': 0.9459827507943713, 'F1-Macro': 0.695156092629685, 'F1-Weighted': 0.938769669129796, 'F1_Class 0': 0.9762393663831035, 'F1_Class 1': 0.8834224598930482, 'F1_Class 2': 0.22580645161290325}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1482 at rate 0.9\n",
      "> Train with CF samples 10297\n",
      "{'Accuracy': 0.8706309577848389, 'F1-Macro': 0.6369039331137997, 'F1-Weighted': 0.8713312693974438, 'F1_Class 0': 0.923125, 'F1_Class 1': 0.7580786026200874, 'F1_Class 2': 0.2295081967213115}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1617 at rate 1.0\n",
      "> Train with CF samples 10431\n",
      "{'Accuracy': 0.8144283121597096, 'F1-Macro': 0.4852720552668283, 'F1-Weighted': 0.768402239089604, 'F1_Class 0': 0.893525949705725, 'F1_Class 1': 0.3927986906710311, 'F1_Class 2': 0.1694915254237288}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1637 at rate 1.0\n",
      "> Train with CF samples 10451\n",
      "{'Accuracy': 0.9260435571687841, 'F1-Macro': 0.6593586134569741, 'F1-Weighted': 0.9170360163903944, 'F1_Class 0': 0.9569985569985571, 'F1_Class 1': 0.8571428571428572, 'F1_Class 2': 0.1639344262295082}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1637 at rate 1.0\n",
      "> Train with CF samples 10451\n",
      "{'Accuracy': 0.9251361161524501, 'F1-Macro': 0.6375338777110451, 'F1-Weighted': 0.9153720802121118, 'F1_Class 0': 0.9567723342939481, 'F1_Class 1': 0.8574686431014823, 'F1_Class 2': 0.09836065573770492}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1626 at rate 1.0\n",
      "> Train with CF samples 10441\n",
      "{'Accuracy': 0.9455288243304585, 'F1-Macro': 0.6871611974459945, 'F1-Weighted': 0.937467919623507, 'F1_Class 0': 0.9754242246928029, 'F1_Class 1': 0.8826695371367063, 'F1_Class 2': 0.20338983050847456}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1647 at rate 1.0\n",
      "> Train with CF samples 10462\n",
      "{'Accuracy': 0.8620063549704948, 'F1-Macro': 0.6186355825570579, 'F1-Weighted': 0.8645644052840665, 'F1_Class 0': 0.9175649968294229, 'F1_Class 1': 0.7508417508417508, 'F1_Class 2': 0.1875}\n",
      "Sports (12306, 4)\n",
      "> Train samples 9844\n",
      "{'Accuracy': 0.892770105605199, 'F1-Macro': 0.5337911781127862, 'F1-Weighted': 0.8739996897566631, 'F1_Class 0': 0.9413735343383585, 'F1_Class 1': 0.66, 'F1_Class 2': 0.0}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.9654611946363267, 'F1-Macro': 0.7124118369639665, 'F1-Weighted': 0.9604615254057507, 'F1_Class 0': 0.9837133550488599, 'F1_Class 1': 0.9271070615034167, 'F1_Class 2': 0.22641509433962262}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.9695245835026413, 'F1-Macro': 0.7065262475997272, 'F1-Weighted': 0.9640208277365606, 'F1_Class 0': 0.9886678418534375, 'F1_Class 1': 0.9268292682926831, 'F1_Class 2': 0.2040816326530612}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.9695245835026413, 'F1-Macro': 0.7810866746190327, 'F1-Weighted': 0.966176519872276, 'F1_Class 0': 0.9861982434127979, 'F1_Class 1': 0.9284903518728718, 'F1_Class 2': 0.42857142857142855}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.884599756196668, 'F1-Macro': 0.6802625665599482, 'F1-Weighted': 0.8974673770574437, 'F1_Class 0': 0.9298715203426123, 'F1_Class 1': 0.8109161793372319, 'F1_Class 2': 0.30000000000000004}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 148 at rate 0.1\n",
      "> Train with CF samples 9992\n",
      "{'Accuracy': 0.8980503655564581, 'F1-Macro': 0.5716485955570286, 'F1-Weighted': 0.8812736434499392, 'F1_Class 0': 0.9434052757793764, 'F1_Class 1': 0.6826516220028209, 'F1_Class 2': 0.08888888888888889}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 146 at rate 0.1\n",
      "> Train with CF samples 9991\n",
      "{'Accuracy': 0.9662738724095896, 'F1-Macro': 0.6796592800547107, 'F1-Weighted': 0.9599072733314891, 'F1_Class 0': 0.9846849108712027, 'F1_Class 1': 0.9292929292929293, 'F1_Class 2': 0.125}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 147 at rate 0.1\n",
      "> Train with CF samples 9992\n",
      "{'Accuracy': 0.9666802112962211, 'F1-Macro': 0.6871368335573167, 'F1-Weighted': 0.9612575354029302, 'F1_Class 0': 0.987921489682939, 'F1_Class 1': 0.9196428571428571, 'F1_Class 2': 0.15384615384615385}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 151 at rate 0.1\n",
      "> Train with CF samples 9996\n",
      "{'Accuracy': 0.9642421779764323, 'F1-Macro': 0.7380873732957675, 'F1-Weighted': 0.959492990614148, 'F1_Class 0': 0.9827284105131415, 'F1_Class 1': 0.9178082191780822, 'F1_Class 2': 0.3137254901960784}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 153 at rate 0.1\n",
      "> Train with CF samples 9998\n",
      "{'Accuracy': 0.8915075172694027, 'F1-Macro': 0.679669664283419, 'F1-Weighted': 0.9023287306918083, 'F1_Class 0': 0.9354924342978497, 'F1_Class 1': 0.813861386138614, 'F1_Class 2': 0.28965517241379307}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 295 at rate 0.2\n",
      "> Train with CF samples 10139\n",
      "{'Accuracy': 0.8964256701868399, 'F1-Macro': 0.540213813127836, 'F1-Weighted': 0.8789095248686379, 'F1_Class 0': 0.9436315663228592, 'F1_Class 1': 0.6770098730606489, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 291 at rate 0.2\n",
      "> Train with CF samples 10136\n",
      "{'Accuracy': 0.9654611946363267, 'F1-Macro': 0.6656927293537367, 'F1-Weighted': 0.9585648104793598, 'F1_Class 0': 0.9842065680621709, 'F1_Class 1': 0.927765237020316, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 294 at rate 0.2\n",
      "> Train with CF samples 10139\n",
      "{'Accuracy': 0.9670865501828525, 'F1-Macro': 0.6769029963803269, 'F1-Weighted': 0.960863359471331, 'F1_Class 0': 0.9879457559015571, 'F1_Class 1': 0.920314253647587, 'F1_Class 2': 0.12244897959183672}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 301 at rate 0.2\n",
      "> Train with CF samples 10146\n",
      "{'Accuracy': 0.9687119057293783, 'F1-Macro': 0.7614174772300699, 'F1-Weighted': 0.9647391416990682, 'F1_Class 0': 0.9857178651966926, 'F1_Class 1': 0.928164196123147, 'F1_Class 2': 0.37037037037037035}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 306 at rate 0.2\n",
      "> Train with CF samples 10151\n",
      "{'Accuracy': 0.8951645672490858, 'F1-Macro': 0.6834018187835899, 'F1-Weighted': 0.9042714583279069, 'F1_Class 0': 0.9376979936642027, 'F1_Class 1': 0.814, 'F1_Class 2': 0.2985074626865672}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 443 at rate 0.30000000000000004\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.8935824532900081, 'F1-Macro': 0.5354396205697914, 'F1-Weighted': 0.8755986970344, 'F1_Class 0': 0.9425011978917106, 'F1_Class 1': 0.6638176638176637, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 437 at rate 0.30000000000000004\n",
      "> Train with CF samples 10282\n",
      "{'Accuracy': 0.9650548557496953, 'F1-Macro': 0.6652591723373884, 'F1-Weighted': 0.9581700851617524, 'F1_Class 0': 0.9839518555667002, 'F1_Class 1': 0.9267192784667417, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 440 at rate 0.30000000000000004\n",
      "> Train with CF samples 10285\n",
      "{'Accuracy': 0.9678992279561154, 'F1-Macro': 0.6892318210275419, 'F1-Weighted': 0.9622298339818627, 'F1_Class 0': 0.9884480160723255, 'F1_Class 1': 0.9223847019122609, 'F1_Class 2': 0.1568627450980392}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 452 at rate 0.30000000000000004\n",
      "> Train with CF samples 10297\n",
      "{'Accuracy': 0.9650548557496953, 'F1-Macro': 0.773116073880915, 'F1-Weighted': 0.9615464023324719, 'F1_Class 0': 0.983745936484121, 'F1_Class 1': 0.9145496535796767, 'F1_Class 2': 0.4210526315789473}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 459 at rate 0.30000000000000004\n",
      "> Train with CF samples 10304\n",
      "{'Accuracy': 0.9041040227549777, 'F1-Macro': 0.7046548621829402, 'F1-Weighted': 0.9126473679697249, 'F1_Class 0': 0.9430680021085925, 'F1_Class 1': 0.8326612903225805, 'F1_Class 2': 0.3382352941176471}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 591 at rate 0.4\n",
      "> Train with CF samples 10435\n",
      "{'Accuracy': 0.9000812347684809, 'F1-Macro': 0.5595718472785617, 'F1-Weighted': 0.8842964124260753, 'F1_Class 0': 0.946127946127946, 'F1_Class 1': 0.6917712691771268, 'F1_Class 2': 0.04081632653061224}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 582 at rate 0.4\n",
      "> Train with CF samples 10427\n",
      "{'Accuracy': 0.9597724502234863, 'F1-Macro': 0.6714536217248694, 'F1-Weighted': 0.9532634797398016, 'F1_Class 0': 0.9805194805194806, 'F1_Class 1': 0.9113924050632911, 'F1_Class 2': 0.12244897959183672}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 587 at rate 0.4\n",
      "> Train with CF samples 10432\n",
      "{'Accuracy': 0.9674928890694839, 'F1-Macro': 0.678362863832736, 'F1-Weighted': 0.9610900905447358, 'F1_Class 0': 0.9877038895859472, 'F1_Class 1': 0.9223847019122609, 'F1_Class 2': 0.125}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 603 at rate 0.4\n",
      "> Train with CF samples 10448\n",
      "{'Accuracy': 0.9626168224299065, 'F1-Macro': 0.7443383488575751, 'F1-Weighted': 0.9581836442144455, 'F1_Class 0': 0.982, 'F1_Class 1': 0.9113924050632911, 'F1_Class 2': 0.33962264150943394}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 612 at rate 0.4\n",
      "> Train with CF samples 10457\n",
      "{'Accuracy': 0.9012596505485575, 'F1-Macro': 0.7034878579776196, 'F1-Weighted': 0.9106233724910647, 'F1_Class 0': 0.9407720782654679, 'F1_Class 1': 0.8316633266533066, 'F1_Class 2': 0.33802816901408445}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 738 at rate 0.5\n",
      "> Train with CF samples 10582\n",
      "{'Accuracy': 0.9029244516653128, 'F1-Macro': 0.5790310263283304, 'F1-Weighted': 0.8874719293810526, 'F1_Class 0': 0.9463813416686704, 'F1_Class 1': 0.7037552155771906, 'F1_Class 2': 0.08695652173913042}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 728 at rate 0.5\n",
      "> Train with CF samples 10573\n",
      "{'Accuracy': 0.9642421779764323, 'F1-Macro': 0.663965553701502, 'F1-Weighted': 0.9573146346984555, 'F1_Class 0': 0.9837133550488599, 'F1_Class 1': 0.923076923076923, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 734 at rate 0.5\n",
      "> Train with CF samples 10579\n",
      "{'Accuracy': 0.9670865501828525, 'F1-Macro': 0.665253644227684, 'F1-Weighted': 0.9600537117990433, 'F1_Class 0': 0.98745609633718, 'F1_Class 1': 0.9213483146067416, 'F1_Class 2': 0.08695652173913042}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 754 at rate 0.5\n",
      "> Train with CF samples 10599\n",
      "{'Accuracy': 0.963023161316538, 'F1-Macro': 0.7571207841885373, 'F1-Weighted': 0.9587927132138372, 'F1_Class 0': 0.9817636772420685, 'F1_Class 1': 0.9122401847575058, 'F1_Class 2': 0.3773584905660377}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 764 at rate 0.5\n",
      "> Train with CF samples 10609\n",
      "{'Accuracy': 0.9024786672084518, 'F1-Macro': 0.6933770003417861, 'F1-Weighted': 0.9116962941213229, 'F1_Class 0': 0.9424498416050686, 'F1_Class 1': 0.8333333333333333, 'F1_Class 2': 0.30434782608695654}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 886 at rate 0.6\n",
      "> Train with CF samples 10730\n",
      "{'Accuracy': 0.904549147034931, 'F1-Macro': 0.6040635084121382, 'F1-Weighted': 0.8905975701969143, 'F1_Class 0': 0.9479768786127167, 'F1_Class 1': 0.7073509015256587, 'F1_Class 2': 0.1568627450980392}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 873 at rate 0.6\n",
      "> Train with CF samples 10718\n",
      "{'Accuracy': 0.9557090613571719, 'F1-Macro': 0.6796280934728706, 'F1-Weighted': 0.949190501223141, 'F1_Class 0': 0.9776119402985075, 'F1_Class 1': 0.8980070339976552, 'F1_Class 2': 0.16326530612244897}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 881 at rate 0.6\n",
      "> Train with CF samples 10726\n",
      "{'Accuracy': 0.9683055668427468, 'F1-Macro': 0.6920048844980881, 'F1-Weighted': 0.9623771786744362, 'F1_Class 0': 0.9879457559015571, 'F1_Class 1': 0.9248035914702581, 'F1_Class 2': 0.16326530612244897}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 904 at rate 0.6\n",
      "> Train with CF samples 10749\n",
      "{'Accuracy': 0.9573344169036977, 'F1-Macro': 0.7072129803359655, 'F1-Weighted': 0.9513053719750204, 'F1_Class 0': 0.9783958281599205, 'F1_Class 1': 0.8983451536643027, 'F1_Class 2': 0.24489795918367344}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 917 at rate 0.6\n",
      "> Train with CF samples 10762\n",
      "{'Accuracy': 0.8967899227956115, 'F1-Macro': 0.6836084762447842, 'F1-Weighted': 0.9071453801160757, 'F1_Class 0': 0.939297124600639, 'F1_Class 1': 0.8237585199610516, 'F1_Class 2': 0.2877697841726618}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1034 at rate 0.7000000000000001\n",
      "> Train with CF samples 10878\n",
      "{'Accuracy': 0.9000812347684809, 'F1-Macro': 0.5461455377508844, 'F1-Weighted': 0.883433060928471, 'F1_Class 0': 0.9456991830850553, 'F1_Class 1': 0.6927374301675978, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1019 at rate 0.7000000000000001\n",
      "> Train with CF samples 10864\n",
      "{'Accuracy': 0.9605851279967493, 'F1-Macro': 0.659610511490175, 'F1-Weighted': 0.9534287410529577, 'F1_Class 0': 0.9812827551784378, 'F1_Class 1': 0.9124423963133641, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1028 at rate 0.7000000000000001\n",
      "> Train with CF samples 10873\n",
      "{'Accuracy': 0.9658675335229582, 'F1-Macro': 0.6637302554345083, 'F1-Weighted': 0.9587662977041973, 'F1_Class 0': 0.986720120270609, 'F1_Class 1': 0.9175141242937853, 'F1_Class 2': 0.08695652173913042}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1055 at rate 0.7000000000000001\n",
      "> Train with CF samples 10900\n",
      "{'Accuracy': 0.9634295002031694, 'F1-Macro': 0.7261727352153259, 'F1-Weighted': 0.9581269891179088, 'F1_Class 0': 0.9820448877805485, 'F1_Class 1': 0.9164733178654293, 'F1_Class 2': 0.27999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1070 at rate 0.7000000000000001\n",
      "> Train with CF samples 10915\n",
      "{'Accuracy': 0.8931328728159285, 'F1-Macro': 0.686436310165852, 'F1-Weighted': 0.9041579584835612, 'F1_Class 0': 0.9367156208277704, 'F1_Class 1': 0.8170377541142304, 'F1_Class 2': 0.3055555555555556}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1182 at rate 0.8\n",
      "> Train with CF samples 11026\n",
      "{'Accuracy': 0.9004874086108855, 'F1-Macro': 0.5470277896651332, 'F1-Weighted': 0.8838996164727284, 'F1_Class 0': 0.945673076923077, 'F1_Class 1': 0.6954102920723226, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1164 at rate 0.8\n",
      "> Train with CF samples 11009\n",
      "{'Accuracy': 0.951239333604226, 'F1-Macro': 0.6482317618072754, 'F1-Weighted': 0.943582322332272, 'F1_Class 0': 0.9754281459419211, 'F1_Class 1': 0.8841607565011819, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1174 at rate 0.8\n",
      "> Train with CF samples 11019\n",
      "{'Accuracy': 0.9691182446160097, 'F1-Macro': 0.6796948537403815, 'F1-Weighted': 0.9628595507110886, 'F1_Class 0': 0.98870765370138, 'F1_Class 1': 0.9279279279279279, 'F1_Class 2': 0.12244897959183672}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1206 at rate 0.8\n",
      "> Train with CF samples 11051\n",
      "{'Accuracy': 0.9626168224299065, 'F1-Macro': 0.7253591206723402, 'F1-Weighted': 0.9573625998105242, 'F1_Class 0': 0.9815277084373439, 'F1_Class 1': 0.9145496535796767, 'F1_Class 2': 0.27999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1223 at rate 0.8\n",
      "> Train with CF samples 11068\n",
      "{'Accuracy': 0.9036976838683461, 'F1-Macro': 0.6994274933173452, 'F1-Weighted': 0.912312475896231, 'F1_Class 0': 0.943766578249337, 'F1_Class 1': 0.8289345063538612, 'F1_Class 2': 0.32558139534883723}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1329 at rate 0.9\n",
      "> Train with CF samples 11173\n",
      "{'Accuracy': 0.9012997562956946, 'F1-Macro': 0.5616165399172103, 'F1-Weighted': 0.8858374022870398, 'F1_Class 0': 0.9468110709987966, 'F1_Class 1': 0.6972222222222221, 'F1_Class 2': 0.04081632653061224}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1310 at rate 0.9\n",
      "> Train with CF samples 11155\n",
      "{'Accuracy': 0.958553433563592, 'F1-Macro': 0.6440516402075739, 'F1-Weighted': 0.950878017948789, 'F1_Class 0': 0.9797954602145174, 'F1_Class 1': 0.908881199538639, 'F1_Class 2': 0.04347826086956521}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1321 at rate 0.9\n",
      "> Train with CF samples 11166\n",
      "{'Accuracy': 0.9666802112962211, 'F1-Macro': 0.6781917870957977, 'F1-Weighted': 0.9600980897584429, 'F1_Class 0': 0.9869608826479438, 'F1_Class 1': 0.9199549041713643, 'F1_Class 2': 0.1276595744680851}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1356 at rate 0.9\n",
      "> Train with CF samples 11201\n",
      "{'Accuracy': 0.9674928890694839, 'F1-Macro': 0.7720679394991321, 'F1-Weighted': 0.9636821508123917, 'F1_Class 0': 0.9844844844844844, 'F1_Class 1': 0.9243119266055045, 'F1_Class 2': 0.4074074074074074}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1376 at rate 0.9\n",
      "> Train with CF samples 11221\n",
      "{'Accuracy': 0.9041040227549777, 'F1-Macro': 0.6936464468932274, 'F1-Weighted': 0.912598671084438, 'F1_Class 0': 0.9445617740232313, 'F1_Class 1': 0.8286852589641435, 'F1_Class 2': 0.30769230769230765}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1477 at rate 1.0\n",
      "> Train with CF samples 11321\n",
      "{'Accuracy': 0.8988627132412672, 'F1-Macro': 0.5443120699110072, 'F1-Weighted': 0.8824299410339483, 'F1_Class 0': 0.9456991830850553, 'F1_Class 1': 0.6872370266479663, 'F1_Class 2': 0.0}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1455 at rate 1.0\n",
      "> Train with CF samples 11300\n",
      "{'Accuracy': 0.9609914668833808, 'F1-Macro': 0.6467464680358234, 'F1-Weighted': 0.9532524038091246, 'F1_Class 0': 0.9815092453773112, 'F1_Class 1': 0.9142857142857144, 'F1_Class 2': 0.044444444444444446}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1468 at rate 1.0\n",
      "> Train with CF samples 11313\n",
      "{'Accuracy': 0.9597724502234863, 'F1-Macro': 0.6560055899957977, 'F1-Weighted': 0.9525664769595658, 'F1_Class 0': 0.983025461807289, 'F1_Class 1': 0.899884925201381, 'F1_Class 2': 0.0851063829787234}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1507 at rate 1.0\n",
      "> Train with CF samples 11352\n",
      "{'Accuracy': 0.9618041446566437, 'F1-Macro': 0.7243583203962446, 'F1-Weighted': 0.9565129382753299, 'F1_Class 0': 0.9810379241516967, 'F1_Class 1': 0.9120370370370371, 'F1_Class 2': 0.27999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1529 at rate 1.0\n",
      "> Train with CF samples 11374\n",
      "{'Accuracy': 0.9069483949613978, 'F1-Macro': 0.7075383708841848, 'F1-Weighted': 0.9152050080333666, 'F1_Class 0': 0.9454160042395335, 'F1_Class 1': 0.8361138370951914, 'F1_Class 2': 0.3410852713178295}\n"
     ]
    }
   ],
   "source": [
    "def get_tpot_pipeline():\n",
    "    return make_pipeline(\n",
    "        StackingEstimator(estimator=LinearSVC(C=15.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.01)),\n",
    "        DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_leaf=12, min_samples_split=3)\n",
    "    )\n",
    "out_dict = {}\n",
    "for d in domains:\n",
    "    out_dict[d] = {}\n",
    "    sel_df = df[df['Domain'] == d]\n",
    "    print(d, sel_df.shape)\n",
    "    X, y = sel_df['Tweet'], sel_df['Label'].astype(int)\n",
    "    \n",
    "    tpot_exported_pipeline = get_tpot_pipeline()\n",
    "    res_tpot_lst = run_experiment_org(tpot_exported_pipeline, n_splits=n_splits)\n",
    "    out_dict[d]['Org'] = res_tpot_lst\n",
    "    out_dict[d]['CF'] = {}\n",
    "    for cf_size_prop_to_data in cf_size_prop_to_data_lst:\n",
    "        tpot_exported_pipeline = get_tpot_pipeline()\n",
    "        \n",
    "        res_tpot_lst = run_experiment_counter_factuals(tpot_exported_pipeline, n_splits=n_splits, cf_size_prop_to_data=cf_size_prop_to_data)\n",
    "        out_dict[d]['CF'][cf_size_prop_to_data] = res_tpot_lst\n",
    "    json.dump(out_dict, open('out/tpot-lshs22.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d5bf872-5f39-4cf6-931d-b363e25d5b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender (9454, 4)\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.812797461660497, 'F1-Macro': 0.5694941927587891, 'F1-Weighted': 0.8382970005950483, 'F1_Class 0': 0.905241235682055, 'F1_Class 1': 0.6574923547400612, 'F1_Class 2': 0.14574898785425103}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.8873611845584347, 'F1-Macro': 0.6798064356328309, 'F1-Weighted': 0.897388736652952, 'F1_Class 0': 0.9345279117849757, 'F1_Class 1': 0.812920592193809, 'F1_Class 2': 0.291970802919708}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.9185616076150185, 'F1-Macro': 0.6731999128173992, 'F1-Weighted': 0.9314022991537707, 'F1_Class 0': 0.9515669515669515, 'F1_Class 1': 0.9180327868852459, 'F1_Class 2': 0.15}\n",
      "> Train samples 7563\n",
      "{'Accuracy': 0.952934955050238, 'F1-Macro': 0.7644835030001648, 'F1-Weighted': 0.9565906181184256, 'F1_Class 0': 0.9780717020536026, 'F1_Class 1': 0.9245742092457421, 'F1_Class 2': 0.3908045977011494}\n",
      "> Train samples 7564\n",
      "{'Accuracy': 0.7634920634920634, 'F1-Macro': 0.5566700304976218, 'F1-Weighted': 0.786425107214643, 'F1_Class 0': 0.8325320512820513, 'F1_Class 1': 0.6723404255319149, 'F1_Class 2': 0.16513761467889906}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 132 at rate 0.1\n",
      "> Train with CF samples 7695\n",
      "{'Accuracy': 0.8117398202009519, 'F1-Macro': 0.5710049318399345, 'F1-Weighted': 0.837986261935399, 'F1_Class 0': 0.9047287899860919, 'F1_Class 1': 0.6574923547400612, 'F1_Class 2': 0.15079365079365079}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 128 at rate 0.1\n",
      "> Train with CF samples 7691\n",
      "{'Accuracy': 0.8852459016393442, 'F1-Macro': 0.6724384809237582, 'F1-Weighted': 0.8961451671306909, 'F1_Class 0': 0.9334712168217856, 'F1_Class 1': 0.8124156545209177, 'F1_Class 2': 0.2714285714285714}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 131 at rate 0.1\n",
      "> Train with CF samples 7694\n",
      "{'Accuracy': 0.923320994182972, 'F1-Macro': 0.6772821856841472, 'F1-Weighted': 0.9358883174971973, 'F1_Class 0': 0.9553191489361703, 'F1_Class 1': 0.9252669039145908, 'F1_Class 2': 0.15126050420168066}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 129 at rate 0.1\n",
      "> Train with CF samples 7692\n",
      "{'Accuracy': 0.9524061343204654, 'F1-Macro': 0.767524878166491, 'F1-Weighted': 0.9555562028284543, 'F1_Class 0': 0.9777158774373259, 'F1_Class 1': 0.9200968523002421, 'F1_Class 2': 0.4047619047619047}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 126 at rate 0.1\n",
      "> Train with CF samples 7690\n",
      "{'Accuracy': 0.7608465608465609, 'F1-Macro': 0.5494325931702939, 'F1-Weighted': 0.7840950921984619, 'F1_Class 0': 0.8306581059390049, 'F1_Class 1': 0.6694915254237288, 'F1_Class 2': 0.14814814814814814}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 263 at rate 0.2\n",
      "> Train with CF samples 7826\n",
      "{'Accuracy': 0.8149127445795875, 'F1-Macro': 0.5739768091407035, 'F1-Weighted': 0.8398984162941902, 'F1_Class 0': 0.9062500000000001, 'F1_Class 1': 0.6605783866057839, 'F1_Class 2': 0.15510204081632653}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 255 at rate 0.2\n",
      "> Train with CF samples 7818\n",
      "{'Accuracy': 0.8863035430988895, 'F1-Macro': 0.6753790756308006, 'F1-Weighted': 0.8958478323926661, 'F1_Class 0': 0.9339298004129387, 'F1_Class 1': 0.8086253369272238, 'F1_Class 2': 0.2835820895522388}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 262 at rate 0.2\n",
      "> Train with CF samples 7825\n",
      "{'Accuracy': 0.9222633527234267, 'F1-Macro': 0.677174040082179, 'F1-Weighted': 0.9332833590063109, 'F1_Class 0': 0.954239091876552, 'F1_Class 1': 0.9165687426556993, 'F1_Class 2': 0.16071428571428573}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 258 at rate 0.2\n",
      "> Train with CF samples 7821\n",
      "{'Accuracy': 0.9534637757800106, 'F1-Macro': 0.7713695028709067, 'F1-Weighted': 0.9562785276716148, 'F1_Class 0': 0.9780717020536026, 'F1_Class 1': 0.9214026602176542, 'F1_Class 2': 0.4146341463414634}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 251 at rate 0.2\n",
      "> Train with CF samples 7815\n",
      "{'Accuracy': 0.7656084656084656, 'F1-Macro': 0.5530415087123838, 'F1-Weighted': 0.7882337312975667, 'F1_Class 0': 0.8344, 'F1_Class 1': 0.6751918158567773, 'F1_Class 2': 0.14953271028037385}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 395 at rate 0.30000000000000004\n",
      "> Train with CF samples 7958\n",
      "{'Accuracy': 0.810153358011634, 'F1-Macro': 0.5700862926435574, 'F1-Weighted': 0.8375306055881058, 'F1_Class 0': 0.9038997214484681, 'F1_Class 1': 0.6584992343032159, 'F1_Class 2': 0.14785992217898833}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 383 at rate 0.30000000000000004\n",
      "> Train with CF samples 7946\n",
      "{'Accuracy': 0.886832363828662, 'F1-Macro': 0.6806999842654554, 'F1-Weighted': 0.8965212457805948, 'F1_Class 0': 0.9334712168217856, 'F1_Class 1': 0.8123324396782842, 'F1_Class 2': 0.2962962962962963}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 394 at rate 0.30000000000000004\n",
      "> Train with CF samples 7957\n",
      "{'Accuracy': 0.9238498149127445, 'F1-Macro': 0.6895862821813908, 'F1-Weighted': 0.935026857187643, 'F1_Class 0': 0.9538352272727272, 'F1_Class 1': 0.9236192714453585, 'F1_Class 2': 0.19130434782608696}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 387 at rate 0.30000000000000004\n",
      "> Train with CF samples 7950\n",
      "{'Accuracy': 0.9502908514013749, 'F1-Macro': 0.7668285164366512, 'F1-Weighted': 0.9535500204564166, 'F1_Class 0': 0.9748603351955307, 'F1_Class 1': 0.920863309352518, 'F1_Class 2': 0.4047619047619047}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 377 at rate 0.30000000000000004\n",
      "> Train with CF samples 7941\n",
      "{'Accuracy': 0.7634920634920634, 'F1-Macro': 0.5547430215123773, 'F1-Weighted': 0.7852889418841986, 'F1_Class 0': 0.8314606741573034, 'F1_Class 1': 0.671152228763667, 'F1_Class 2': 0.1616161616161616}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 526 at rate 0.4\n",
      "> Train with CF samples 8089\n",
      "{'Accuracy': 0.8122686409307245, 'F1-Macro': 0.5699618299482959, 'F1-Weighted': 0.8385714481995548, 'F1_Class 0': 0.9047287899860919, 'F1_Class 1': 0.6605783866057839, 'F1_Class 2': 0.14457831325301204}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 510 at rate 0.4\n",
      "> Train with CF samples 8073\n",
      "{'Accuracy': 0.8905341089370703, 'F1-Macro': 0.6874704393452316, 'F1-Weighted': 0.8987293075660875, 'F1_Class 0': 0.9353951890034364, 'F1_Class 1': 0.8145161290322581, 'F1_Class 2': 0.3125}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 525 at rate 0.4\n",
      "> Train with CF samples 8088\n",
      "{'Accuracy': 0.920676890534109, 'F1-Macro': 0.6854320479344254, 'F1-Weighted': 0.9327632384100918, 'F1_Class 0': 0.9523131672597864, 'F1_Class 1': 0.9191090269636577, 'F1_Class 2': 0.18487394957983191}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 516 at rate 0.4\n",
      "> Train with CF samples 8079\n",
      "{'Accuracy': 0.9502908514013749, 'F1-Macro': 0.7516770608620685, 'F1-Weighted': 0.9535330667502909, 'F1_Class 0': 0.9770194986072424, 'F1_Class 1': 0.916565900846433, 'F1_Class 2': 0.36144578313253006}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 502 at rate 0.4\n",
      "> Train with CF samples 8066\n",
      "{'Accuracy': 0.7634920634920634, 'F1-Macro': 0.548753301106596, 'F1-Weighted': 0.7854940767182754, 'F1_Class 0': 0.8331996792301525, 'F1_Class 1': 0.6672268907563025, 'F1_Class 2': 0.14583333333333331}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 658 at rate 0.5\n",
      "> Train with CF samples 8221\n",
      "{'Accuracy': 0.8138551031200423, 'F1-Macro': 0.5718178375304357, 'F1-Weighted': 0.840506180079699, 'F1_Class 0': 0.9058046576294753, 'F1_Class 1': 0.665648854961832, 'F1_Class 2': 0.144}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 638 at rate 0.5\n",
      "> Train with CF samples 8201\n",
      "{'Accuracy': 0.8900052882072977, 'F1-Macro': 0.6822621060118982, 'F1-Weighted': 0.8984814228490067, 'F1_Class 0': 0.9353951890034364, 'F1_Class 1': 0.8145161290322581, 'F1_Class 2': 0.296875}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 656 at rate 0.5\n",
      "> Train with CF samples 8219\n",
      "{'Accuracy': 0.9227921734531994, 'F1-Macro': 0.6819072882196499, 'F1-Weighted': 0.9351035491111322, 'F1_Class 0': 0.9545777146912705, 'F1_Class 1': 0.923076923076923, 'F1_Class 2': 0.1680672268907563}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 644 at rate 0.5\n",
      "> Train with CF samples 8207\n",
      "{'Accuracy': 0.9508196721311475, 'F1-Macro': 0.7610855055051101, 'F1-Weighted': 0.9540534472209469, 'F1_Class 0': 0.9755586592178771, 'F1_Class 1': 0.9221556886227545, 'F1_Class 2': 0.3855421686746988}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 628 at rate 0.5\n",
      "> Train with CF samples 8192\n",
      "{'Accuracy': 0.771957671957672, 'F1-Macro': 0.561821206549339, 'F1-Weighted': 0.7925776715963214, 'F1_Class 0': 0.8395061728395061, 'F1_Class 1': 0.6757446808510639, 'F1_Class 2': 0.1702127659574468}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 789 at rate 0.6\n",
      "> Train with CF samples 8352\n",
      "{'Accuracy': 0.8149127445795875, 'F1-Macro': 0.5753947782972558, 'F1-Weighted': 0.8403942354979701, 'F1_Class 0': 0.9054242002781641, 'F1_Class 1': 0.6656580937972769, 'F1_Class 2': 0.15510204081632653}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 766 at rate 0.6\n",
      "> Train with CF samples 8329\n",
      "{'Accuracy': 0.8915917503966155, 'F1-Macro': 0.6867988246815825, 'F1-Weighted': 0.9001896933291461, 'F1_Class 0': 0.9370917841182538, 'F1_Class 1': 0.8156123822341859, 'F1_Class 2': 0.30769230769230765}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 787 at rate 0.6\n",
      "> Train with CF samples 8350\n",
      "{'Accuracy': 0.9217345319936542, 'F1-Macro': 0.6766305412498345, 'F1-Weighted': 0.9329964860999133, 'F1_Class 0': 0.9538352272727272, 'F1_Class 1': 0.9167643610785463, 'F1_Class 2': 0.1592920353982301}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 773 at rate 0.6\n",
      "> Train with CF samples 8336\n",
      "{'Accuracy': 0.9476467477525119, 'F1-Macro': 0.765602011083812, 'F1-Weighted': 0.9507753578434406, 'F1_Class 0': 0.9730863334498426, 'F1_Class 1': 0.9140811455847255, 'F1_Class 2': 0.4096385542168675}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 754 at rate 0.6\n",
      "> Train with CF samples 8318\n",
      "{'Accuracy': 0.7687830687830688, 'F1-Macro': 0.5587277950387473, 'F1-Weighted': 0.789998118295163, 'F1_Class 0': 0.8371907422186753, 'F1_Class 1': 0.6723259762308997, 'F1_Class 2': 0.16666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 921 at rate 0.7000000000000001\n",
      "> Train with CF samples 8484\n",
      "{'Accuracy': 0.8122686409307245, 'F1-Macro': 0.5693235541473528, 'F1-Weighted': 0.8389895942738299, 'F1_Class 0': 0.9036521739130435, 'F1_Class 1': 0.6666666666666666, 'F1_Class 2': 0.13765182186234817}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 893 at rate 0.7000000000000001\n",
      "> Train with CF samples 8456\n",
      "{'Accuracy': 0.8905341089370703, 'F1-Macro': 0.6834376352654051, 'F1-Weighted': 0.898515163671394, 'F1_Class 0': 0.9358050120151047, 'F1_Class 1': 0.812920592193809, 'F1_Class 2': 0.3015873015873016}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 918 at rate 0.7000000000000001\n",
      "> Train with CF samples 8481\n",
      "{'Accuracy': 0.9222633527234267, 'F1-Macro': 0.6869709246768543, 'F1-Weighted': 0.9339231895991046, 'F1_Class 0': 0.9537695590327169, 'F1_Class 1': 0.9191090269636577, 'F1_Class 2': 0.18803418803418806}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 902 at rate 0.7000000000000001\n",
      "> Train with CF samples 8465\n",
      "{'Accuracy': 0.9492332099418297, 'F1-Macro': 0.763231102520637, 'F1-Weighted': 0.9519518811275757, 'F1_Class 0': 0.9745200698080281, 'F1_Class 1': 0.9151732377538829, 'F1_Class 2': 0.4}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 879 at rate 0.7000000000000001\n",
      "> Train with CF samples 8443\n",
      "{'Accuracy': 0.7714285714285715, 'F1-Macro': 0.5610698527279461, 'F1-Weighted': 0.7921967218253786, 'F1_Class 0': 0.8390438247011953, 'F1_Class 1': 0.6757446808510639, 'F1_Class 2': 0.16842105263157894}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1052 at rate 0.8\n",
      "> Train with CF samples 8615\n",
      "{'Accuracy': 0.8149127445795875, 'F1-Macro': 0.5731136533180031, 'F1-Weighted': 0.83991356511949, 'F1_Class 0': 0.9049271339347675, 'F1_Class 1': 0.6656534954407295, 'F1_Class 2': 0.1487603305785124}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1021 at rate 0.8\n",
      "> Train with CF samples 8584\n",
      "{'Accuracy': 0.8873611845584347, 'F1-Macro': 0.6772672611725268, 'F1-Weighted': 0.8961879630906335, 'F1_Class 0': 0.9347079037800688, 'F1_Class 1': 0.8070175438596492, 'F1_Class 2': 0.2900763358778626}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1050 at rate 0.8\n",
      "> Train with CF samples 8613\n",
      "{'Accuracy': 0.9212057112638815, 'F1-Macro': 0.6750304371702532, 'F1-Weighted': 0.9334020328656193, 'F1_Class 0': 0.9544807965860597, 'F1_Class 1': 0.9167643610785463, 'F1_Class 2': 0.15384615384615385}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1031 at rate 0.8\n",
      "> Train with CF samples 8594\n",
      "{'Accuracy': 0.9502908514013749, 'F1-Macro': 0.7659536247101463, 'F1-Weighted': 0.9528971218798143, 'F1_Class 0': 0.9748427672955975, 'F1_Class 1': 0.9179548156956006, 'F1_Class 2': 0.4050632911392405}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1005 at rate 0.8\n",
      "> Train with CF samples 8569\n",
      "{'Accuracy': 0.7751322751322751, 'F1-Macro': 0.5663448300151482, 'F1-Weighted': 0.7948651537086237, 'F1_Class 0': 0.84181240063593, 'F1_Class 1': 0.6774468085106383, 'F1_Class 2': 0.1797752808988764}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1184 at rate 0.9\n",
      "> Train with CF samples 8747\n",
      "{'Accuracy': 0.8159703860391327, 'F1-Macro': 0.5720598148691751, 'F1-Weighted': 0.841918018569974, 'F1_Class 0': 0.9065647794373045, 'F1_Class 1': 0.6696969696969697, 'F1_Class 2': 0.13991769547325103}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1148 at rate 0.9\n",
      "> Train with CF samples 8711\n",
      "{'Accuracy': 0.8889476467477525, 'F1-Macro': 0.6786703945159555, 'F1-Weighted': 0.8981184894850747, 'F1_Class 0': 0.9357167411481608, 'F1_Class 1': 0.8124156545209177, 'F1_Class 2': 0.2878787878787879}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1181 at rate 0.9\n",
      "> Train with CF samples 8744\n",
      "{'Accuracy': 0.9227921734531994, 'F1-Macro': 0.6824489036792007, 'F1-Weighted': 0.933646929446054, 'F1_Class 0': 0.955223880597015, 'F1_Class 1': 0.9135514018691588, 'F1_Class 2': 0.17857142857142858}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1160 at rate 0.9\n",
      "> Train with CF samples 8723\n",
      "{'Accuracy': 0.9502908514013749, 'F1-Macro': 0.7704439629587009, 'F1-Weighted': 0.9524172187624823, 'F1_Class 0': 0.9745022703457912, 'F1_Class 1': 0.9157769869513642, 'F1_Class 2': 0.42105263157894735}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1130 at rate 0.9\n",
      "> Train with CF samples 8694\n",
      "{'Accuracy': 0.7788359788359789, 'F1-Macro': 0.5697585630998235, 'F1-Weighted': 0.7980225469299324, 'F1_Class 0': 0.8450257629805787, 'F1_Class 1': 0.6803418803418803, 'F1_Class 2': 0.1839080459770115}\n",
      "> Total Tweets used to generate counterfactuals 1315\n",
      "> Total counterfactuals added 1315\n",
      "> Counterfactual size 1315 at rate 1.0\n",
      "> Train with CF samples 8878\n",
      "{'Accuracy': 0.8154415653093601, 'F1-Macro': 0.5714514781237892, 'F1-Weighted': 0.8411891684790551, 'F1_Class 0': 0.9061848505906879, 'F1_Class 1': 0.6676737160120846, 'F1_Class 2': 0.14049586776859502}\n",
      "> Total Tweets used to generate counterfactuals 1276\n",
      "> Total counterfactuals added 1276\n",
      "> Counterfactual size 1276 at rate 1.0\n",
      "> Train with CF samples 8839\n",
      "{'Accuracy': 0.88841882601798, 'F1-Macro': 0.6780282625643451, 'F1-Weighted': 0.8975176921782385, 'F1_Class 0': 0.9353951890034364, 'F1_Class 1': 0.8108108108108109, 'F1_Class 2': 0.2878787878787879}\n",
      "> Total Tweets used to generate counterfactuals 1312\n",
      "> Total counterfactuals added 1312\n",
      "> Counterfactual size 1312 at rate 1.0\n",
      "> Train with CF samples 8875\n",
      "{'Accuracy': 0.9249074563722898, 'F1-Macro': 0.6801598852468618, 'F1-Weighted': 0.9348731901712175, 'F1_Class 0': 0.956367506207875, 'F1_Class 1': 0.9158878504672897, 'F1_Class 2': 0.16822429906542055}\n",
      "> Total Tweets used to generate counterfactuals 1289\n",
      "> Total counterfactuals added 1289\n",
      "> Counterfactual size 1289 at rate 1.0\n",
      "> Train with CF samples 8852\n",
      "{'Accuracy': 0.9492332099418297, 'F1-Macro': 0.7685774534695553, 'F1-Weighted': 0.9515332303279835, 'F1_Class 0': 0.9730863334498426, 'F1_Class 1': 0.9170616113744076, 'F1_Class 2': 0.4155844155844156}\n",
      "> Total Tweets used to generate counterfactuals 1256\n",
      "> Total counterfactuals added 1256\n",
      "> Counterfactual size 1256 at rate 1.0\n",
      "> Train with CF samples 8820\n",
      "{'Accuracy': 0.7777777777777778, 'F1-Macro': 0.5693629058963241, 'F1-Weighted': 0.7970716268213698, 'F1_Class 0': 0.84456780333069, 'F1_Class 1': 0.6774744027303754, 'F1_Class 2': 0.18604651162790697}\n",
      "Religion (10869, 4)\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.8229070837166513, 'F1-Macro': 0.5420273740003263, 'F1-Weighted': 0.8325252033397752, 'F1_Class 0': 0.9098017313599553, 'F1_Class 1': 0.5171102661596958, 'F1_Class 2': 0.19917012448132781}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.8827046918123275, 'F1-Macro': 0.6589842071098159, 'F1-Weighted': 0.8973295004541658, 'F1_Class 0': 0.936462507155123, 'F1_Class 1': 0.7759398496240602, 'F1_Class 2': 0.26455026455026454}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.8817847286108556, 'F1-Macro': 0.66175201957015, 'F1-Weighted': 0.8969835272096762, 'F1_Class 0': 0.931673422119282, 'F1_Class 1': 0.7994389901823281, 'F1_Class 2': 0.2541436464088398}\n",
      "> Train samples 8695\n",
      "{'Accuracy': 0.9236430542778289, 'F1-Macro': 0.6894889630508386, 'F1-Weighted': 0.9259291284286543, 'F1_Class 0': 0.9607508532423208, 'F1_Class 1': 0.8281461434370772, 'F1_Class 2': 0.27956989247311825}\n",
      "> Train samples 8696\n",
      "{'Accuracy': 0.5918085595950299, 'F1-Macro': 0.4223111029224862, 'F1-Weighted': 0.6390774039832672, 'F1_Class 0': 0.6898826979472141, 'F1_Class 1': 0.44371727748691103, 'F1_Class 2': 0.1333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 123 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.8238270469181233, 'F1-Macro': 0.5445622282462071, 'F1-Weighted': 0.8337494889691104, 'F1_Class 0': 0.9098017313599553, 'F1_Class 1': 0.5247148288973383, 'F1_Class 2': 0.19917012448132781}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8817\n",
      "{'Accuracy': 0.8840846366145354, 'F1-Macro': 0.6583151286901855, 'F1-Weighted': 0.8982175864755964, 'F1_Class 0': 0.9369112189551814, 'F1_Class 1': 0.7799696509863431, 'F1_Class 2': 0.2580645161290323}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 123 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.8831646734130635, 'F1-Macro': 0.6665821858015827, 'F1-Weighted': 0.8975575859114828, 'F1_Class 0': 0.9323308270676692, 'F1_Class 1': 0.797752808988764, 'F1_Class 2': 0.2696629213483146}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8817\n",
      "{'Accuracy': 0.9250229990800368, 'F1-Macro': 0.6993262163862107, 'F1-Weighted': 0.9279362039948745, 'F1_Class 0': 0.9616150127949957, 'F1_Class 1': 0.8333333333333334, 'F1_Class 2': 0.30303030303030304}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 122 at rate 0.1\n",
      "> Train with CF samples 8818\n",
      "{'Accuracy': 0.5941095260009204, 'F1-Macro': 0.43073330403114135, 'F1-Weighted': 0.64144632972466, 'F1_Class 0': 0.692054192603442, 'F1_Class 1': 0.44459016393442624, 'F1_Class 2': 0.15555555555555559}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 246 at rate 0.2\n",
      "> Train with CF samples 8941\n",
      "{'Accuracy': 0.8238270469181233, 'F1-Macro': 0.5441663258814976, 'F1-Weighted': 0.8333071236515315, 'F1_Class 0': 0.9095982142857143, 'F1_Class 1': 0.5229007633587786, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 245 at rate 0.2\n",
      "> Train with CF samples 8940\n",
      "{'Accuracy': 0.8817847286108556, 'F1-Macro': 0.6549048840810855, 'F1-Weighted': 0.897061334452523, 'F1_Class 0': 0.9359267734553776, 'F1_Class 1': 0.7787878787878788, 'F1_Class 2': 0.25}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 247 at rate 0.2\n",
      "> Train with CF samples 8942\n",
      "{'Accuracy': 0.8891444342226311, 'F1-Macro': 0.6745266347442471, 'F1-Weighted': 0.9031813839612218, 'F1_Class 0': 0.9353721869590305, 'F1_Class 1': 0.8170212765957449, 'F1_Class 2': 0.2711864406779661}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 244 at rate 0.2\n",
      "> Train with CF samples 8939\n",
      "{'Accuracy': 0.9268629254829807, 'F1-Macro': 0.6906288699141839, 'F1-Weighted': 0.9291142284848369, 'F1_Class 0': 0.9628790025502976, 'F1_Class 1': 0.8381742738589212, 'F1_Class 2': 0.27083333333333326}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 245 at rate 0.2\n",
      "> Train with CF samples 8941\n",
      "{'Accuracy': 0.5959502991256328, 'F1-Macro': 0.41668149528415394, 'F1-Weighted': 0.6432879524906954, 'F1_Class 0': 0.6951442132165025, 'F1_Class 1': 0.44620462046204623, 'F1_Class 2': 0.10869565217391304}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 369 at rate 0.30000000000000004\n",
      "> Train with CF samples 9064\n",
      "{'Accuracy': 0.8256669733210672, 'F1-Macro': 0.5460508083676432, 'F1-Weighted': 0.8346130663267708, 'F1_Class 0': 0.9105600445806631, 'F1_Class 1': 0.525911708253359, 'F1_Class 2': 0.2016806722689076}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9062\n",
      "{'Accuracy': 0.8840846366145354, 'F1-Macro': 0.6561205935555711, 'F1-Weighted': 0.8980798550087262, 'F1_Class 0': 0.9368751785204228, 'F1_Class 1': 0.780120481927711, 'F1_Class 2': 0.25136612021857924}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 370 at rate 0.30000000000000004\n",
      "> Train with CF samples 9065\n",
      "{'Accuracy': 0.8882244710211592, 'F1-Macro': 0.6761221381109753, 'F1-Weighted': 0.9025256642200002, 'F1_Class 0': 0.9349522983521248, 'F1_Class 1': 0.8140845070422535, 'F1_Class 2': 0.2793296089385475}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9062\n",
      "{'Accuracy': 0.9310027598896045, 'F1-Macro': 0.6858150021336473, 'F1-Weighted': 0.9320568440073541, 'F1_Class 0': 0.9658095507205425, 'F1_Class 1': 0.8444444444444444, 'F1_Class 2': 0.24719101123595508}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 367 at rate 0.30000000000000004\n",
      "> Train with CF samples 9063\n",
      "{'Accuracy': 0.5991716520938795, 'F1-Macro': 0.4338732282640183, 'F1-Weighted': 0.6463145568292697, 'F1_Class 0': 0.6975218658892128, 'F1_Class 1': 0.44679444811632524, 'F1_Class 2': 0.15730337078651688}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 492 at rate 0.4\n",
      "> Train with CF samples 9187\n",
      "{'Accuracy': 0.828426862925483, 'F1-Macro': 0.5513659397464437, 'F1-Weighted': 0.837550207347613, 'F1_Class 0': 0.9116745611590972, 'F1_Class 1': 0.5381679389312977, 'F1_Class 2': 0.20425531914893616}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 489 at rate 0.4\n",
      "> Train with CF samples 9184\n",
      "{'Accuracy': 0.8859245630174793, 'F1-Macro': 0.6581603568768867, 'F1-Weighted': 0.8987068174448268, 'F1_Class 0': 0.9381590196637218, 'F1_Class 1': 0.7764350453172206, 'F1_Class 2': 0.2598870056497175}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 494 at rate 0.4\n",
      "> Train with CF samples 9189\n",
      "{'Accuracy': 0.8891444342226311, 'F1-Macro': 0.6709621820841353, 'F1-Weighted': 0.9027887554260922, 'F1_Class 0': 0.9362561292183443, 'F1_Class 1': 0.8107344632768362, 'F1_Class 2': 0.2658959537572254}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 489 at rate 0.4\n",
      "> Train with CF samples 9184\n",
      "{'Accuracy': 0.9356025758969642, 'F1-Macro': 0.7020981200504343, 'F1-Weighted': 0.9370405046945727, 'F1_Class 0': 0.9683794466403162, 'F1_Class 1': 0.8583450210378681, 'F1_Class 2': 0.27956989247311825}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 490 at rate 0.4\n",
      "> Train with CF samples 9186\n",
      "{'Accuracy': 0.6074551311550851, 'F1-Macro': 0.43525122787808385, 'F1-Weighted': 0.6545506439715465, 'F1_Class 0': 0.7069027827972533, 'F1_Class 1': 0.4514824797843667, 'F1_Class 2': 0.1473684210526316}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 614 at rate 0.5\n",
      "> Train with CF samples 9309\n",
      "{'Accuracy': 0.8252069917203312, 'F1-Macro': 0.5472346962228146, 'F1-Weighted': 0.834464770234557, 'F1_Class 0': 0.9096486335750139, 'F1_Class 1': 0.5295238095238096, 'F1_Class 2': 0.20253164556962025}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 612 at rate 0.5\n",
      "> Train with CF samples 9307\n",
      "{'Accuracy': 0.8836246550137995, 'F1-Macro': 0.6487236450845466, 'F1-Weighted': 0.8976863983141847, 'F1_Class 0': 0.9372862029646523, 'F1_Class 1': 0.7781155015197568, 'F1_Class 2': 0.23076923076923078}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 617 at rate 0.5\n",
      "> Train with CF samples 9312\n",
      "{'Accuracy': 0.891444342226311, 'F1-Macro': 0.6791833594054338, 'F1-Weighted': 0.9049969416811351, 'F1_Class 0': 0.937481993661769, 'F1_Class 1': 0.8159771754636232, 'F1_Class 2': 0.2840909090909091}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 611 at rate 0.5\n",
      "> Train with CF samples 9306\n",
      "{'Accuracy': 0.9383624655013799, 'F1-Macro': 0.6993452683730531, 'F1-Weighted': 0.9391504696800227, 'F1_Class 0': 0.9702079820123665, 'F1_Class 1': 0.8640915593705293, 'F1_Class 2': 0.2637362637362637}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 612 at rate 0.5\n",
      "> Train with CF samples 9308\n",
      "{'Accuracy': 0.6088357109986194, 'F1-Macro': 0.4311071377256696, 'F1-Weighted': 0.655176313594611, 'F1_Class 0': 0.7075812274368232, 'F1_Class 1': 0.45387205387205387, 'F1_Class 2': 0.13186813186813187}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 737 at rate 0.6\n",
      "> Train with CF samples 9432\n",
      "{'Accuracy': 0.8247470101195952, 'F1-Macro': 0.5439136700992372, 'F1-Weighted': 0.8335673766444166, 'F1_Class 0': 0.9094455280022291, 'F1_Class 1': 0.5257142857142858, 'F1_Class 2': 0.19658119658119658}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 734 at rate 0.6\n",
      "> Train with CF samples 9429\n",
      "{'Accuracy': 0.8863845446182153, 'F1-Macro': 0.6562382901688218, 'F1-Weighted': 0.8997242669004212, 'F1_Class 0': 0.9383913291500285, 'F1_Class 1': 0.7831325301204819, 'F1_Class 2': 0.24719101123595508}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 740 at rate 0.6\n",
      "> Train with CF samples 9435\n",
      "{'Accuracy': 0.8932842686292548, 'F1-Macro': 0.679455094507062, 'F1-Weighted': 0.9054188525199539, 'F1_Class 0': 0.9390103567318757, 'F1_Class 1': 0.810198300283286, 'F1_Class 2': 0.2891566265060241}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 733 at rate 0.6\n",
      "> Train with CF samples 9428\n",
      "{'Accuracy': 0.9402023919043239, 'F1-Macro': 0.7039463673452362, 'F1-Weighted': 0.9406419749271179, 'F1_Class 0': 0.9710267229254571, 'F1_Class 1': 0.8680851063829786, 'F1_Class 2': 0.2727272727272727}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 734 at rate 0.6\n",
      "> Train with CF samples 9430\n",
      "{'Accuracy': 0.6143580303727566, 'F1-Macro': 0.4379193246222865, 'F1-Weighted': 0.6610607918608234, 'F1_Class 0': 0.7138749101365925, 'F1_Class 1': 0.4570259208731242, 'F1_Class 2': 0.14285714285714285}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 860 at rate 0.7000000000000001\n",
      "> Train with CF samples 9555\n",
      "{'Accuracy': 0.827046918123275, 'F1-Macro': 0.5513267475011048, 'F1-Weighted': 0.8362745657757994, 'F1_Class 0': 0.9099526066350712, 'F1_Class 1': 0.538899430740038, 'F1_Class 2': 0.2051282051282051}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 856 at rate 0.7000000000000001\n",
      "> Train with CF samples 9551\n",
      "{'Accuracy': 0.8886844526218951, 'F1-Macro': 0.6624794895309655, 'F1-Weighted': 0.901749391197979, 'F1_Class 0': 0.939298945568538, 'F1_Class 1': 0.7897125567322237, 'F1_Class 2': 0.25842696629213485}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 864 at rate 0.7000000000000001\n",
      "> Train with CF samples 9559\n",
      "{'Accuracy': 0.889604415823367, 'F1-Macro': 0.6735366685483046, 'F1-Weighted': 0.9029866190834125, 'F1_Class 0': 0.9353348729792148, 'F1_Class 1': 0.8162692847124825, 'F1_Class 2': 0.26900584795321636}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 855 at rate 0.7000000000000001\n",
      "> Train with CF samples 9550\n",
      "{'Accuracy': 0.9388224471021159, 'F1-Macro': 0.7030691049251674, 'F1-Weighted': 0.9392035478873598, 'F1_Class 0': 0.9701576576576577, 'F1_Class 1': 0.8631875881523273, 'F1_Class 2': 0.2758620689655172}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 857 at rate 0.7000000000000001\n",
      "> Train with CF samples 9553\n",
      "{'Accuracy': 0.6194201564657156, 'F1-Macro': 0.43721299333601166, 'F1-Weighted': 0.6649940231980019, 'F1_Class 0': 0.7180222142601218, 'F1_Class 1': 0.46174863387978143, 'F1_Class 2': 0.13186813186813187}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 983 at rate 0.8\n",
      "> Train with CF samples 9678\n",
      "{'Accuracy': 0.8298068077276909, 'F1-Macro': 0.5541108129871429, 'F1-Weighted': 0.8382742038330758, 'F1_Class 0': 0.9115191986644406, 'F1_Class 1': 0.5430210325047802, 'F1_Class 2': 0.20779220779220778}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 978 at rate 0.8\n",
      "> Train with CF samples 9673\n",
      "{'Accuracy': 0.8868445262189513, 'F1-Macro': 0.6580823089228346, 'F1-Weighted': 0.9005639738364545, 'F1_Class 0': 0.9380177092259354, 'F1_Class 1': 0.7904191616766467, 'F1_Class 2': 0.2458100558659218}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 987 at rate 0.8\n",
      "> Train with CF samples 9682\n",
      "{'Accuracy': 0.890064397424103, 'F1-Macro': 0.672030618643728, 'F1-Weighted': 0.9026087156684809, 'F1_Class 0': 0.9372481289579735, 'F1_Class 1': 0.8033946251768034, 'F1_Class 2': 0.2754491017964072}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 978 at rate 0.8\n",
      "> Train with CF samples 9673\n",
      "{'Accuracy': 0.9402023919043239, 'F1-Macro': 0.7026231528761041, 'F1-Weighted': 0.9405739712554423, 'F1_Class 0': 0.971894322653176, 'F1_Class 1': 0.8632478632478634, 'F1_Class 2': 0.2727272727272727}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 979 at rate 0.8\n",
      "> Train with CF samples 9675\n",
      "{'Accuracy': 0.6152784169351128, 'F1-Macro': 0.43449968981949333, 'F1-Weighted': 0.661563989964861, 'F1_Class 0': 0.7147988505747126, 'F1_Class 1': 0.4568320870156356, 'F1_Class 2': 0.13186813186813187}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 1106 at rate 0.9\n",
      "> Train with CF samples 9801\n",
      "{'Accuracy': 0.8293468261269549, 'F1-Macro': 0.5544143607760431, 'F1-Weighted': 0.8388668554041778, 'F1_Class 0': 0.9116253136325619, 'F1_Class 1': 0.5464895635673624, 'F1_Class 2': 0.2051282051282051}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 1101 at rate 0.9\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.8873045078196872, 'F1-Macro': 0.6569602828716716, 'F1-Weighted': 0.9006892621979133, 'F1_Class 0': 0.9390313390313391, 'F1_Class 1': 0.7860394537177541, 'F1_Class 2': 0.2458100558659218}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 1111 at rate 0.9\n",
      "> Train with CF samples 9806\n",
      "{'Accuracy': 0.8919043238270469, 'F1-Macro': 0.6723004706159493, 'F1-Weighted': 0.903731411426951, 'F1_Class 0': 0.9379310344827586, 'F1_Class 1': 0.8073654390934845, 'F1_Class 2': 0.271604938271605}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 1100 at rate 0.9\n",
      "> Train with CF samples 9795\n",
      "{'Accuracy': 0.9415823367065317, 'F1-Macro': 0.6999894063045525, 'F1-Weighted': 0.9414657614103212, 'F1_Class 0': 0.9721988205560236, 'F1_Class 1': 0.868945868945869, 'F1_Class 2': 0.25882352941176473}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 1102 at rate 0.9\n",
      "> Train with CF samples 9798\n",
      "{'Accuracy': 0.6221813161527842, 'F1-Macro': 0.43210765300338044, 'F1-Weighted': 0.6676853054506453, 'F1_Class 0': 0.7216863165416221, 'F1_Class 1': 0.46227709190672145, 'F1_Class 2': 0.11235955056179774}\n",
      "> Total Tweets used to generate counterfactuals 1229\n",
      "> Total counterfactuals added 1229\n",
      "> Counterfactual size 1229 at rate 1.0\n",
      "> Train with CF samples 9924\n",
      "{'Accuracy': 0.8293468261269549, 'F1-Macro': 0.5537112793519346, 'F1-Weighted': 0.8380083078042752, 'F1_Class 0': 0.9112162538268856, 'F1_Class 1': 0.5430210325047802, 'F1_Class 2': 0.20689655172413793}\n",
      "> Total Tweets used to generate counterfactuals 1223\n",
      "> Total counterfactuals added 1223\n",
      "> Counterfactual size 1223 at rate 1.0\n",
      "> Train with CF samples 9918\n",
      "{'Accuracy': 0.8873045078196872, 'F1-Macro': 0.6593667892033945, 'F1-Weighted': 0.9004498919151511, 'F1_Class 0': 0.939298945568538, 'F1_Class 1': 0.7818181818181819, 'F1_Class 2': 0.2569832402234637}\n",
      "> Total Tweets used to generate counterfactuals 1234\n",
      "> Total counterfactuals added 1234\n",
      "> Counterfactual size 1234 at rate 1.0\n",
      "> Train with CF samples 9929\n",
      "{'Accuracy': 0.8965041398344066, 'F1-Macro': 0.6785394581038706, 'F1-Weighted': 0.9075546489733402, 'F1_Class 0': 0.9406366504158303, 'F1_Class 1': 0.8165007112375534, 'F1_Class 2': 0.2784810126582279}\n",
      "> Total Tweets used to generate counterfactuals 1222\n",
      "> Total counterfactuals added 1222\n",
      "> Counterfactual size 1222 at rate 1.0\n",
      "> Train with CF samples 9917\n",
      "{'Accuracy': 0.9415823367065317, 'F1-Macro': 0.6999894063045525, 'F1-Weighted': 0.9414657614103212, 'F1_Class 0': 0.9721988205560236, 'F1_Class 1': 0.868945868945869, 'F1_Class 2': 0.25882352941176473}\n",
      "> Total Tweets used to generate counterfactuals 1224\n",
      "> Total counterfactuals added 1224\n",
      "> Counterfactual size 1224 at rate 1.0\n",
      "> Train with CF samples 9920\n",
      "{'Accuracy': 0.6254026691210308, 'F1-Macro': 0.43391401807829527, 'F1-Weighted': 0.6706556932768475, 'F1_Class 0': 0.7248752672843906, 'F1_Class 1': 0.46450723638869745, 'F1_Class 2': 0.11235955056179774}\n",
      "Race (12013, 4)\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9113607990012484, 'F1-Macro': 0.6416349182316438, 'F1-Weighted': 0.9101165026024023, 'F1_Class 0': 0.9577861163227017, 'F1_Class 1': 0.6272493573264781, 'F1_Class 2': 0.3398692810457516}\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9288389513108615, 'F1-Macro': 0.6807926379665172, 'F1-Weighted': 0.933180897946766, 'F1_Class 0': 0.9693076374018559, 'F1_Class 1': 0.7633928571428572, 'F1_Class 2': 0.30967741935483867}\n",
      "> Train samples 9610\n",
      "{'Accuracy': 0.9234290470245526, 'F1-Macro': 0.6721202805446836, 'F1-Weighted': 0.9334304066890722, 'F1_Class 0': 0.9663030303030303, 'F1_Class 1': 0.8086785009861933, 'F1_Class 2': 0.2413793103448276}\n",
      "> Train samples 9611\n",
      "{'Accuracy': 0.9338051623646961, 'F1-Macro': 0.718173842860652, 'F1-Weighted': 0.9395053723609454, 'F1_Class 0': 0.971912832929782, 'F1_Class 1': 0.782608695652174, 'F1_Class 2': 0.39999999999999997}\n",
      "> Train samples 9611\n",
      "{'Accuracy': 0.5274771024146545, 'F1-Macro': 0.4527282628767111, 'F1-Weighted': 0.6063567689955937, 'F1_Class 0': 0.6470022443090735, 'F1_Class 1': 0.29310344827586204, 'F1_Class 2': 0.4180790960451978}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 88 at rate 0.1\n",
      "> Train with CF samples 9698\n",
      "{'Accuracy': 0.9109446525176862, 'F1-Macro': 0.6414466854705478, 'F1-Weighted': 0.9100228522027788, 'F1_Class 0': 0.957521708519127, 'F1_Class 1': 0.629156010230179, 'F1_Class 2': 0.3376623376623376}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 89 at rate 0.1\n",
      "> Train with CF samples 9699\n",
      "{'Accuracy': 0.9288389513108615, 'F1-Macro': 0.6800661217393279, 'F1-Weighted': 0.9341745329668164, 'F1_Class 0': 0.969740290683822, 'F1_Class 1': 0.7723214285714286, 'F1_Class 2': 0.2981366459627329}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 87 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.9259259259259259, 'F1-Macro': 0.6763366995550072, 'F1-Weighted': 0.9353405854138388, 'F1_Class 0': 0.9675544794188862, 'F1_Class 1': 0.8158415841584159, 'F1_Class 2': 0.24561403508771928}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 86 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.9350541215653622, 'F1-Macro': 0.7191098209966134, 'F1-Weighted': 0.9401819238684745, 'F1_Class 0': 0.9724238026124818, 'F1_Class 1': 0.7849056603773584, 'F1_Class 2': 0.39999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 86 at rate 0.1\n",
      "> Train with CF samples 9697\n",
      "{'Accuracy': 0.525811823480433, 'F1-Macro': 0.4508947776590919, 'F1-Weighted': 0.6051876712534553, 'F1_Class 0': 0.6461538461538461, 'F1_Class 1': 0.28894806924101196, 'F1_Class 2': 0.4175824175824176}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 176 at rate 0.2\n",
      "> Train with CF samples 9786\n",
      "{'Accuracy': 0.9146899708697461, 'F1-Macro': 0.6490259792747789, 'F1-Weighted': 0.913060987166304, 'F1_Class 0': 0.9592123769338959, 'F1_Class 1': 0.6430379746835443, 'F1_Class 2': 0.3448275862068966}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 179 at rate 0.2\n",
      "> Train with CF samples 9789\n",
      "{'Accuracy': 0.9317519766957969, 'F1-Macro': 0.6840333872144136, 'F1-Weighted': 0.9353374715818745, 'F1_Class 0': 0.9700285442435775, 'F1_Class 1': 0.7807017543859649, 'F1_Class 2': 0.3013698630136986}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 173 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.9263420724094882, 'F1-Macro': 0.6768092266248793, 'F1-Weighted': 0.9353436726432544, 'F1_Class 0': 0.9678044057129024, 'F1_Class 1': 0.8126232741617356, 'F1_Class 2': 0.25}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 172 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.9363030807660283, 'F1-Macro': 0.7155733985961734, 'F1-Weighted': 0.9413808530355268, 'F1_Class 0': 0.9736651365064025, 'F1_Class 1': 0.7900763358778625, 'F1_Class 2': 0.3829787234042553}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 172 at rate 0.2\n",
      "> Train with CF samples 9783\n",
      "{'Accuracy': 0.5287260616153205, 'F1-Macro': 0.4507392425184174, 'F1-Weighted': 0.6082733638659075, 'F1_Class 0': 0.649616368286445, 'F1_Class 1': 0.2908366533864542, 'F1_Class 2': 0.411764705882353}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 264 at rate 0.30000000000000004\n",
      "> Train with CF samples 9874\n",
      "{'Accuracy': 0.914273824386184, 'F1-Macro': 0.6512354747749011, 'F1-Weighted': 0.9126801161580306, 'F1_Class 0': 0.9587435536802625, 'F1_Class 1': 0.6412213740458015, 'F1_Class 2': 0.35374149659863946}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 268 at rate 0.30000000000000004\n",
      "> Train with CF samples 9878\n",
      "{'Accuracy': 0.9309196837286725, 'F1-Macro': 0.6792282249941621, 'F1-Weighted': 0.9347196340065624, 'F1_Class 0': 0.9695528068506185, 'F1_Class 1': 0.7824175824175824, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 260 at rate 0.30000000000000004\n",
      "> Train with CF samples 9870\n",
      "{'Accuracy': 0.924261339991677, 'F1-Macro': 0.6727972541718193, 'F1-Weighted': 0.9333813944117132, 'F1_Class 0': 0.9668200532816662, 'F1_Class 1': 0.8015717092337917, 'F1_Class 2': 0.25}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 259 at rate 0.30000000000000004\n",
      "> Train with CF samples 9870\n",
      "{'Accuracy': 0.9383846794338052, 'F1-Macro': 0.7249935467157934, 'F1-Weighted': 0.9432339658423036, 'F1_Class 0': 0.9743713733075435, 'F1_Class 1': 0.7977315689981097, 'F1_Class 2': 0.4028776978417266}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 258 at rate 0.30000000000000004\n",
      "> Train with CF samples 9869\n",
      "{'Accuracy': 0.5320566194837635, 'F1-Macro': 0.45772281466950915, 'F1-Weighted': 0.6109213602895968, 'F1_Class 0': 0.6519795657726692, 'F1_Class 1': 0.29261744966442954, 'F1_Class 2': 0.4285714285714286}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 352 at rate 0.4\n",
      "> Train with CF samples 9962\n",
      "{'Accuracy': 0.9109446525176862, 'F1-Macro': 0.639540133329681, 'F1-Weighted': 0.9094979830570185, 'F1_Class 0': 0.957581438950082, 'F1_Class 1': 0.6233766233766235, 'F1_Class 2': 0.3376623376623376}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 357 at rate 0.4\n",
      "> Train with CF samples 9967\n",
      "{'Accuracy': 0.9305035372451103, 'F1-Macro': 0.673506138168829, 'F1-Weighted': 0.9355000742925412, 'F1_Class 0': 0.9709800190294957, 'F1_Class 1': 0.7820224719101124, 'F1_Class 2': 0.267515923566879}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 346 at rate 0.4\n",
      "> Train with CF samples 9956\n",
      "{'Accuracy': 0.9255097794423637, 'F1-Macro': 0.6743067608089378, 'F1-Weighted': 0.9340828489374664, 'F1_Class 0': 0.9675858732462506, 'F1_Class 1': 0.8007889546351086, 'F1_Class 2': 0.2545454545454545}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 345 at rate 0.4\n",
      "> Train with CF samples 9956\n",
      "{'Accuracy': 0.9379683597002498, 'F1-Macro': 0.7181857983828791, 'F1-Weighted': 0.9429213469157769, 'F1_Class 0': 0.974643805843999, 'F1_Class 1': 0.7969348659003832, 'F1_Class 2': 0.3829787234042553}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 344 at rate 0.4\n",
      "> Train with CF samples 9955\n",
      "{'Accuracy': 0.5320566194837635, 'F1-Macro': 0.4538657662362453, 'F1-Weighted': 0.611215340360633, 'F1_Class 0': 0.6526315789473683, 'F1_Class 1': 0.2927807486631016, 'F1_Class 2': 0.4161849710982659}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 440 at rate 0.5\n",
      "> Train with CF samples 10050\n",
      "{'Accuracy': 0.9096962130669995, 'F1-Macro': 0.6370389795255964, 'F1-Weighted': 0.9078683378193814, 'F1_Class 0': 0.9566643241977044, 'F1_Class 1': 0.6145833333333334, 'F1_Class 2': 0.3398692810457516}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 446 at rate 0.5\n",
      "> Train with CF samples 10056\n",
      "{'Accuracy': 0.9330004161464835, 'F1-Macro': 0.6802803497901899, 'F1-Weighted': 0.937022631444275, 'F1_Class 0': 0.9726646066080343, 'F1_Class 1': 0.7767857142857143, 'F1_Class 2': 0.2913907284768212}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 433 at rate 0.5\n",
      "> Train with CF samples 10043\n",
      "{'Accuracy': 0.9250936329588015, 'F1-Macro': 0.6802734115676482, 'F1-Weighted': 0.9344731509906107, 'F1_Class 0': 0.9670542635658914, 'F1_Class 1': 0.8063241106719369, 'F1_Class 2': 0.2674418604651163}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 431 at rate 0.5\n",
      "> Train with CF samples 10042\n",
      "{'Accuracy': 0.940466278101582, 'F1-Macro': 0.7291291325473918, 'F1-Weighted': 0.9449762025068158, 'F1_Class 0': 0.9755979705242812, 'F1_Class 1': 0.803030303030303, 'F1_Class 2': 0.40875912408759124}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 430 at rate 0.5\n",
      "> Train with CF samples 10041\n",
      "{'Accuracy': 0.5341382181515404, 'F1-Macro': 0.4543971926498782, 'F1-Weighted': 0.6134089307435564, 'F1_Class 0': 0.6552053486150907, 'F1_Class 1': 0.29225589225589227, 'F1_Class 2': 0.41573033707865165}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 528 at rate 0.6\n",
      "> Train with CF samples 10138\n",
      "{'Accuracy': 0.9109446525176862, 'F1-Macro': 0.6406474755876824, 'F1-Weighted': 0.9096400221491692, 'F1_Class 0': 0.9573370839193623, 'F1_Class 1': 0.6269430051813472, 'F1_Class 2': 0.3376623376623376}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 536 at rate 0.6\n",
      "> Train with CF samples 10146\n",
      "{'Accuracy': 0.9363295880149812, 'F1-Macro': 0.6894840811719131, 'F1-Weighted': 0.9394468784513725, 'F1_Class 0': 0.9726775956284153, 'F1_Class 1': 0.8, 'F1_Class 2': 0.29577464788732394}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 520 at rate 0.6\n",
      "> Train with CF samples 10130\n",
      "{'Accuracy': 0.9238451935081149, 'F1-Macro': 0.680819892954088, 'F1-Weighted': 0.9346241838981099, 'F1_Class 0': 0.9665373423860328, 'F1_Class 1': 0.813627254509018, 'F1_Class 2': 0.26229508196721313}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 517 at rate 0.6\n",
      "> Train with CF samples 10128\n",
      "{'Accuracy': 0.9367194004995837, 'F1-Macro': 0.724666873207661, 'F1-Weighted': 0.9417795894558776, 'F1_Class 0': 0.9731689630166788, 'F1_Class 1': 0.7923809523809524, 'F1_Class 2': 0.4084507042253521}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 515 at rate 0.6\n",
      "> Train with CF samples 10126\n",
      "{'Accuracy': 0.5374687760199833, 'F1-Macro': 0.45683325786140966, 'F1-Weighted': 0.6169047893992121, 'F1_Class 0': 0.6590476190476191, 'F1_Class 1': 0.29284750337381915, 'F1_Class 2': 0.4186046511627907}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 616 at rate 0.7000000000000001\n",
      "> Train with CF samples 10226\n",
      "{'Accuracy': 0.9117769454848107, 'F1-Macro': 0.6386164377351647, 'F1-Weighted': 0.9092492469548383, 'F1_Class 0': 0.9571929824561404, 'F1_Class 1': 0.6253229974160206, 'F1_Class 2': 0.3333333333333333}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 625 at rate 0.7000000000000001\n",
      "> Train with CF samples 10235\n",
      "{'Accuracy': 0.9334165626300458, 'F1-Macro': 0.6821847199756873, 'F1-Weighted': 0.9371021565731924, 'F1_Class 0': 0.9714828897338403, 'F1_Class 1': 0.7893569844789358, 'F1_Class 2': 0.2857142857142857}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 606 at rate 0.7000000000000001\n",
      "> Train with CF samples 10216\n",
      "{'Accuracy': 0.9225967540574282, 'F1-Macro': 0.6820611641232536, 'F1-Weighted': 0.9336105456576523, 'F1_Class 0': 0.9655339805825242, 'F1_Class 1': 0.810379241516966, 'F1_Class 2': 0.27027027027027023}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 603 at rate 0.7000000000000001\n",
      "> Train with CF samples 10214\n",
      "{'Accuracy': 0.9383846794338052, 'F1-Macro': 0.7266576328108779, 'F1-Weighted': 0.9435552014789913, 'F1_Class 0': 0.974135847232294, 'F1_Class 1': 0.8030592734225621, 'F1_Class 2': 0.4027777777777778}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 601 at rate 0.7000000000000001\n",
      "> Train with CF samples 10212\n",
      "{'Accuracy': 0.537052456286428, 'F1-Macro': 0.46179118524712476, 'F1-Weighted': 0.6158184126038898, 'F1_Class 0': 0.6571246819338422, 'F1_Class 1': 0.2955003357958361, 'F1_Class 2': 0.43274853801169594}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 704 at rate 0.8\n",
      "> Train with CF samples 10314\n",
      "{'Accuracy': 0.910528506034124, 'F1-Macro': 0.6340926683167868, 'F1-Weighted': 0.9087002710546489, 'F1_Class 0': 0.9564402810304449, 'F1_Class 1': 0.6307692307692307, 'F1_Class 2': 0.3150684931506849}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 714 at rate 0.8\n",
      "> Train with CF samples 10324\n",
      "{'Accuracy': 0.9338327091136079, 'F1-Macro': 0.6756920185104467, 'F1-Weighted': 0.9370683134592745, 'F1_Class 0': 0.9719848053181387, 'F1_Class 1': 0.7893569844789358, 'F1_Class 2': 0.26573426573426573}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 693 at rate 0.8\n",
      "> Train with CF samples 10303\n",
      "{'Accuracy': 0.9217644610903037, 'F1-Macro': 0.6709707014900385, 'F1-Weighted': 0.9325806336587888, 'F1_Class 0': 0.9658181818181818, 'F1_Class 1': 0.804, 'F1_Class 2': 0.24309392265193372}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 690 at rate 0.8\n",
      "> Train with CF samples 10301\n",
      "{'Accuracy': 0.9429641965029142, 'F1-Macro': 0.7384668071534319, 'F1-Weighted': 0.9477410579459272, 'F1_Class 0': 0.9763399323998069, 'F1_Class 1': 0.8223938223938224, 'F1_Class 2': 0.4166666666666667}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 687 at rate 0.8\n",
      "> Train with CF samples 10298\n",
      "{'Accuracy': 0.5387177352206495, 'F1-Macro': 0.44986172679015857, 'F1-Weighted': 0.6178988034652463, 'F1_Class 0': 0.660532994923858, 'F1_Class 1': 0.2957937584803257, 'F1_Class 2': 0.3932584269662921}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 792 at rate 0.9\n",
      "> Train with CF samples 10402\n",
      "{'Accuracy': 0.9113607990012484, 'F1-Macro': 0.632778731964349, 'F1-Weighted': 0.9097292929260151, 'F1_Class 0': 0.9571328179901616, 'F1_Class 1': 0.6377551020408163, 'F1_Class 2': 0.30344827586206896}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 804 at rate 0.9\n",
      "> Train with CF samples 10414\n",
      "{'Accuracy': 0.9359134415314191, 'F1-Macro': 0.6868721847108975, 'F1-Weighted': 0.9387249317780719, 'F1_Class 0': 0.9719714964370546, 'F1_Class 1': 0.800875273522976, 'F1_Class 2': 0.28776978417266186}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 779 at rate 0.9\n",
      "> Train with CF samples 10389\n",
      "{'Accuracy': 0.9234290470245526, 'F1-Macro': 0.6737119825593032, 'F1-Weighted': 0.9338420661780417, 'F1_Class 0': 0.9667878787878788, 'F1_Class 1': 0.8071570576540755, 'F1_Class 2': 0.24719101123595508}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 776 at rate 0.9\n",
      "> Train with CF samples 10387\n",
      "{'Accuracy': 0.9417152373022482, 'F1-Macro': 0.7394479739582186, 'F1-Weighted': 0.9459740050462286, 'F1_Class 0': 0.9751507840772015, 'F1_Class 1': 0.8115384615384617, 'F1_Class 2': 0.4316546762589928}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 773 at rate 0.9\n",
      "> Train with CF samples 10384\n",
      "{'Accuracy': 0.5412156536219817, 'F1-Macro': 0.45994345271873877, 'F1-Weighted': 0.6201217905011573, 'F1_Class 0': 0.6622306717363753, 'F1_Class 1': 0.29654705484089366, 'F1_Class 2': 0.42105263157894735}\n",
      "> Total Tweets used to generate counterfactuals 880\n",
      "> Total counterfactuals added 880\n",
      "> Counterfactual size 880 at rate 1.0\n",
      "> Train with CF samples 10490\n",
      "{'Accuracy': 0.9088639200998752, 'F1-Macro': 0.6294478083870879, 'F1-Weighted': 0.9075282555665682, 'F1_Class 0': 0.9555035128805621, 'F1_Class 1': 0.6302083333333333, 'F1_Class 2': 0.3026315789473684}\n",
      "> Total Tweets used to generate counterfactuals 893\n",
      "> Total counterfactuals added 893\n",
      "> Counterfactual size 893 at rate 1.0\n",
      "> Train with CF samples 10503\n",
      "{'Accuracy': 0.9375780274656679, 'F1-Macro': 0.6937224121644556, 'F1-Weighted': 0.9404076986634177, 'F1_Class 0': 0.9729087452471484, 'F1_Class 1': 0.8061002178649236, 'F1_Class 2': 0.3021582733812949}\n",
      "> Total Tweets used to generate counterfactuals 866\n",
      "> Total counterfactuals added 866\n",
      "> Counterfactual size 866 at rate 1.0\n",
      "> Train with CF samples 10476\n",
      "{'Accuracy': 0.9267582188930503, 'F1-Macro': 0.685139593227055, 'F1-Weighted': 0.9363400861372567, 'F1_Class 0': 0.9672885873515872, 'F1_Class 1': 0.8237623762376237, 'F1_Class 2': 0.264367816091954}\n",
      "> Total Tweets used to generate counterfactuals 862\n",
      "> Total counterfactuals added 862\n",
      "> Counterfactual size 862 at rate 1.0\n",
      "> Train with CF samples 10473\n",
      "{'Accuracy': 0.9417152373022482, 'F1-Macro': 0.7363969671091996, 'F1-Weighted': 0.9461033196107889, 'F1_Class 0': 0.9758919961427195, 'F1_Class 1': 0.8077669902912621, 'F1_Class 2': 0.425531914893617}\n",
      "> Total Tweets used to generate counterfactuals 859\n",
      "> Total counterfactuals added 859\n",
      "> Counterfactual size 859 at rate 1.0\n",
      "> Train with CF samples 10470\n",
      "{'Accuracy': 0.5407993338884263, 'F1-Macro': 0.4504640375288222, 'F1-Weighted': 0.6202371771922796, 'F1_Class 0': 0.6632911392405063, 'F1_Class 1': 0.2950373895309314, 'F1_Class 2': 0.3930635838150289}\n",
      "Politics (11018, 4)\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.7990018148820327, 'F1-Macro': 0.5264852388436424, 'F1-Weighted': 0.7820627676099755, 'F1_Class 0': 0.8963774220724515, 'F1_Class 1': 0.43779527559055115, 'F1_Class 2': 0.24528301886792453}\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.8929219600725953, 'F1-Macro': 0.6918437863244272, 'F1-Weighted': 0.9026230237462775, 'F1_Class 0': 0.9450349225630125, 'F1_Class 1': 0.816542948038176, 'F1_Class 2': 0.313953488372093}\n",
      "> Train samples 8814\n",
      "{'Accuracy': 0.8879310344827587, 'F1-Macro': 0.6719749777919631, 'F1-Weighted': 0.9050872721277107, 'F1_Class 0': 0.9489858635525508, 'F1_Class 1': 0.8216560509554139, 'F1_Class 2': 0.24528301886792453}\n",
      "> Train samples 8815\n",
      "{'Accuracy': 0.923286427598729, 'F1-Macro': 0.7404308725154426, 'F1-Weighted': 0.9290812521870049, 'F1_Class 0': 0.9673082798655668, 'F1_Class 1': 0.8511066398390342, 'F1_Class 2': 0.40287769784172667}\n",
      "> Train samples 8815\n",
      "{'Accuracy': 0.752156150703586, 'F1-Macro': 0.5823532413100128, 'F1-Weighted': 0.7808959612252773, 'F1_Class 0': 0.8317174515235458, 'F1_Class 1': 0.6584615384615383, 'F1_Class 2': 0.25688073394495414}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 162 at rate 0.1\n",
      "> Train with CF samples 8976\n",
      "{'Accuracy': 0.7976406533575318, 'F1-Macro': 0.5215320194747738, 'F1-Weighted': 0.7804737621235268, 'F1_Class 0': 0.8956228956228957, 'F1_Class 1': 0.4342313787638669, 'F1_Class 2': 0.23474178403755872}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 164 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.8938294010889292, 'F1-Macro': 0.6886299925470275, 'F1-Weighted': 0.9036796475621127, 'F1_Class 0': 0.946060606060606, 'F1_Class 1': 0.8192513368983957, 'F1_Class 2': 0.3005780346820809}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 164 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.8892921960072595, 'F1-Macro': 0.6758112421839191, 'F1-Weighted': 0.9063378608774159, 'F1_Class 0': 0.9496314496314496, 'F1_Class 1': 0.8242811501597443, 'F1_Class 2': 0.2535211267605634}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 163 at rate 0.1\n",
      "> Train with CF samples 8978\n",
      "{'Accuracy': 0.9278256922378575, 'F1-Macro': 0.7457712638983877, 'F1-Weighted': 0.9333922274990978, 'F1_Class 0': 0.9708383961117861, 'F1_Class 1': 0.8580246913580247, 'F1_Class 2': 0.40845070422535207}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 165 at rate 0.1\n",
      "> Train with CF samples 8980\n",
      "{'Accuracy': 0.7616886064457558, 'F1-Macro': 0.5884052712752784, 'F1-Weighted': 0.7899323485995123, 'F1_Class 0': 0.841526297696803, 'F1_Class 1': 0.665625, 'F1_Class 2': 0.25806451612903225}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 323 at rate 0.2\n",
      "> Train with CF samples 9137\n",
      "{'Accuracy': 0.8012704174228675, 'F1-Macro': 0.5298394386890963, 'F1-Weighted': 0.7830567301974478, 'F1_Class 0': 0.897982062780269, 'F1_Class 1': 0.43561208267090623, 'F1_Class 2': 0.2559241706161137}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 327 at rate 0.2\n",
      "> Train with CF samples 9141\n",
      "{'Accuracy': 0.897912885662432, 'F1-Macro': 0.6882257667873679, 'F1-Weighted': 0.9070342226761404, 'F1_Class 0': 0.948036253776435, 'F1_Class 1': 0.8292158968850698, 'F1_Class 2': 0.2874251497005988}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 327 at rate 0.2\n",
      "> Train with CF samples 9141\n",
      "{'Accuracy': 0.8897459165154264, 'F1-Macro': 0.6748451505883123, 'F1-Weighted': 0.9066001870192658, 'F1_Class 0': 0.9512718357339871, 'F1_Class 1': 0.8197424892703863, 'F1_Class 2': 0.2535211267605634}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 325 at rate 0.2\n",
      "> Train with CF samples 9140\n",
      "{'Accuracy': 0.929187471629596, 'F1-Macro': 0.7511733086744274, 'F1-Weighted': 0.934526891773574, 'F1_Class 0': 0.9714805825242719, 'F1_Class 1': 0.859504132231405, 'F1_Class 2': 0.4225352112676056}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 329 at rate 0.2\n",
      "> Train with CF samples 9144\n",
      "{'Accuracy': 0.7662278710848842, 'F1-Macro': 0.5893111167530215, 'F1-Weighted': 0.7931765851416617, 'F1_Class 0': 0.8431170614486783, 'F1_Class 1': 0.67601246105919, 'F1_Class 2': 0.24880382775119617}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 485 at rate 0.30000000000000004\n",
      "> Train with CF samples 9299\n",
      "{'Accuracy': 0.8003629764065335, 'F1-Macro': 0.5273361252282854, 'F1-Weighted': 0.7833431785717091, 'F1_Class 0': 0.8973063973063974, 'F1_Class 1': 0.4405705229793978, 'F1_Class 2': 0.24413145539906098}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 491 at rate 0.30000000000000004\n",
      "> Train with CF samples 9305\n",
      "{'Accuracy': 0.9006352087114338, 'F1-Macro': 0.7014130050620825, 'F1-Weighted': 0.9096145237512129, 'F1_Class 0': 0.949121744397335, 'F1_Class 1': 0.8336886993603412, 'F1_Class 2': 0.32142857142857145}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 491 at rate 0.30000000000000004\n",
      "> Train with CF samples 9305\n",
      "{'Accuracy': 0.8915607985480943, 'F1-Macro': 0.6782736074666297, 'F1-Weighted': 0.9074233961638986, 'F1_Class 0': 0.9513016845329251, 'F1_Class 1': 0.8226495726495726, 'F1_Class 2': 0.2608695652173913}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 488 at rate 0.30000000000000004\n",
      "> Train with CF samples 9303\n",
      "{'Accuracy': 0.9364502950522016, 'F1-Macro': 0.7537879209837653, 'F1-Weighted': 0.9410865420298864, 'F1_Class 0': 0.9755213055303716, 'F1_Class 1': 0.8770833333333334, 'F1_Class 2': 0.40875912408759124}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 494 at rate 0.30000000000000004\n",
      "> Train with CF samples 9309\n",
      "{'Accuracy': 0.7734906945074898, 'F1-Macro': 0.6011527240558394, 'F1-Weighted': 0.7983377432256937, 'F1_Class 0': 0.8469702156795618, 'F1_Class 1': 0.6837606837606838, 'F1_Class 2': 0.27272727272727276}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 647 at rate 0.4\n",
      "> Train with CF samples 9461\n",
      "{'Accuracy': 0.8008166969147006, 'F1-Macro': 0.5287530800251644, 'F1-Weighted': 0.7824478408120287, 'F1_Class 0': 0.8977304567105631, 'F1_Class 1': 0.4338118022328548, 'F1_Class 2': 0.2547169811320755}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 655 at rate 0.4\n",
      "> Train with CF samples 9469\n",
      "{'Accuracy': 0.895644283121597, 'F1-Macro': 0.6828860729654466, 'F1-Weighted': 0.9048689503775795, 'F1_Class 0': 0.9467634603750755, 'F1_Class 1': 0.8247863247863247, 'F1_Class 2': 0.27710843373493976}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 655 at rate 0.4\n",
      "> Train with CF samples 9469\n",
      "{'Accuracy': 0.8924682395644283, 'F1-Macro': 0.6778893569420811, 'F1-Weighted': 0.9085279046910387, 'F1_Class 0': 0.9529051987767584, 'F1_Class 1': 0.8223896663078579, 'F1_Class 2': 0.2583732057416268}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 650 at rate 0.4\n",
      "> Train with CF samples 9465\n",
      "{'Accuracy': 0.9369042215161144, 'F1-Macro': 0.7516840086988413, 'F1-Weighted': 0.9411808065475339, 'F1_Class 0': 0.9761545427105343, 'F1_Class 1': 0.8759124087591241, 'F1_Class 2': 0.40298507462686567}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 659 at rate 0.4\n",
      "> Train with CF samples 9474\n",
      "{'Accuracy': 0.769859282796187, 'F1-Macro': 0.5908396039671707, 'F1-Weighted': 0.7969229335252553, 'F1_Class 0': 0.84775915155662, 'F1_Class 1': 0.6771406127258445, 'F1_Class 2': 0.24761904761904763}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 808 at rate 0.5\n",
      "> Train with CF samples 9622\n",
      "{'Accuracy': 0.8008166969147006, 'F1-Macro': 0.5279722844691787, 'F1-Weighted': 0.7835649732160223, 'F1_Class 0': 0.89736399326977, 'F1_Class 1': 0.44126984126984126, 'F1_Class 2': 0.24528301886792453}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 818 at rate 0.5\n",
      "> Train with CF samples 9632\n",
      "{'Accuracy': 0.8988203266787659, 'F1-Macro': 0.6845338819881716, 'F1-Weighted': 0.9086883692670966, 'F1_Class 0': 0.9495010583610523, 'F1_Class 1': 0.8335123523093447, 'F1_Class 2': 0.27058823529411763}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 818 at rate 0.5\n",
      "> Train with CF samples 9632\n",
      "{'Accuracy': 0.8924682395644283, 'F1-Macro': 0.6788318233165908, 'F1-Weighted': 0.9088412805182992, 'F1_Class 0': 0.9519142419601839, 'F1_Class 1': 0.8274383708467311, 'F1_Class 2': 0.2571428571428572}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 813 at rate 0.5\n",
      "> Train with CF samples 9628\n",
      "{'Accuracy': 0.9378120744439401, 'F1-Macro': 0.7495650705660392, 'F1-Weighted': 0.9424956805084922, 'F1_Class 0': 0.9779655900996077, 'F1_Class 1': 0.8765690376569037, 'F1_Class 2': 0.39416058394160586}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 824 at rate 0.5\n",
      "> Train with CF samples 9639\n",
      "{'Accuracy': 0.7780299591466182, 'F1-Macro': 0.5999558848117709, 'F1-Weighted': 0.8031703282473147, 'F1_Class 0': 0.852760736196319, 'F1_Class 1': 0.6871069182389936, 'F1_Class 2': 0.26}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 970 at rate 0.6\n",
      "> Train with CF samples 9784\n",
      "{'Accuracy': 0.8012704174228675, 'F1-Macro': 0.5302485931151512, 'F1-Weighted': 0.7821709396685549, 'F1_Class 0': 0.8983478017362083, 'F1_Class 1': 0.4294871794871795, 'F1_Class 2': 0.26291079812206575}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 982 at rate 0.6\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.9029038112522686, 'F1-Macro': 0.6975160420394371, 'F1-Weighted': 0.9117338612520537, 'F1_Class 0': 0.9504830917874396, 'F1_Class 1': 0.8408602150537634, 'F1_Class 2': 0.3012048192771084}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 982 at rate 0.6\n",
      "> Train with CF samples 9796\n",
      "{'Accuracy': 0.8933756805807622, 'F1-Macro': 0.6805157912098553, 'F1-Weighted': 0.9090015349645681, 'F1_Class 0': 0.9519730804527378, 'F1_Class 1': 0.8274383708467311, 'F1_Class 2': 0.2621359223300971}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 976 at rate 0.6\n",
      "> Train with CF samples 9791\n",
      "{'Accuracy': 0.9373581479800273, 'F1-Macro': 0.7552101528189018, 'F1-Weighted': 0.9423428724314141, 'F1_Class 0': 0.9770531400966183, 'F1_Class 1': 0.8772298006295907, 'F1_Class 2': 0.4113475177304965}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 988 at rate 0.6\n",
      "> Train with CF samples 9803\n",
      "{'Accuracy': 0.781661370857921, 'F1-Macro': 0.6041878510099221, 'F1-Weighted': 0.8058454694709029, 'F1_Class 0': 0.8559782608695653, 'F1_Class 1': 0.6871552403467296, 'F1_Class 2': 0.2694300518134715}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1132 at rate 0.7000000000000001\n",
      "> Train with CF samples 9946\n",
      "{'Accuracy': 0.8012704174228675, 'F1-Macro': 0.5331917944996304, 'F1-Weighted': 0.7819623192505514, 'F1_Class 0': 0.8977877345281434, 'F1_Class 1': 0.4294871794871795, 'F1_Class 2': 0.27230046948356806}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1146 at rate 0.7000000000000001\n",
      "> Train with CF samples 9960\n",
      "{'Accuracy': 0.9024500907441017, 'F1-Macro': 0.6919540229885057, 'F1-Weighted': 0.9105087145628636, 'F1_Class 0': 0.95, 'F1_Class 1': 0.8383620689655172, 'F1_Class 2': 0.28750000000000003}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1146 at rate 0.7000000000000001\n",
      "> Train with CF samples 9960\n",
      "{'Accuracy': 0.8960980036297641, 'F1-Macro': 0.6834424585914268, 'F1-Weighted': 0.9105524514625934, 'F1_Class 0': 0.9519730804527378, 'F1_Class 1': 0.8343949044585987, 'F1_Class 2': 0.2639593908629441}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1138 at rate 0.7000000000000001\n",
      "> Train with CF samples 9953\n",
      "{'Accuracy': 0.9355424421243759, 'F1-Macro': 0.7455839247451791, 'F1-Weighted': 0.9405659045256342, 'F1_Class 0': 0.9761689291101056, 'F1_Class 1': 0.8748685594111462, 'F1_Class 2': 0.38571428571428573}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1153 at rate 0.7000000000000001\n",
      "> Train with CF samples 9968\n",
      "{'Accuracy': 0.7793917385383567, 'F1-Macro': 0.6025613980541844, 'F1-Weighted': 0.8046039230516422, 'F1_Class 0': 0.8544217687074831, 'F1_Class 1': 0.6872525732383215, 'F1_Class 2': 0.2660098522167488}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1294 at rate 0.8\n",
      "> Train with CF samples 10108\n",
      "{'Accuracy': 0.7999092558983666, 'F1-Macro': 0.5288454986174435, 'F1-Weighted': 0.7813916260394427, 'F1_Class 0': 0.8961075329039485, 'F1_Class 1': 0.4345047923322683, 'F1_Class 2': 0.2559241706161137}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1310 at rate 0.8\n",
      "> Train with CF samples 10124\n",
      "{'Accuracy': 0.9047186932849365, 'F1-Macro': 0.6886905197233224, 'F1-Weighted': 0.9121041168044433, 'F1_Class 0': 0.9515789473684211, 'F1_Class 1': 0.8417653390742733, 'F1_Class 2': 0.2727272727272727}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1310 at rate 0.8\n",
      "> Train with CF samples 10124\n",
      "{'Accuracy': 0.8988203266787659, 'F1-Macro': 0.6860649245757576, 'F1-Weighted': 0.9127922843516927, 'F1_Class 0': 0.9539212694537686, 'F1_Class 1': 0.8376068376068377, 'F1_Class 2': 0.2666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1301 at rate 0.8\n",
      "> Train with CF samples 10116\n",
      "{'Accuracy': 0.9396277802995915, 'F1-Macro': 0.755322298181904, 'F1-Weighted': 0.9441742717155456, 'F1_Class 0': 0.9783132530120482, 'F1_Class 1': 0.8818565400843882, 'F1_Class 2': 0.40579710144927533}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1318 at rate 0.8\n",
      "> Train with CF samples 10133\n",
      "{'Accuracy': 0.784384929641398, 'F1-Macro': 0.599954685586256, 'F1-Weighted': 0.8086166975686272, 'F1_Class 0': 0.8583050847457627, 'F1_Class 1': 0.6941362916006338, 'F1_Class 2': 0.24742268041237114}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1455 at rate 0.9\n",
      "> Train with CF samples 10269\n",
      "{'Accuracy': 0.8021778584392014, 'F1-Macro': 0.5303060064258519, 'F1-Weighted': 0.7819304408422186, 'F1_Class 0': 0.8980162056440346, 'F1_Class 1': 0.4294871794871795, 'F1_Class 2': 0.2634146341463415}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1473 at rate 0.9\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.9051724137931034, 'F1-Macro': 0.6910381793825501, 'F1-Weighted': 0.912035134310768, 'F1_Class 0': 0.9512341962673089, 'F1_Class 1': 0.8418803418803418, 'F1_Class 2': 0.27999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1473 at rate 0.9\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.8997277676950998, 'F1-Macro': 0.6870117248665316, 'F1-Weighted': 0.9135060850265041, 'F1_Class 0': 0.9550870760769935, 'F1_Class 1': 0.8365180467091295, 'F1_Class 2': 0.2694300518134715}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1463 at rate 0.9\n",
      "> Train with CF samples 10278\n",
      "{'Accuracy': 0.9391738538356786, 'F1-Macro': 0.7620052240305405, 'F1-Weighted': 0.943776460232731, 'F1_Class 0': 0.9776974080771549, 'F1_Class 1': 0.879746835443038, 'F1_Class 2': 0.42857142857142855}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1482 at rate 0.9\n",
      "> Train with CF samples 10297\n",
      "{'Accuracy': 0.7871084884248751, 'F1-Macro': 0.611291687927222, 'F1-Weighted': 0.8097346789827475, 'F1_Class 0': 0.8587876735523197, 'F1_Class 1': 0.694006309148265, 'F1_Class 2': 0.28108108108108104}\n",
      "> Total Tweets used to generate counterfactuals 1617\n",
      "> Total counterfactuals added 1617\n",
      "> Counterfactual size 1617 at rate 1.0\n",
      "> Train with CF samples 10431\n",
      "{'Accuracy': 0.8008166969147006, 'F1-Macro': 0.528586013986014, 'F1-Weighted': 0.7811816144835201, 'F1_Class 0': 0.8973426573426574, 'F1_Class 1': 0.4288, 'F1_Class 2': 0.25961538461538464}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1637 at rate 1.0\n",
      "> Train with CF samples 10451\n",
      "{'Accuracy': 0.9056261343012704, 'F1-Macro': 0.691750430094801, 'F1-Weighted': 0.9124975802133228, 'F1_Class 0': 0.9512341962673089, 'F1_Class 1': 0.844017094017094, 'F1_Class 2': 0.27999999999999997}\n",
      "> Total Tweets used to generate counterfactuals 1637\n",
      "> Total counterfactuals added 1637\n",
      "> Counterfactual size 1637 at rate 1.0\n",
      "> Train with CF samples 10451\n",
      "{'Accuracy': 0.9006352087114338, 'F1-Macro': 0.6881882203690716, 'F1-Weighted': 0.9141878043692202, 'F1_Class 0': 0.9554334554334555, 'F1_Class 1': 0.8382978723404256, 'F1_Class 2': 0.2708333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1626\n",
      "> Total counterfactuals added 1626\n",
      "> Counterfactual size 1626 at rate 1.0\n",
      "> Train with CF samples 10441\n",
      "{'Accuracy': 0.9405356332274172, 'F1-Macro': 0.762508010252202, 'F1-Weighted': 0.944684002557971, 'F1_Class 0': 0.9780186690755797, 'F1_Class 1': 0.8830347734457322, 'F1_Class 2': 0.4264705882352941}\n",
      "> Total Tweets used to generate counterfactuals 1647\n",
      "> Total counterfactuals added 1647\n",
      "> Counterfactual size 1647 at rate 1.0\n",
      "> Train with CF samples 10462\n",
      "{'Accuracy': 0.7898320472083522, 'F1-Macro': 0.607566826733467, 'F1-Weighted': 0.8113517040882272, 'F1_Class 0': 0.8608108108108108, 'F1_Class 1': 0.6959937156323646, 'F1_Class 2': 0.2658959537572254}\n",
      "Sports (12306, 4)\n",
      "> Train samples 9844\n",
      "{'Accuracy': 0.9029244516653128, 'F1-Macro': 0.6360632231869118, 'F1-Weighted': 0.8948513513627626, 'F1_Class 0': 0.9506922516395434, 'F1_Class 1': 0.7103064066852367, 'F1_Class 2': 0.24719101123595508}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.9593661113368549, 'F1-Macro': 0.7610826580545385, 'F1-Weighted': 0.9629254187037815, 'F1_Class 0': 0.9883248730964467, 'F1_Class 1': 0.9050925925925924, 'F1_Class 2': 0.3898305084745763}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.9479886225111743, 'F1-Macro': 0.7147713612892752, 'F1-Weighted': 0.9541461841069785, 'F1_Class 0': 0.9854257223216569, 'F1_Class 1': 0.8798185941043083, 'F1_Class 2': 0.2790697674418604}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.963023161316538, 'F1-Macro': 0.7959659660419768, 'F1-Weighted': 0.9653270535690529, 'F1_Class 0': 0.9883130081300813, 'F1_Class 1': 0.908675799086758, 'F1_Class 2': 0.49090909090909085}\n",
      "> Train samples 9845\n",
      "{'Accuracy': 0.8358390898008939, 'F1-Macro': 0.6416615892358467, 'F1-Weighted': 0.8692956674000161, 'F1_Class 0': 0.9016666666666667, 'F1_Class 1': 0.7861386138613861, 'F1_Class 2': 0.2371794871794872}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 148 at rate 0.1\n",
      "> Train with CF samples 9992\n",
      "{'Accuracy': 0.9029244516653128, 'F1-Macro': 0.6352246925210373, 'F1-Weighted': 0.8949894080295002, 'F1_Class 0': 0.9509232264334305, 'F1_Class 1': 0.7103064066852367, 'F1_Class 2': 0.24444444444444444}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 146 at rate 0.1\n",
      "> Train with CF samples 9991\n",
      "{'Accuracy': 0.9593661113368549, 'F1-Macro': 0.7610826580545385, 'F1-Weighted': 0.9629254187037815, 'F1_Class 0': 0.9883248730964467, 'F1_Class 1': 0.9050925925925924, 'F1_Class 2': 0.3898305084745763}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 147 at rate 0.1\n",
      "> Train with CF samples 9992\n",
      "{'Accuracy': 0.9479886225111743, 'F1-Macro': 0.7109616639771108, 'F1-Weighted': 0.9541277775443177, 'F1_Class 0': 0.9851738241308793, 'F1_Class 1': 0.8820861678004533, 'F1_Class 2': 0.265625}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 151 at rate 0.1\n",
      "> Train with CF samples 9996\n",
      "{'Accuracy': 0.9634295002031694, 'F1-Macro': 0.7987365526232383, 'F1-Weighted': 0.9654993912676891, 'F1_Class 0': 0.9885699771399543, 'F1_Class 1': 0.9076396807297605, 'F1_Class 2': 0.5000000000000001}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 153 at rate 0.1\n",
      "> Train with CF samples 9998\n",
      "{'Accuracy': 0.8382771231206827, 'F1-Macro': 0.6432348788125041, 'F1-Weighted': 0.8716619847928702, 'F1_Class 0': 0.9041019955654103, 'F1_Class 1': 0.7884231536926148, 'F1_Class 2': 0.2371794871794872}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 295 at rate 0.2\n",
      "> Train with CF samples 10139\n",
      "{'Accuracy': 0.9033306255077173, 'F1-Macro': 0.618704420861374, 'F1-Weighted': 0.8950153982550947, 'F1_Class 0': 0.9511543134872419, 'F1_Class 1': 0.7144827586206896, 'F1_Class 2': 0.19047619047619047}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 291 at rate 0.2\n",
      "> Train with CF samples 10136\n",
      "{'Accuracy': 0.9597724502234863, 'F1-Macro': 0.7624741507020238, 'F1-Weighted': 0.9631433274954742, 'F1_Class 0': 0.9883367139959433, 'F1_Class 1': 0.9059233449477353, 'F1_Class 2': 0.39316239316239315}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 294 at rate 0.2\n",
      "> Train with CF samples 10139\n",
      "{'Accuracy': 0.9483949613978058, 'F1-Macro': 0.7090060287940307, 'F1-Weighted': 0.9548649753481965, 'F1_Class 0': 0.9864692366607097, 'F1_Class 1': 0.8810068649885584, 'F1_Class 2': 0.2595419847328244}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 301 at rate 0.2\n",
      "> Train with CF samples 10146\n",
      "{'Accuracy': 0.9642421779764323, 'F1-Macro': 0.8026277243203838, 'F1-Weighted': 0.9664390545011005, 'F1_Class 0': 0.9890779781559563, 'F1_Class 1': 0.9097142857142857, 'F1_Class 2': 0.509090909090909}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 306 at rate 0.2\n",
      "> Train with CF samples 10151\n",
      "{'Accuracy': 0.8390898008939456, 'F1-Macro': 0.6438309385790629, 'F1-Weighted': 0.8722700775273969, 'F1_Class 0': 0.9047619047619047, 'F1_Class 1': 0.7887887887887888, 'F1_Class 2': 0.23794212218649521}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 443 at rate 0.30000000000000004\n",
      "> Train with CF samples 10287\n",
      "{'Accuracy': 0.9041429731925265, 'F1-Macro': 0.6342255139558293, 'F1-Weighted': 0.8959554070056175, 'F1_Class 0': 0.9509232264334305, 'F1_Class 1': 0.7164591977869985, 'F1_Class 2': 0.2352941176470588}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 437 at rate 0.30000000000000004\n",
      "> Train with CF samples 10282\n",
      "{'Accuracy': 0.9589597724502235, 'F1-Macro': 0.7652638150966021, 'F1-Weighted': 0.9624072125789046, 'F1_Class 0': 0.9883189436262062, 'F1_Class 1': 0.9006928406466511, 'F1_Class 2': 0.4067796610169492}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 440 at rate 0.30000000000000004\n",
      "> Train with CF samples 10285\n",
      "{'Accuracy': 0.94676960585128, 'F1-Macro': 0.7070006958501032, 'F1-Weighted': 0.9534530263349608, 'F1_Class 0': 0.985433171479683, 'F1_Class 1': 0.8779931584948689, 'F1_Class 2': 0.25757575757575757}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 452 at rate 0.30000000000000004\n",
      "> Train with CF samples 10297\n",
      "{'Accuracy': 0.9666802112962211, 'F1-Macro': 0.8131071652872933, 'F1-Weighted': 0.9686821748328879, 'F1_Class 0': 0.9895912668189897, 'F1_Class 1': 0.9176201372997712, 'F1_Class 2': 0.5321100917431193}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 459 at rate 0.30000000000000004\n",
      "> Train with CF samples 10304\n",
      "{'Accuracy': 0.8390898008939456, 'F1-Macro': 0.6434122504568093, 'F1-Weighted': 0.8723873406655273, 'F1_Class 0': 0.9051175656984785, 'F1_Class 1': 0.7879396984924624, 'F1_Class 2': 0.2371794871794872}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 591 at rate 0.4\n",
      "> Train with CF samples 10435\n",
      "{'Accuracy': 0.9037367993501219, 'F1-Macro': 0.6327209354705091, 'F1-Weighted': 0.8955845198699266, 'F1_Class 0': 0.9509232264334305, 'F1_Class 1': 0.7146814404432132, 'F1_Class 2': 0.23255813953488372}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 582 at rate 0.4\n",
      "> Train with CF samples 10427\n",
      "{'Accuracy': 0.958553433563592, 'F1-Macro': 0.759877550031269, 'F1-Weighted': 0.962107081711289, 'F1_Class 0': 0.9880680375729882, 'F1_Class 1': 0.9017341040462427, 'F1_Class 2': 0.3898305084745763}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 587 at rate 0.4\n",
      "> Train with CF samples 10432\n",
      "{'Accuracy': 0.9532710280373832, 'F1-Macro': 0.7193878746239545, 'F1-Weighted': 0.958753500652219, 'F1_Class 0': 0.987496810410819, 'F1_Class 1': 0.8964732650739476, 'F1_Class 2': 0.27419354838709675}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 603 at rate 0.4\n",
      "> Train with CF samples 10448\n",
      "{'Accuracy': 0.9662738724095896, 'F1-Macro': 0.8063343723592719, 'F1-Weighted': 0.968327787502957, 'F1_Class 0': 0.990104034509008, 'F1_Class 1': 0.9151376146788991, 'F1_Class 2': 0.5137614678899082}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 612 at rate 0.4\n",
      "> Train with CF samples 10457\n",
      "{'Accuracy': 0.8415278342137342, 'F1-Macro': 0.6456252597484403, 'F1-Weighted': 0.8748436570295985, 'F1_Class 0': 0.9069317867992267, 'F1_Class 1': 0.7935222672064778, 'F1_Class 2': 0.23642172523961663}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 738 at rate 0.5\n",
      "> Train with CF samples 10582\n",
      "{'Accuracy': 0.9037367993501219, 'F1-Macro': 0.6152305652690905, 'F1-Weighted': 0.8950065481646095, 'F1_Class 0': 0.9506682867557715, 'F1_Class 1': 0.7178082191780822, 'F1_Class 2': 0.1772151898734177}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 728 at rate 0.5\n",
      "> Train with CF samples 10573\n",
      "{'Accuracy': 0.9593661113368549, 'F1-Macro': 0.7596644132155096, 'F1-Weighted': 0.9630081530105985, 'F1_Class 0': 0.9888381532217149, 'F1_Class 1': 0.9036004645760745, 'F1_Class 2': 0.3865546218487395}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 734 at rate 0.5\n",
      "> Train with CF samples 10579\n",
      "{'Accuracy': 0.9553027224705404, 'F1-Macro': 0.7230233821265859, 'F1-Weighted': 0.9602599083515712, 'F1_Class 0': 0.9885350318471336, 'F1_Class 1': 0.8995433789954338, 'F1_Class 2': 0.2809917355371901}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 754 at rate 0.5\n",
      "> Train with CF samples 10599\n",
      "{'Accuracy': 0.9687119057293783, 'F1-Macro': 0.8177539650107531, 'F1-Weighted': 0.9705217148014195, 'F1_Class 0': 0.991114496064991, 'F1_Class 1': 0.9200913242009131, 'F1_Class 2': 0.5420560747663552}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 764 at rate 0.5\n",
      "> Train with CF samples 10609\n",
      "{'Accuracy': 0.8435595286468915, 'F1-Macro': 0.6475760838528326, 'F1-Weighted': 0.8766855352875372, 'F1_Class 0': 0.9083885209713025, 'F1_Class 1': 0.797160243407708, 'F1_Class 2': 0.2371794871794872}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 886 at rate 0.6\n",
      "> Train with CF samples 10730\n",
      "{'Accuracy': 0.9041429731925265, 'F1-Macro': 0.6226520644887992, 'F1-Weighted': 0.8954579534923207, 'F1_Class 0': 0.9509232264334305, 'F1_Class 1': 0.717032967032967, 'F1_Class 2': 0.2}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 873 at rate 0.6\n",
      "> Train with CF samples 10718\n",
      "{'Accuracy': 0.9622104835432751, 'F1-Macro': 0.7669451751749109, 'F1-Weighted': 0.9653967334442372, 'F1_Class 0': 0.9898528665651954, 'F1_Class 1': 0.9109826589595375, 'F1_Class 2': 0.4000000000000001}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 881 at rate 0.6\n",
      "> Train with CF samples 10726\n",
      "{'Accuracy': 0.9536773669240146, 'F1-Macro': 0.7209645403465252, 'F1-Weighted': 0.9588729961388869, 'F1_Class 0': 0.987496810410819, 'F1_Class 1': 0.8967082860385925, 'F1_Class 2': 0.2786885245901639}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 904 at rate 0.6\n",
      "> Train with CF samples 10749\n",
      "{'Accuracy': 0.9703372612759041, 'F1-Macro': 0.8140359855280628, 'F1-Weighted': 0.9722082509074573, 'F1_Class 0': 0.992137966015724, 'F1_Class 1': 0.926605504587156, 'F1_Class 2': 0.5233644859813084}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 917 at rate 0.6\n",
      "> Train with CF samples 10762\n",
      "{'Accuracy': 0.8431531897602601, 'F1-Macro': 0.6472627810869936, 'F1-Weighted': 0.8767006244935632, 'F1_Class 0': 0.9080872205354678, 'F1_Class 1': 0.7987804878048781, 'F1_Class 2': 0.23492063492063492}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1034 at rate 0.7000000000000001\n",
      "> Train with CF samples 10878\n",
      "{'Accuracy': 0.9037367993501219, 'F1-Macro': 0.6136755955643495, 'F1-Weighted': 0.8951796154916388, 'F1_Class 0': 0.9511543134872419, 'F1_Class 1': 0.717032967032967, 'F1_Class 2': 0.1728395061728395}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1019 at rate 0.7000000000000001\n",
      "> Train with CF samples 10864\n",
      "{'Accuracy': 0.9618041446566437, 'F1-Macro': 0.7657103107322273, 'F1-Weighted': 0.9651327054092139, 'F1_Class 0': 0.9895965490992134, 'F1_Class 1': 0.9109826589595375, 'F1_Class 2': 0.39655172413793105}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1028 at rate 0.7000000000000001\n",
      "> Train with CF samples 10873\n",
      "{'Accuracy': 0.9496139780577001, 'F1-Macro': 0.7104686935031763, 'F1-Weighted': 0.9563000080248747, 'F1_Class 0': 0.9869731800766284, 'F1_Class 1': 0.8868571428571429, 'F1_Class 2': 0.25757575757575757}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1055 at rate 0.7000000000000001\n",
      "> Train with CF samples 10900\n",
      "{'Accuracy': 0.9703372612759041, 'F1-Macro': 0.8182383713290186, 'F1-Weighted': 0.9722451766366724, 'F1_Class 0': 0.9921339761481858, 'F1_Class 1': 0.9255441008018328, 'F1_Class 2': 0.537037037037037}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1070 at rate 0.7000000000000001\n",
      "> Train with CF samples 10915\n",
      "{'Accuracy': 0.8451848841934173, 'F1-Macro': 0.6497852644213178, 'F1-Weighted': 0.876679589716377, 'F1_Class 0': 0.908740005514199, 'F1_Class 1': 0.7947686116700202, 'F1_Class 2': 0.24584717607973425}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1182 at rate 0.8\n",
      "> Train with CF samples 11026\n",
      "{'Accuracy': 0.9033306255077173, 'F1-Macro': 0.6123847487963031, 'F1-Weighted': 0.8948218183908017, 'F1_Class 0': 0.9511543134872419, 'F1_Class 1': 0.7152682255845942, 'F1_Class 2': 0.17073170731707316}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1164 at rate 0.8\n",
      "> Train with CF samples 11009\n",
      "{'Accuracy': 0.9622104835432751, 'F1-Macro': 0.7646988701384387, 'F1-Weighted': 0.9656146358167333, 'F1_Class 0': 0.990365111561866, 'F1_Class 1': 0.910569105691057, 'F1_Class 2': 0.39316239316239315}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1174 at rate 0.8\n",
      "> Train with CF samples 11019\n",
      "{'Accuracy': 0.9516456724908574, 'F1-Macro': 0.7141952545784944, 'F1-Weighted': 0.9574655358303249, 'F1_Class 0': 0.9882712901580826, 'F1_Class 1': 0.8865979381443301, 'F1_Class 2': 0.26771653543307083}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1206 at rate 0.8\n",
      "> Train with CF samples 11051\n",
      "{'Accuracy': 0.9683055668427468, 'F1-Macro': 0.8001531810314808, 'F1-Weighted': 0.9707080079099893, 'F1_Class 0': 0.9921339761481858, 'F1_Class 1': 0.9218390804597701, 'F1_Class 2': 0.4864864864864865}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1223 at rate 0.8\n",
      "> Train with CF samples 11068\n",
      "{'Accuracy': 0.8451848841934173, 'F1-Macro': 0.649014601845488, 'F1-Weighted': 0.8778360672290243, 'F1_Class 0': 0.9095920617420066, 'F1_Class 1': 0.797969543147208, 'F1_Class 2': 0.23948220064724918}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1329 at rate 0.9\n",
      "> Train with CF samples 11173\n",
      "{'Accuracy': 0.9017059301380991, 'F1-Macro': 0.6092745638200184, 'F1-Weighted': 0.8933340494525044, 'F1_Class 0': 0.9504132231404958, 'F1_Class 1': 0.7107438016528925, 'F1_Class 2': 0.16666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1310 at rate 0.9\n",
      "> Train with CF samples 11155\n",
      "{'Accuracy': 0.9622104835432751, 'F1-Macro': 0.7737099628534319, 'F1-Weighted': 0.9651490379935455, 'F1_Class 0': 0.9890945980218108, 'F1_Class 1': 0.9109826589595375, 'F1_Class 2': 0.4210526315789474}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1321 at rate 0.9\n",
      "> Train with CF samples 11166\n",
      "{'Accuracy': 0.9557090613571719, 'F1-Macro': 0.7260256095914976, 'F1-Weighted': 0.9601514377158988, 'F1_Class 0': 0.9882712901580826, 'F1_Class 1': 0.89920724801812, 'F1_Class 2': 0.29059829059829057}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1356 at rate 0.9\n",
      "> Train with CF samples 11201\n",
      "{'Accuracy': 0.9715562779357985, 'F1-Macro': 0.8167005259601766, 'F1-Weighted': 0.9733208320307148, 'F1_Class 0': 0.9929006085192696, 'F1_Class 1': 0.9288990825688074, 'F1_Class 2': 0.5283018867924528}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1376 at rate 0.9\n",
      "> Train with CF samples 11221\n",
      "{'Accuracy': 0.8488419341731004, 'F1-Macro': 0.6533563285974201, 'F1-Weighted': 0.8798327652476746, 'F1_Class 0': 0.9113436123348018, 'F1_Class 1': 0.8004032258064516, 'F1_Class 2': 0.2483221476510067}\n",
      "> Total Tweets used to generate counterfactuals 1477\n",
      "> Total counterfactuals added 1477\n",
      "> Counterfactual size 1477 at rate 1.0\n",
      "> Train with CF samples 11321\n",
      "{'Accuracy': 0.9021121039805037, 'F1-Macro': 0.6099427197266404, 'F1-Weighted': 0.8938424583895933, 'F1_Class 0': 0.950644298565524, 'F1_Class 1': 0.7125171939477303, 'F1_Class 2': 0.16666666666666666}\n",
      "> Total Tweets used to generate counterfactuals 1455\n",
      "> Total counterfactuals added 1455\n",
      "> Counterfactual size 1455 at rate 1.0\n",
      "> Train with CF samples 11300\n",
      "{'Accuracy': 0.9618041446566437, 'F1-Macro': 0.7675949992528714, 'F1-Weighted': 0.9648585592710786, 'F1_Class 0': 0.9893455098934549, 'F1_Class 1': 0.9099307159353348, 'F1_Class 2': 0.40350877192982454}\n",
      "> Total Tweets used to generate counterfactuals 1468\n",
      "> Total counterfactuals added 1468\n",
      "> Counterfactual size 1468 at rate 1.0\n",
      "> Train with CF samples 11313\n",
      "{'Accuracy': 0.9548963835839089, 'F1-Macro': 0.723771730914588, 'F1-Weighted': 0.9597996316229322, 'F1_Class 0': 0.9877551020408163, 'F1_Class 1': 0.9002267573696145, 'F1_Class 2': 0.2833333333333333}\n",
      "> Total Tweets used to generate counterfactuals 1507\n",
      "> Total counterfactuals added 1507\n",
      "> Counterfactual size 1507 at rate 1.0\n",
      "> Train with CF samples 11352\n",
      "{'Accuracy': 0.9715562779357985, 'F1-Macro': 0.8154102171106022, 'F1-Weighted': 0.9734307099809368, 'F1_Class 0': 0.9929006085192696, 'F1_Class 1': 0.9299655568312284, 'F1_Class 2': 0.5233644859813084}\n",
      "> Total Tweets used to generate counterfactuals 1529\n",
      "> Total counterfactuals added 1529\n",
      "> Counterfactual size 1529 at rate 1.0\n",
      "> Train with CF samples 11374\n",
      "{'Accuracy': 0.8480292563998375, 'F1-Macro': 0.6519966923270888, 'F1-Weighted': 0.8793633520547923, 'F1_Class 0': 0.9113436123348018, 'F1_Class 1': 0.797979797979798, 'F1_Class 2': 0.24666666666666667}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_davidson_pipeline():\n",
    "    # CHANGE --> solver='liblinear' added for l1, otherwise won't work\n",
    "    # CHANGE --> max_iter=10000 added for l2, otherwsie ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    return Pipeline([('sel-lg-l1', SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01, solver='liblinear'))),\n",
    "                     ('lg-l2',LogisticRegression(class_weight='balanced',penalty='l2',C=0.01,  max_iter=10000))])\n",
    "    \n",
    "for d in domains:\n",
    "    out_dict[d] = {}\n",
    "    sel_df = df[df['Domain'] == d]\n",
    "    print(d, sel_df.shape)\n",
    "    X, y = sel_df['Tweet'], sel_df['Label'].astype(int)\n",
    "\n",
    "    davidson_pipeline = get_davidson_pipeline()\n",
    "    res_dav_lst = run_experiment_org(davidson_pipeline, n_splits=n_splits)\n",
    "    out_dict[d]['Org'] = res_dav_lst\n",
    "    out_dict[d]['CF'] = {}\n",
    "    for cf_size_prop_to_data in cf_size_prop_to_data_lst:\n",
    "        davidson_pipeline = get_davidson_pipeline()\n",
    "    \n",
    "        res_dav_lst = run_experiment_counter_factuals(davidson_pipeline, n_splits=n_splits, cf_size_prop_to_data=cf_size_prop_to_data)\n",
    "        out_dict[d]['CF'][cf_size_prop_to_data] = res_dav_lst\n",
    "    json.dump(out_dict, open('out/davidson-pipline-lshs22.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f3406da-ca7c-4e84-97da-f8ec0f4cd211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1.0\n"
     ]
    }
   ],
   "source": [
    "print('done', cf_size_prop_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5bfba6b-56ba-463b-9df1-eea3dfb20bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 > 0.05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
